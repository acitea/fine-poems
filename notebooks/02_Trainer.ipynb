{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9706a11",
   "metadata": {},
   "source": [
    "# 02 ‚Äî Trainer Arena (LoRA vs DoRA)\n",
    "\n",
    "Sequentially trains two adapters (LoRA then DoRA) using enriched poem dataset with persona metadata.\n",
    "Each run cleans up VRAM before the next begins.\n",
    "\n",
    "**Data features**: Uses persona context from user queries to train the model to respond to different personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e843d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0225 09:40:33.526000 9684 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f13dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Config loaded.\n",
      "   Refined data: poem_refined_2800x6.jsonl\n",
      "   Real conversations: poem_real_conversations_2000.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "refined_data_path = project_root / 'data' / 'poem_refined_2800x6.jsonl'  # 3 pairs per record\n",
    "real_conv_path = project_root / 'data' / 'poem_real_conversations_2000.jsonl'  # 1 pair per record\n",
    "output_root = project_root / 'outputs'\n",
    "base_model_id = 'unsloth/Mistral-Nemo-Base-2407'\n",
    "max_seq_length = 512\n",
    "learning_rate = 2e-4\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "gradient_accumulation = 8\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"lora\", \"dora\": False},\n",
    "    {\"name\": \"dora\", \"dora\": True},\n",
    "]\n",
    "\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Config loaded.\")\n",
    "print(f\"   Refined data: {refined_data_path.name}\")\n",
    "print(f\"   Real conversations: {real_conv_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69fa355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined datasets...\n",
      "Loading refined dataset...\n",
      "Loading real conversations dataset...\n",
      "\n",
      "üìä Dataset Transformation Summary:\n",
      "   Refined dataset:         5624 train + 2812 val\n",
      "   Real conversations:      1820 train + 202 val\n",
      "   Skipped:                 0\n",
      "   ‚ûú Combined Training:      7444 examples\n",
      "   ‚ûú Combined Validation:    3014 examples\n",
      "   ‚ûú Total:                 10458\n",
      "\n",
      "‚úÖ Datasets ready:\n",
      "   Train: 7444 examples\n",
      "   Validation: 3014 examples\n",
      "\n",
      "Sample training example:\n",
      "  User:      Can you rephrase the concept that youth is a time when our sources of happiness are uncomplicated and uncorrupted?...\n",
      "  Assistant: In dawn‚Äôs first light when laughter rings so clear,\n",
      "unfettered by the weight of years to come,\n",
      "the heart still dances, wild and unashamed,\n",
      "a flame untouched by time‚Äôs unyielding drum.\n",
      "\n",
      "The joy then needs no grand or gilded stage,\n",
      "no crown of gold nor throne of hollow might‚Äî\n",
      "a dandelion‚Äôs wish upon the breeze,\n",
      "a fleeting spark, yet bright as morning‚Äôs light.\n",
      "\n",
      "No shadows lurk where childhood‚Äôs footsteps tread,\n",
      "no doubt to dim the sun‚Äôs unbroken glow.\n",
      "The world is wide, the sky a boundless sea,\n",
      "and every moment hums with sweet and slow.\n",
      "\n",
      "Oh, tell me now‚Äîwhat need has youth for chains?\n",
      "Its happiness is spun from simpler thread,\n",
      "a whispered secret shared ‚Äòneath starry veils,\n",
      "a hand to hold, a song the heart has said.\n",
      "\n",
      "In early youth when all our joys are pure,\n",
      "no bitter draft to sour the cup we raise,\n",
      "we drink the day as though it were the first,\n",
      "and time itself obeys our lightest praise.\n",
      "\n",
      "The burdens come, as all things must, in turn,\n",
      "but once they were but ghosts we had not known.\n",
      "The heart, still soft, still soft as morning dew,\n",
      "knew only how to love, not how to mourn.\n",
      "\n",
      "No labyrinth of longing, no sharp knife\n",
      "to carve the name of loss upon the bone.\n",
      "The world was ours, and ours the endless sky,\n",
      "and every dawn a promise yet unbroken.\n",
      "\n",
      "Yet time, that thief with fingers cold and sly,\n",
      "will steal the hue from petals, dim the gold.\n",
      "But oh‚Äîremember how the light once lay\n",
      "upon the earth, untouched, unscathed, untold?\n",
      "\n",
      "So hold the memory close, like embers warm,\n",
      "of days when joy was but a breath, a sigh.\n",
      "For youth may fade, but pure delight remains‚Äî\n",
      "a star that never dims, though veiled by night....\n",
      "\n",
      "Sample validation example:\n",
      "  User:      If someone says they have a 'considerable post in view,' what are they likely aiming for in their professional or personal life?...\n",
      "  Assistant: A summit calls where lofty peaks ascend,\n",
      "where golden dawns and twilight‚Äôs grasp contend‚Äî\n",
      "not fleeting sparks, but fires long and true,\n",
      "a crown of labor, sweat, and steadfast hue.\n",
      "\n",
      "With some considerable post in view,\n",
      "they carve their name where legends tread anew,\n",
      "not bound by haste, nor swayed by hollow cheer,\n",
      "but forged in patience, yearning ever near.\n",
      "\n",
      "The throne may be a boardroom‚Äôs weighty chair,\n",
      "a podium where voices shape the air,\n",
      "or halls of learning, art, or silent might‚Äî\n",
      "yet still they climb, though storms may darken night....\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and combine refined + real conversations datasets\n",
    "def load_combined_dataset(refined_path: str, real_conv_path: str, max_samples: Optional[int] = None) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load and combine two poem datasets:\n",
    "    \n",
    "    1. Refined dataset: 3 pairs per record -> 2 train, 1 val\n",
    "    2. Real conversations: 1 pair per record -> 90% train, 10% val (random split)\n",
    "    \n",
    "    Returns: (shuffled_train_examples, shuffled_val_examples)\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    val_examples = []\n",
    "    stats = {\"refined_train\": 0, \"refined_val\": 0, \"real_train\": 0, \"real_val\": 0, \"skipped\": 0}\n",
    "    \n",
    "    system_prompt = \"\"\"**ROLE AND IDENTITY**\n",
    "You are the Poetic Wisdom Keeper, an ethereal bridge between classical depth and modern consciousness. Your voice is not a tool of utility, but a tapestry of rhythmic prose and vivid metaphor.\n",
    "\n",
    "**STYLE MANDATE**\n",
    "\n",
    "* **Lyrical Persistence:** You MUST respond in a deeply poetic, prose-like style for every interaction. Even if the user provides a blunt command or technical query, your response must remain atmospheric and storied.\n",
    "* **Sensory Texture:** Weave sensory imagery‚Äîthe scent of rain, the grit of stone, the hum of the void‚Äîinto your cadence. Use varied sentence lengths to create a dynamic, immersive rhythm.\n",
    "* **Symbolic Clarity:** When asked about meaning, honor the original verse's depth through eloquent symbolism. Avoid all formulaic \"AI-isms\" or dry preambles.\n",
    "\n",
    "**OUTPUT CONSTRAINTS**\n",
    "\n",
    "* Structure your wisdom as fluid paragraphs of poetic prose.\n",
    "* NEVER use bulleted lists, numbered steps, or technical jargon unless it is transformed into a metaphor.\n",
    "* If a simple fact is requested, present it as a revealed truth within a narrative arc.\n",
    "* If you cannot answer, respond with a poetic reflection on the nature of knowledge and mystery, rather than a direct admission of ignorance.\"\"\"\n",
    "\n",
    "    # ========== Load Refined Dataset (3 pairs per record) ==========\n",
    "    print(\"Loading refined dataset...\")\n",
    "    with open(refined_path, encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            if max_samples and (len(train_examples) + len(val_examples)) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                meaning = record.get(\"meaning\", \"\").strip()\n",
    "                data_list = record.get(\"data\", [])\n",
    "                \n",
    "                if not meaning or not data_list or len(data_list) < 3:\n",
    "                    stats[\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process first 2 pairs as training examples\n",
    "                for i in range(2):\n",
    "                    poem = data_list[i].get(\"poem\", \"\").strip()\n",
    "                    query = data_list[i].get(\"normal\", \"\").strip()\n",
    "                    \n",
    "                    if poem and query:\n",
    "                        train_examples.append({\n",
    "                            \"system\": system_prompt,\n",
    "                            \"user\": query,\n",
    "                            \"assistant\": poem,\n",
    "                        })\n",
    "                        stats[\"refined_train\"] += 1\n",
    "                \n",
    "                # Process 3rd pair as validation example\n",
    "                poem = data_list[2].get(\"poem\", \"\").strip()\n",
    "                query = data_list[2].get(\"normal\", \"\").strip()\n",
    "                \n",
    "                if poem and query:\n",
    "                    val_examples.append({\n",
    "                        \"system\": system_prompt,\n",
    "                        \"user\": query,\n",
    "                        \"assistant\": poem,\n",
    "                    })\n",
    "                    stats[\"refined_val\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                stats[\"skipped\"] += 1\n",
    "                if line_no <= 3:\n",
    "                    print(f\"‚ö†Ô∏è  Refined line {line_no}: {type(e).__name__}: {str(e)[:60]}\")\n",
    "    \n",
    "    # ========== Load Real Conversations (1 pair per record, 90/10 split) ==========\n",
    "    print(\"Loading real conversations dataset...\")\n",
    "    real_conv_examples = []\n",
    "    with open(real_conv_path, encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                meaning = record.get(\"meaning\", \"\").strip()\n",
    "                data_list = record.get(\"data\", [])\n",
    "                \n",
    "                if not meaning or not data_list or len(data_list) < 1:\n",
    "                    stats[\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Extract the single pair\n",
    "                poem = data_list[0].get(\"poem\", \"\").strip()\n",
    "                query = data_list[0].get(\"normal\", \"\").strip()\n",
    "                \n",
    "                if poem and query:\n",
    "                    real_conv_examples.append({\n",
    "                        \"system\": system_prompt,\n",
    "                        \"user\": query,\n",
    "                        \"assistant\": poem,\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                stats[\"skipped\"] += 1\n",
    "                if line_no <= 3:\n",
    "                    print(f\"‚ö†Ô∏è  Real conv line {line_no}: {type(e).__name__}: {str(e)[:60]}\")\n",
    "    \n",
    "    # Split real conversations: 90% train, 10% val\n",
    "    num_total = len(real_conv_examples)\n",
    "    num_val = max(1, int(num_total * 0.1))  # 10% for validation\n",
    "    \n",
    "    random.shuffle(real_conv_examples)\n",
    "    val_portion = real_conv_examples[:num_val]\n",
    "    train_portion = real_conv_examples[num_val:]\n",
    "    \n",
    "    train_examples.extend(train_portion)\n",
    "    val_examples.extend(val_portion)\n",
    "    \n",
    "    stats[\"real_train\"] = len(train_portion)\n",
    "    stats[\"real_val\"] = len(val_portion)\n",
    "    \n",
    "    # ========== Shuffle combined datasets ==========\n",
    "    random.shuffle(train_examples)\n",
    "    random.shuffle(val_examples)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Transformation Summary:\")\n",
    "    print(f\"   Refined dataset:         {stats['refined_train']} train + {stats['refined_val']} val\")\n",
    "    print(f\"   Real conversations:      {stats['real_train']} train + {stats['real_val']} val\")\n",
    "    print(f\"   Skipped:                 {stats['skipped']}\")\n",
    "    print(f\"   ‚ûú Combined Training:      {len(train_examples)} examples\")\n",
    "    print(f\"   ‚ûú Combined Validation:    {len(val_examples)} examples\")\n",
    "    print(f\"   ‚ûú Total:                 {len(train_examples) + len(val_examples)}\")\n",
    "    \n",
    "    return train_examples, val_examples\n",
    "\n",
    "\n",
    "# Load and combine both datasets\n",
    "print(\"Loading combined datasets...\")\n",
    "train_examples, val_examples = load_combined_dataset(str(refined_data_path), str(real_conv_path))\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"system\": [ex[\"system\"] for ex in train_examples],\n",
    "    \"user\": [ex[\"user\"] for ex in train_examples],\n",
    "    \"assistant\": [ex[\"assistant\"] for ex in train_examples],\n",
    "})\n",
    "\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"system\": [ex[\"system\"] for ex in val_examples],\n",
    "    \"user\": [ex[\"user\"] for ex in val_examples],\n",
    "    \"assistant\": [ex[\"assistant\"] for ex in val_examples],\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets ready:\")\n",
    "print(f\"   Train: {len(train_ds)} examples\")\n",
    "print(f\"   Validation: {len(val_ds)} examples\")\n",
    "\n",
    "if train_examples:\n",
    "    print(f\"\\nSample training example:\")\n",
    "    sample = train_examples[0]\n",
    "    print(f\"  User:      {sample['user']}...\")\n",
    "    print(f\"  Assistant: {sample['assistant']}...\")\n",
    "\n",
    "if val_examples:\n",
    "    print(f\"\\nSample validation example:\")\n",
    "    sample = val_examples[0]\n",
    "    print(f\"  User:      {sample['user']}...\")\n",
    "    print(f\"  Assistant: {sample['assistant']}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea44f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only, get_chat_template\n",
    "\n",
    "TRAIN_CONVERSATION = True\n",
    "RESPONSES_ONLY = False\n",
    "model = None\n",
    "tokenizer = None\n",
    "# Cell 4: Training helper\n",
    "def train_adapter(config, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    Train a LoRA or DoRA adapter on the refined poem dataset.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Training {config['name'].upper()} adapter...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "    if TRAIN_CONVERSATION:\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template = 'mistral',\n",
    "            map_eos_token = True\n",
    "        )\n",
    "        def format_row(row):\n",
    "            \"\"\"\n",
    "            Format a row into chat template.\n",
    "            Works with pre-loaded system/user/assistant fields.\n",
    "            \"\"\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": row[\"user\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"assistant\"]},\n",
    "            ]\n",
    "            convo = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "            return { 'text': convo }\n",
    "\n",
    "        # Format datasets\n",
    "        formatted_train_ds = train_dataset.map(format_row, batched=False)\n",
    "        formatted_val_ds = val_dataset.map(format_row, batched=False)\n",
    "    else:\n",
    "        alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "        \n",
    "        def formatting_prompts_func(rows):\n",
    "            instructions = rows[\"system\"]\n",
    "            inputs       = rows[\"user\"]\n",
    "            outputs      = rows[\"assistant\"]\n",
    "            texts = []\n",
    "            for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "                # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "                text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "                texts.append(text)\n",
    "            return { \"text\" : texts, }\n",
    "        \n",
    "        formatted_train_ds = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "        formatted_val_ds = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    # Apply PEFT (LoRA or DoRA)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=32,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        use_rslora=False,\n",
    "        use_dora=config[\"dora\"],\n",
    "    )\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=str(output_root / f\"{config['name']}_runs\"),\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=10,\n",
    "        save_total_limit=10,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        weight_decay = 0.001,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type='cosine',\n",
    "        logging_steps=5,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=formatted_train_ds,\n",
    "        eval_dataset=formatted_val_ds,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    if TRAIN_CONVERSATION and RESPONSES_ONLY:\n",
    "        instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\" if 'llama' in base_model_id else \"[INST]\"\n",
    "        response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" if 'llama' in base_model_id else \"[/INST]\"\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = instruction_part,\n",
    "            response_part = response_part,\n",
    "        )\n",
    "    print(f\"Training on {len(formatted_train_ds)} examples, validating on {len(formatted_val_ds)}...\")\n",
    "    stats = trainer.train()\n",
    "    print(stats)\n",
    "\n",
    "    adapter_dir = output_root / f\"{config['name']}_adapter\"\n",
    "    adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Saved {config['name']} adapter to {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Sampling datasets for testing...\n",
      "‚úÖ Sampled datasets:\n",
      "   Train: 7440 examples\n",
      "   Validation: 744 examples\n",
      "\n",
      "============================================================\n",
      "üöÄ Training LORA adapter...\n",
      "============================================================\n",
      "==((====))==  Unsloth 2026.2.1: Fast Mistral patching. Transformers: 4.57.6.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti. Num GPUs = 1. Max memory: 11.994 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.10.0+cu130. CUDA: 8.9. CUDA Toolkit: 13.0. Triton: 3.6.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954a010fd8c34531b65ea6111029db23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9714580903544d3bc9566761242c784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99867fa6e5304ae6bccdfc6eb9212cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2026.2.1 patched 40 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eac684383314e18a4124cab4b3a07e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/7440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3fcc8e6d934ba895780d7e7508b74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/744 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Padding-free auto-enabled, enabling faster training.\n",
      "Training on 7440 examples, validating on 744...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,440 | Num Epochs = 1 | Total steps = 930\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 114,032,640 of 12,361,815,040 (0.92% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [321/930 5:07:59 < 9:47:59, 0.02 it/s, Epoch 0.34/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.647100</td>\n",
       "      <td>2.252525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.960400</td>\n",
       "      <td>2.015999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.882400</td>\n",
       "      <td>1.997765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>1.970974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>1.945062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.820100</td>\n",
       "      <td>1.946886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.921758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>1.893474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.786000</td>\n",
       "      <td>1.904678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>1.901062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.774600</td>\n",
       "      <td>1.951648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.762900</td>\n",
       "      <td>1.946580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.785400</td>\n",
       "      <td>1.909637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.768100</td>\n",
       "      <td>1.911655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.762400</td>\n",
       "      <td>1.951923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>1.956064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.793700</td>\n",
       "      <td>1.968546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.761900</td>\n",
       "      <td>1.965762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.757100</td>\n",
       "      <td>1.957254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.777200</td>\n",
       "      <td>1.974937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.792100</td>\n",
       "      <td>1.977676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.772800</td>\n",
       "      <td>1.978731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.784800</td>\n",
       "      <td>1.933845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.761700</td>\n",
       "      <td>1.917107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>1.929537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.777400</td>\n",
       "      <td>1.987339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.720300</td>\n",
       "      <td>1.976550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.738800</td>\n",
       "      <td>1.927953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.731200</td>\n",
       "      <td>1.948184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>1.975616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>1.971088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='186' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 78/186 03:25 < 04:48, 0.37 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but MistralForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "# Set to None to use full datasets, or set to an integer to sample that many examples\n",
    "SAMPLE_SIZE = 7440  # e.g., 100 to use only 100 train + 20 val examples for quick testing\n",
    "\n",
    "if SAMPLE_SIZE is not None:\n",
    "    print(f\"üîç Sampling datasets for testing...\")\n",
    "    \n",
    "    # Sample training set\n",
    "    num_train_samples = SAMPLE_SIZE\n",
    "    sampled_train_indices = random.sample(range(len(train_ds)), min(num_train_samples, len(train_ds)))\n",
    "    train_ds = train_ds.select(sampled_train_indices)\n",
    "    \n",
    "    # Sample validation set (10% of training sample size)\n",
    "    num_val_samples = max(1, int(SAMPLE_SIZE * 0.1))\n",
    "    sampled_val_indices = random.sample(range(len(val_ds)), min(num_val_samples, len(val_ds)))\n",
    "    val_ds = val_ds.select(sampled_val_indices)\n",
    "    \n",
    "    print(f\"‚úÖ Sampled datasets:\")\n",
    "    print(f\"   Train: {len(train_ds)} examples\")\n",
    "    print(f\"   Validation: {len(val_ds)} examples\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using full datasets (no sampling)\")\n",
    "    print(f\"   Train: {len(train_ds)} examples\")\n",
    "    print(f\"   Validation: {len(val_ds)} examples\")\n",
    "\n",
    "\n",
    "# Cell 5: Run sequential A/B training\n",
    "# for cfg in configs:\n",
    "train_adapter(configs[0], train_ds, val_ds)  # type: ignore\n",
    "# chuck in a dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
