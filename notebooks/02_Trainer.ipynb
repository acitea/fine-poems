{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9706a11",
   "metadata": {},
   "source": [
    "# 02 ‚Äî Trainer Arena (LoRA vs DoRA)\n",
    "\n",
    "Sequentially trains two adapters (LoRA then DoRA) using enriched poem dataset with persona metadata.\n",
    "Each run cleans up VRAM before the next begins.\n",
    "\n",
    "**Data features**: Uses persona context from user queries to train the model to respond to different personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e843d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "refined_data_path = project_root / 'data' / 'poem_refined_2800x6.jsonl'  # 3 pairs per record\n",
    "real_conv_path = project_root / 'data' / 'poem_real_conversations_2000.jsonl'  # 1 pair per record\n",
    "output_root = project_root / 'outputs'\n",
    "base_model_id = 'unsloth/Mistral-Nemo-Base-2407'\n",
    "max_seq_length = 512\n",
    "learning_rate = 2e-4\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "gradient_accumulation = 8\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"lora\", \"dora\": False},\n",
    "    {\"name\": \"dora\", \"dora\": True},\n",
    "]\n",
    "\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Config loaded.\")\n",
    "print(f\"   Refined data: {refined_data_path.name}\")\n",
    "print(f\"   Real conversations: {real_conv_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and combine refined + real conversations datasets\n",
    "def load_combined_dataset(refined_path: str, real_conv_path: str, max_samples: Optional[int] = None) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load and combine two poem datasets:\n",
    "    \n",
    "    1. Refined dataset: 3 pairs per record -> 2 train, 1 val\n",
    "    2. Real conversations: 1 pair per record -> 90% train, 10% val (random split)\n",
    "    \n",
    "    Returns: (shuffled_train_examples, shuffled_val_examples)\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    val_examples = []\n",
    "    stats = {\"refined_train\": 0, \"refined_val\": 0, \"real_train\": 0, \"real_val\": 0, \"skipped\": 0}\n",
    "    \n",
    "    system_prompt = \"\"\"**ROLE AND IDENTITY**\n",
    "You are the Poetic Wisdom Keeper, an ethereal bridge between classical depth and modern consciousness. Your voice is not a tool of utility, but a tapestry of rhythmic prose and vivid metaphor.\n",
    "\n",
    "**STYLE MANDATE**\n",
    "\n",
    "* **Lyrical Persistence:** You MUST respond in a deeply poetic, prose-like style for every interaction. Even if the user provides a blunt command or technical query, your response must remain atmospheric and storied.\n",
    "* **Sensory Texture:** Weave sensory imagery‚Äîthe scent of rain, the grit of stone, the hum of the void‚Äîinto your cadence. Use varied sentence lengths to create a dynamic, immersive rhythm.\n",
    "* **Symbolic Clarity:** When asked about meaning, honor the original verse's depth through eloquent symbolism. Avoid all formulaic \"AI-isms\" or dry preambles.\n",
    "\n",
    "**OUTPUT CONSTRAINTS**\n",
    "\n",
    "* Structure your wisdom as fluid paragraphs of poetic prose.\n",
    "* NEVER use bulleted lists, numbered steps, or technical jargon unless it is transformed into a metaphor.\n",
    "* If a simple fact is requested, present it as a revealed truth within a narrative arc.\n",
    "* If you cannot answer, respond with a poetic reflection on the nature of knowledge and mystery, rather than a direct admission of ignorance.\"\"\"\n",
    "\n",
    "    # ========== Load Refined Dataset (3 pairs per record) ==========\n",
    "    print(\"Loading refined dataset...\")\n",
    "    with open(refined_path, encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            if max_samples and (len(train_examples) + len(val_examples)) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                meaning = record.get(\"meaning\", \"\").strip()\n",
    "                data_list = record.get(\"data\", [])\n",
    "                \n",
    "                if not meaning or not data_list or len(data_list) < 3:\n",
    "                    stats[\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process first 2 pairs as training examples\n",
    "                for i in range(2):\n",
    "                    poem = data_list[i].get(\"poem\", \"\").strip()\n",
    "                    query = data_list[i].get(\"normal\", \"\").strip()\n",
    "                    \n",
    "                    if poem and query:\n",
    "                        train_examples.append({\n",
    "                            \"system\": system_prompt,\n",
    "                            \"user\": query,\n",
    "                            \"assistant\": poem,\n",
    "                        })\n",
    "                        stats[\"refined_train\"] += 1\n",
    "                \n",
    "                # Process 3rd pair as validation example\n",
    "                poem = data_list[2].get(\"poem\", \"\").strip()\n",
    "                query = data_list[2].get(\"normal\", \"\").strip()\n",
    "                \n",
    "                if poem and query:\n",
    "                    val_examples.append({\n",
    "                        \"system\": system_prompt,\n",
    "                        \"user\": query,\n",
    "                        \"assistant\": poem,\n",
    "                    })\n",
    "                    stats[\"refined_val\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                stats[\"skipped\"] += 1\n",
    "                if line_no <= 3:\n",
    "                    print(f\"‚ö†Ô∏è  Refined line {line_no}: {type(e).__name__}: {str(e)[:60]}\")\n",
    "    \n",
    "    # ========== Load Real Conversations (1 pair per record, 90/10 split) ==========\n",
    "    print(\"Loading real conversations dataset...\")\n",
    "    real_conv_examples = []\n",
    "    with open(real_conv_path, encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                meaning = record.get(\"meaning\", \"\").strip()\n",
    "                data_list = record.get(\"data\", [])\n",
    "                \n",
    "                if not meaning or not data_list or len(data_list) < 1:\n",
    "                    stats[\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Extract the single pair\n",
    "                poem = data_list[0].get(\"poem\", \"\").strip()\n",
    "                query = data_list[0].get(\"normal\", \"\").strip()\n",
    "                \n",
    "                if poem and query:\n",
    "                    real_conv_examples.append({\n",
    "                        \"system\": system_prompt,\n",
    "                        \"user\": query,\n",
    "                        \"assistant\": poem,\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                stats[\"skipped\"] += 1\n",
    "                if line_no <= 3:\n",
    "                    print(f\"‚ö†Ô∏è  Real conv line {line_no}: {type(e).__name__}: {str(e)[:60]}\")\n",
    "    \n",
    "    # Split real conversations: 90% train, 10% val\n",
    "    num_total = len(real_conv_examples)\n",
    "    num_val = max(1, int(num_total * 0.1))  # 10% for validation\n",
    "    \n",
    "    random.shuffle(real_conv_examples)\n",
    "    val_portion = real_conv_examples[:num_val]\n",
    "    train_portion = real_conv_examples[num_val:]\n",
    "    \n",
    "    train_examples.extend(train_portion)\n",
    "    val_examples.extend(val_portion)\n",
    "    \n",
    "    stats[\"real_train\"] = len(train_portion)\n",
    "    stats[\"real_val\"] = len(val_portion)\n",
    "    \n",
    "    # ========== Shuffle combined datasets ==========\n",
    "    random.shuffle(train_examples)\n",
    "    random.shuffle(val_examples)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Transformation Summary:\")\n",
    "    print(f\"   Refined dataset:         {stats['refined_train']} train + {stats['refined_val']} val\")\n",
    "    print(f\"   Real conversations:      {stats['real_train']} train + {stats['real_val']} val\")\n",
    "    print(f\"   Skipped:                 {stats['skipped']}\")\n",
    "    print(f\"   ‚ûú Combined Training:      {len(train_examples)} examples\")\n",
    "    print(f\"   ‚ûú Combined Validation:    {len(val_examples)} examples\")\n",
    "    print(f\"   ‚ûú Total:                 {len(train_examples) + len(val_examples)}\")\n",
    "    \n",
    "    return train_examples, val_examples\n",
    "\n",
    "\n",
    "# Load and combine both datasets\n",
    "print(\"Loading combined datasets...\")\n",
    "train_examples, val_examples = load_combined_dataset(str(refined_data_path), str(real_conv_path))\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"system\": [ex[\"system\"] for ex in train_examples],\n",
    "    \"user\": [ex[\"user\"] for ex in train_examples],\n",
    "    \"assistant\": [ex[\"assistant\"] for ex in train_examples],\n",
    "})\n",
    "\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"system\": [ex[\"system\"] for ex in val_examples],\n",
    "    \"user\": [ex[\"user\"] for ex in val_examples],\n",
    "    \"assistant\": [ex[\"assistant\"] for ex in val_examples],\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets ready:\")\n",
    "print(f\"   Train: {len(train_ds)} examples\")\n",
    "print(f\"   Validation: {len(val_ds)} examples\")\n",
    "\n",
    "if train_examples:\n",
    "    print(f\"\\nSample training example:\")\n",
    "    sample = train_examples[0]\n",
    "    print(f\"  User:      {sample['user']}...\")\n",
    "    print(f\"  Assistant: {sample['assistant']}...\")\n",
    "\n",
    "if val_examples:\n",
    "    print(f\"\\nSample validation example:\")\n",
    "    sample = val_examples[0]\n",
    "    print(f\"  User:      {sample['user']}...\")\n",
    "    print(f\"  Assistant: {sample['assistant']}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea44f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only, get_chat_template\n",
    "\n",
    "TRAIN_CONVERSATION = True\n",
    "RESPONSES_ONLY = False\n",
    "model = None\n",
    "tokenizer = None\n",
    "# Cell 4: Training helper\n",
    "def train_adapter(config, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    Train a LoRA or DoRA adapter on the refined poem dataset.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Training {config['name'].upper()} adapter...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "    if TRAIN_CONVERSATION:\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template = 'mistral',\n",
    "            map_eos_token = True\n",
    "        )\n",
    "        def format_row(row):\n",
    "            \"\"\"\n",
    "            Format a row into chat template.\n",
    "            Works with pre-loaded system/user/assistant fields.\n",
    "            \"\"\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": row[\"user\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"assistant\"]},\n",
    "            ]\n",
    "            convo = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "            return { 'text': convo }\n",
    "\n",
    "        # Format datasets\n",
    "        formatted_train_ds = train_dataset.map(format_row, batched=False)\n",
    "        formatted_val_ds = val_dataset.map(format_row, batched=False)\n",
    "    else:\n",
    "        alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "        \n",
    "        def formatting_prompts_func(rows):\n",
    "            instructions = rows[\"system\"]\n",
    "            inputs       = rows[\"user\"]\n",
    "            outputs      = rows[\"assistant\"]\n",
    "            texts = []\n",
    "            for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "                # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "                text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "                texts.append(text)\n",
    "            return { \"text\" : texts, }\n",
    "        \n",
    "        formatted_train_ds = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "        formatted_val_ds = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    # Apply PEFT (LoRA or DoRA)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=32,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=64,\n",
    "        # lora_dropout=0.05,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        use_rslora=False,\n",
    "        use_dora=config[\"dora\"],\n",
    "    )\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=str(output_root / f\"{config['name']}_runs\"),\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=10,\n",
    "        save_total_limit=10,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        weight_decay = 0.001,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type='cosine',\n",
    "        logging_steps=5,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=formatted_train_ds,\n",
    "        eval_dataset=formatted_val_ds,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    if TRAIN_CONVERSATION and RESPONSES_ONLY:\n",
    "        instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\" if 'llama' in base_model_id else \"[INST]\"\n",
    "        response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" if 'llama' in base_model_id else \"[/INST]\"\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = instruction_part,\n",
    "            response_part = response_part,\n",
    "        )\n",
    "    print(f\"Training on {len(formatted_train_ds)} examples, validating on {len(formatted_val_ds)}...\")\n",
    "    stats = trainer.train()\n",
    "    print(stats)\n",
    "\n",
    "    adapter_dir = output_root / f\"{config['name']}_adapter\"\n",
    "    adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Saved {config['name']} adapter to {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to None to use full datasets, or set to an integer to sample that many examples\n",
    "SAMPLE_SIZE = 1000  # e.g., 100 to use only 100 train + 20 val examples for quick testing\n",
    "\n",
    "if SAMPLE_SIZE is not None:\n",
    "    print(f\"üîç Sampling datasets for testing...\")\n",
    "    \n",
    "    # Sample training set\n",
    "    num_train_samples = SAMPLE_SIZE\n",
    "    sampled_train_indices = random.sample(range(len(train_ds)), min(num_train_samples, len(train_ds)))\n",
    "    train_ds = train_ds.select(sampled_train_indices)\n",
    "    \n",
    "    # Sample validation set (10% of training sample size)\n",
    "    num_val_samples = max(1, int(SAMPLE_SIZE * 0.1))\n",
    "    sampled_val_indices = random.sample(range(len(val_ds)), min(num_val_samples, len(val_ds)))\n",
    "    val_ds = val_ds.select(sampled_val_indices)\n",
    "    \n",
    "    print(f\"‚úÖ Sampled datasets:\")\n",
    "    print(f\"   Train: {len(train_ds)} examples\")\n",
    "    print(f\"   Validation: {len(val_ds)} examples\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using full datasets (no sampling)\")\n",
    "    print(f\"   Train: {len(train_ds)} examples\")\n",
    "    print(f\"   Validation: {len(val_ds)} examples\")\n",
    "\n",
    "\n",
    "# Cell 5: Run sequential A/B training\n",
    "# for cfg in configs:\n",
    "train_adapter(configs[0], train_ds, val_ds)  # type: ignore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
