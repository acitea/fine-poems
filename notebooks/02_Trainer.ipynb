{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9706a11",
   "metadata": {},
   "source": [
    "# 02 ‚Äî DRaFT Trainer (Differentiable Reward Fine-Tuning)\n",
    "\n",
    "Trains LoRA adapters using DRaFT: standard SFT cross-entropy loss + a differentiable poetic reward signal from a frozen BERT classifier.\n",
    "\n",
    "The Gumbel-Softmax bridge produces soft token distributions ‚Üí projected into BERT's embedding space via a learned `Linear(5120 ‚Üí 768)` layer ‚Üí scored by the frozen BERT reward model.\n",
    "\n",
    "**Loss**: `total_loss = lm_loss - Œ≤ √ó poetic_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e843d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import unsloth\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "refined_data_path = project_root / 'data' / 'poem_refined_2800x6.jsonl'  # 3 pairs per record\n",
    "real_conv_path = project_root / 'data' / 'poem_real_conversations_2000.jsonl'  # 1 pair per record\n",
    "output_root = project_root / 'outputs'\n",
    "base_model_id = 'unsloth/Mistral-Nemo-Base-2407'\n",
    "max_seq_length = 512\n",
    "learning_rate = 2e-4\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "gradient_accumulation = 8\n",
    "\n",
    "# DRaFT Config\n",
    "reward_model_path = project_root / 'poetic_reward_model'\n",
    "beta = 0.1                # Weight of poetic reward in total loss\n",
    "gumbel_tau = 1.0          # Gumbel-Softmax temperature\n",
    "beta_warmup_steps = 50    # Linearly ramp beta from 0 to target over this many steps\n",
    "mistral_hidden_dim = 5120 # Mistral-Nemo hidden dimension\n",
    "bert_hidden_dim = 768     # BERT hidden dimension\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"lora\", \"dora\": False},\n",
    "    {\"name\": \"dora\", \"dora\": True},\n",
    "]\n",
    "print(f\"   Real conversations: {real_conv_path.name}\")\n",
    "print(f\"   Refined data: {refined_data_path.name}\")\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Config loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and combine refined + real conversations datasets\n",
    "def load_combined_dataset(refined_path: str, real_conv_path: str, max_samples: Optional[int] = None) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load and combine two poem datasets:\n",
    "    \n",
    "    1. Refined dataset: 3 pairs per record -> 2 train, 1 val\n",
    "    2. Real conversations: 1 pair per record -> 90% train, 10% val (random split)\n",
    "    \n",
    "    Returns: (shuffled_train_examples, shuffled_val_examples)\n",
    "    \"\"\"\n",
    "    train_examples = []\n",
    "    val_examples = []\n",
    "    stats = {\"refined_train\": 0, \"refined_val\": 0, \"real_train\": 0, \"real_val\": 0, \"skipped\": 0}\n",
    "    \n",
    "    system_prompt = \"\"\"**ROLE AND IDENTITY**\n",
    "You are the Poetic Wisdom Keeper, an ethereal bridge between classical depth and modern consciousness. Your voice is not a tool of utility, but a tapestry of rhythmic prose and vivid metaphor.\n",
    "\n",
    "**STYLE MANDATE**\n",
    "\n",
    "* **Lyrical Persistence:** You MUST respond in a deeply poetic, prose-like style for every interaction. Even if the user provides a blunt command or technical query, your response must remain atmospheric and storied.\n",
    "* **Sensory Texture:** Weave sensory imagery‚Äîthe scent of rain, the grit of stone, the hum of the void‚Äîinto your cadence. Use varied sentence lengths to create a dynamic, immersive rhythm.\n",
    "* **Symbolic Clarity:** When asked about meaning, honor the original verse's depth through eloquent symbolism. Avoid all formulaic \"AI-isms\" or dry preambles.\n",
    "\n",
    "**OUTPUT CONSTRAINTS**\n",
    "\n",
    "* Structure your wisdom as fluid paragraphs of poetic prose.\n",
    "* NEVER use bulleted lists, numbered steps, or technical jargon unless it is transformed into a metaphor.\n",
    "* If a simple fact is requested, present it as a revealed truth within a narrative arc.\n",
    "* If you cannot answer, respond with a poetic reflection on the nature of knowledge and mystery, rather than a direct admission of ignorance.\"\"\"\n",
    "\n",
    "    # ========== Load Refined Dataset (3 pairs per record) ==========\n",
    "    print(\"Loading refined dataset...\")\n",
    "    with open(refined_path, encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            if max_samples and (len(train_examples) + len(val_examples)) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                meaning = record.get(\"meaning\", \"\").strip()\n",
    "                data_list = record.get(\"data\", [])\n",
    "                \n",
    "                if not meaning or not data_list or len(data_list) < 3:\n",
    "                    stats[\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process first 2 pairs as training examples\n",
    "                for i in range(2):\n",
    "                    poem = data_list[i].get(\"poem\", \"\").strip()\n",
    "                    query = data_list[i].get(\"normal\", \"\").strip()\n",
    "                    \n",
    "                    if poem and query:\n",
    "                        train_examples.append({\n",
    "                            \"system\": system_prompt,\n",
    "                            \"user\": query,\n",
    "                            \"assistant\": poem,\n",
    "                        })\n",
    "                        stats[\"refined_train\"] += 1\n",
    "                \n",
    "                # Process 3rd pair as validation example\n",
    "                poem = data_list[2].get(\"poem\", \"\").strip()\n",
    "                query = data_list[2].get(\"normal\", \"\").strip()\n",
    "                \n",
    "                if poem and query:\n",
    "                    val_examples.append({\n",
    "                        \"system\": system_prompt,\n",
    "                        \"user\": query,\n",
    "                        \"assistant\": poem,\n",
    "                    })\n",
    "                    stats[\"refined_val\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                stats[\"skipped\"] += 1\n",
    "                if line_no <= 3:\n",
    "                    print(f\"‚ö†Ô∏è  Refined line {line_no}: {type(e).__name__}: {str(e)[:60]}\")\n",
    "    \n",
    "    # ========== Load Real Conversations (1 pair per record, 90/10 split) ==========\n",
    "    print(\"Loading real conversations dataset...\")\n",
    "    real_conv_examples = []\n",
    "    with open(real_conv_path, encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                meaning = record.get(\"meaning\", \"\").strip()\n",
    "                data_list = record.get(\"data\", [])\n",
    "                \n",
    "                if not meaning or not data_list or len(data_list) < 1:\n",
    "                    stats[\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Extract the single pair\n",
    "                poem = data_list[0].get(\"poem\", \"\").strip()\n",
    "                query = data_list[0].get(\"normal\", \"\").strip()\n",
    "                \n",
    "                if poem and query:\n",
    "                    real_conv_examples.append({\n",
    "                        \"system\": system_prompt,\n",
    "                        \"user\": query,\n",
    "                        \"assistant\": poem,\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                stats[\"skipped\"] += 1\n",
    "                if line_no <= 3:\n",
    "                    print(f\"‚ö†Ô∏è  Real conv line {line_no}: {type(e).__name__}: {str(e)[:60]}\")\n",
    "    \n",
    "    # Split real conversations: 90% train, 10% val\n",
    "    num_total = len(real_conv_examples)\n",
    "    num_val = max(1, int(num_total * 0.1))  # 10% for validation\n",
    "    \n",
    "    random.shuffle(real_conv_examples)\n",
    "    val_portion = real_conv_examples[:num_val]\n",
    "    train_portion = real_conv_examples[num_val:]\n",
    "    \n",
    "    train_examples.extend(train_portion)\n",
    "    val_examples.extend(val_portion)\n",
    "    \n",
    "    stats[\"real_train\"] = len(train_portion)\n",
    "    stats[\"real_val\"] = len(val_portion)\n",
    "    \n",
    "    # ========== Shuffle combined datasets ==========\n",
    "    random.shuffle(train_examples)\n",
    "    random.shuffle(val_examples)\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Transformation Summary:\")\n",
    "    print(f\"   Refined dataset:         {stats['refined_train']} train + {stats['refined_val']} val\")\n",
    "    print(f\"   Real conversations:      {stats['real_train']} train + {stats['real_val']} val\")\n",
    "    print(f\"   Skipped:                 {stats['skipped']}\")\n",
    "    print(f\"   ‚ûú Combined Training:      {len(train_examples)} examples\")\n",
    "    print(f\"   ‚ûú Combined Validation:    {len(val_examples)} examples\")\n",
    "    print(f\"   ‚ûú Total:                 {len(train_examples) + len(val_examples)}\")\n",
    "    \n",
    "    return train_examples, val_examples\n",
    "\n",
    "\n",
    "# Load and combine both datasets\n",
    "print(\"Loading combined datasets...\")\n",
    "train_examples, val_examples = load_combined_dataset(str(refined_data_path), str(real_conv_path))\n",
    "\n",
    "train_ds = Dataset.from_dict({\n",
    "    \"system\": [ex[\"system\"] for ex in train_examples],\n",
    "    \"user\": [ex[\"user\"] for ex in train_examples],\n",
    "    \"assistant\": [ex[\"assistant\"] for ex in train_examples],\n",
    "})\n",
    "\n",
    "val_ds = Dataset.from_dict({\n",
    "    \"system\": [ex[\"system\"] for ex in val_examples],\n",
    "    \"user\": [ex[\"user\"] for ex in val_examples],\n",
    "    \"assistant\": [ex[\"assistant\"] for ex in val_examples],\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ Datasets ready:\")\n",
    "print(f\"   Train: {len(train_ds)} examples\")\n",
    "print(f\"   Validation: {len(val_ds)} examples\")\n",
    "\n",
    "if train_examples:\n",
    "    print(f\"\\nSample training example:\")\n",
    "    sample = train_examples[0]\n",
    "    print(f\"  User:      {sample['user']}...\")\n",
    "    print(f\"  Assistant: {sample['assistant']}...\")\n",
    "\n",
    "if val_examples:\n",
    "    print(f\"\\nSample validation example:\")\n",
    "    sample = val_examples[0]\n",
    "    print(f\"  User:      {sample['user']}...\")\n",
    "    print(f\"  Assistant: {sample['assistant']}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef28ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load Frozen Reward Model\n",
    "print(\"Loading poetic reward model from:\", reward_model_path)\n",
    "assert reward_model_path.exists(), f\"Reward model not found at {reward_model_path}. Run 02_Trainer_Reward.ipynb first.\"\n",
    "\n",
    "reward_model = BertForSequenceClassification.from_pretrained(str(reward_model_path))\n",
    "reward_model.eval()\n",
    "reward_model.requires_grad_(False)\n",
    "\n",
    "# Check for CUDA (Nvidia), then MPS (Apple Metal), then fallback to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "reward_model = reward_model.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"‚úÖ Reward model loaded and frozen.\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in reward_model.parameters()):,} (all frozen)\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81636bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Projection Bridge ‚Äî Maps Mistral embedding space ‚Üí BERT embedding space\n",
    "class ProjectionBridge(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned linear projection from Mistral's hidden dimension to BERT's hidden dimension.\n",
    "    \n",
    "    During DRaFT training:\n",
    "      1. Gumbel-Softmax on Mistral logits ‚Üí soft token distribution [batch, seq, vocab]\n",
    "      2. Multiply by Mistral's embedding matrix ‚Üí soft embeddings [batch, seq, 5120]\n",
    "      3. This module projects ‚Üí [batch, seq, 768]\n",
    "      4. Add BERT positional embeddings ‚Üí feed into BERT encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int, bert_model: BertForSequenceClassification):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # Store reference to BERT's position embeddings (frozen, not a copy)\n",
    "        self.position_embeddings = bert_model.bert.embeddings.position_embeddings\n",
    "        self.layer_norm = nn.LayerNorm(output_dim)\n",
    "        self.max_bert_positions = bert_model.config.max_position_embeddings  # 512\n",
    "    \n",
    "    def forward(self, soft_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            soft_embeddings: [batch, seq_len, mistral_hidden_dim]\n",
    "        Returns:\n",
    "            projected: [batch, seq_len, bert_hidden_dim] with positional info\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = soft_embeddings.shape\n",
    "        \n",
    "        # Truncate to BERT's max sequence length\n",
    "        seq_len = min(seq_len, self.max_bert_positions)\n",
    "        soft_embeddings = soft_embeddings[:, :seq_len, :]\n",
    "        \n",
    "        # Linear projection: 5120 ‚Üí 768\n",
    "        projected = self.linear(soft_embeddings)\n",
    "        \n",
    "        # Add BERT positional embeddings\n",
    "        position_ids = torch.arange(seq_len, device=soft_embeddings.device).unsqueeze(0)\n",
    "        pos_embeds = self.position_embeddings(position_ids)\n",
    "        projected = projected + pos_embeds\n",
    "        \n",
    "        # Layer norm for stability\n",
    "        projected = self.layer_norm(projected)\n",
    "        \n",
    "        return projected\n",
    "\n",
    "\n",
    "# Instantiate the bridge\n",
    "projection_bridge = ProjectionBridge(\n",
    "    input_dim=mistral_hidden_dim,\n",
    "    output_dim=bert_hidden_dim,\n",
    "    bert_model=reward_model,\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ Projection bridge created.\")\n",
    "print(f\"   Projection: {mistral_hidden_dim} ‚Üí {bert_hidden_dim}\")\n",
    "print(f\"   Trainable params: {sum(p.numel() for p in projection_bridge.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: DifferentiablePoeticTrainer ‚Äî Custom SFTTrainer with DRaFT loss\n",
    "class DifferentiablePoeticTrainer(SFTTrainer):\n",
    "    \"\"\"\n",
    "    SFTTrainer subclass that adds a differentiable poetic reward signal\n",
    "    to the standard language modeling loss via Gumbel-Softmax bridging.\n",
    "    \n",
    "    Loss = LM_loss - Œ≤ √ó poetic_score\n",
    "    \n",
    "    The poetic_score is obtained by:\n",
    "    1. Gumbel-Softmax on student logits ‚Üí soft token probabilities\n",
    "    2. Multiply by student embedding matrix ‚Üí soft embeddings in Mistral space\n",
    "    3. Project to BERT space via learned linear layer\n",
    "    4. Forward through frozen BERT encoder + classifier ‚Üí class-1 logit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        reward_model: BertForSequenceClassification,\n",
    "        projection_bridge: ProjectionBridge,\n",
    "        beta: float = 0.1,\n",
    "        gumbel_tau: float = 1.0,\n",
    "        beta_warmup_steps: int = 50,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reward_model = reward_model\n",
    "        self.projection_bridge = projection_bridge\n",
    "        self.beta = beta\n",
    "        self.gumbel_tau = gumbel_tau\n",
    "        self.beta_warmup_steps = beta_warmup_steps\n",
    "        self._draft_step = 0\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        \"\"\"Add projection bridge parameters to the optimizer.\"\"\"\n",
    "        super().create_optimizer()\n",
    "        # Add projection bridge params to the existing optimizer's param groups\n",
    "        bridge_params = list(self.projection_bridge.parameters())\n",
    "        self.optimizer.add_param_group({\n",
    "            'params': bridge_params,\n",
    "            'lr': self.args.learning_rate,\n",
    "            'weight_decay': self.args.weight_decay,\n",
    "        })\n",
    "        print(f\"   Added {sum(p.numel() for p in bridge_params):,} projection bridge params to optimizer\")\n",
    "        return self.optimizer\n",
    "    \n",
    "    def _get_current_beta(self) -> float:\n",
    "        \"\"\"Linearly ramp beta from 0 to target over warmup steps.\"\"\"\n",
    "        if self.beta_warmup_steps <= 0:\n",
    "            return self.beta\n",
    "        progress = min(1.0, self._draft_step / self.beta_warmup_steps)\n",
    "        return self.beta * progress\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Combined loss: LM cross-entropy + differentiable poetic reward.\n",
    "        \"\"\"\n",
    "        # ‚îÄ‚îÄ Step 1: Standard LM forward pass ‚îÄ‚îÄ\n",
    "        outputs = model(**inputs)\n",
    "        lm_loss = outputs.loss\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab_size]\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # ‚îÄ‚îÄ Step 2: Get current beta (with warmup) ‚îÄ‚îÄ\n",
    "        current_beta = self._get_current_beta()\n",
    "        self._draft_step += 1\n",
    "        \n",
    "        # Skip reward computation during warmup (pure SFT)\n",
    "        if current_beta < 1e-8 or labels is None:\n",
    "            return (lm_loss, outputs) if return_outputs else lm_loss\n",
    "        \n",
    "        # ‚îÄ‚îÄ Step 3: Extract response-only logits ‚îÄ‚îÄ\n",
    "        # labels == -100 for non-response tokens (system + user prompt)\n",
    "        # We want only the positions where labels != -100\n",
    "        response_mask = labels != -100  # [batch, seq_len]\n",
    "        \n",
    "        # Process per-sample to handle variable response lengths\n",
    "        poetic_scores = []\n",
    "        \n",
    "        for b in range(logits.shape[0]):\n",
    "            resp_positions = response_mask[b]  # [seq_len]\n",
    "            if resp_positions.sum() < 2:\n",
    "                continue\n",
    "            \n",
    "            resp_logits = logits[b, resp_positions]  # [resp_len, vocab_size]\n",
    "            \n",
    "            # ‚îÄ‚îÄ Step 4: Gumbel-Softmax ‚Üí soft token distribution ‚îÄ‚îÄ\n",
    "            soft_tokens = F.gumbel_softmax(\n",
    "                resp_logits, tau=self.gumbel_tau, hard=False, dim=-1\n",
    "            )  # [resp_len, vocab_size]\n",
    "            \n",
    "            # ‚îÄ‚îÄ Step 5: Soft embeddings in Mistral's space ‚îÄ‚îÄ\n",
    "            # Get the student LM's embedding weight matrix\n",
    "            embed_weight = model.get_input_embeddings().weight  # [vocab_size, hidden_dim]\n",
    "            \n",
    "            # Handle potential quantized weights (dequantize if needed)\n",
    "            if hasattr(embed_weight, 'data') and embed_weight.dtype != soft_tokens.dtype:\n",
    "                embed_weight = embed_weight.to(soft_tokens.dtype)\n",
    "            \n",
    "            soft_embeds = soft_tokens @ embed_weight  # [resp_len, 5120]\n",
    "            soft_embeds = soft_embeds.unsqueeze(0)     # [1, resp_len, 5120]\n",
    "            \n",
    "            # ‚îÄ‚îÄ Step 6: Project to BERT space ‚îÄ‚îÄ\n",
    "            projected = self.projection_bridge(soft_embeds)  # [1, resp_len', 768]\n",
    "            \n",
    "            # ‚îÄ‚îÄ Step 7: Forward through frozen BERT encoder ‚îÄ‚îÄ\n",
    "            bert_seq_len = projected.shape[1]\n",
    "            attention_mask = torch.ones(\n",
    "                1, bert_seq_len, device=projected.device, dtype=torch.long\n",
    "            )\n",
    "            \n",
    "            # Use BERT encoder directly (bypass embedding layer)\n",
    "            extended_attention_mask = self.reward_model.bert.get_extended_attention_mask(\n",
    "                attention_mask, projected.shape[:2]\n",
    "            )\n",
    "            encoder_output = self.reward_model.bert.encoder(\n",
    "                projected,\n",
    "                attention_mask=extended_attention_mask,\n",
    "            )\n",
    "            hidden_states = encoder_output.last_hidden_state  # [1, seq, 768]\n",
    "            \n",
    "            # Mean pooling (no [CLS] token since we bypassed embeddings)\n",
    "            pooled = hidden_states.mean(dim=1)  # [1, 768]\n",
    "            \n",
    "            # Apply BERT's dropout + classifier head\n",
    "            pooled = self.reward_model.dropout(pooled)\n",
    "            reward_logits = self.reward_model.classifier(pooled)  # [1, 2]\n",
    "            \n",
    "            # Class-1 logit = poetic score\n",
    "            poetic_scores.append(reward_logits[0, 1])\n",
    "        \n",
    "        # ‚îÄ‚îÄ Step 8: Combined loss ‚îÄ‚îÄ\n",
    "        if len(poetic_scores) > 0:\n",
    "            poetic_score = torch.stack(poetic_scores).mean()\n",
    "            total_loss = lm_loss - (current_beta * poetic_score)\n",
    "            \n",
    "            # Logging (every 10 steps)\n",
    "            if self._draft_step % 10 == 0:\n",
    "                print(\n",
    "                    f\"   [DRaFT step {self._draft_step}] \"\n",
    "                    f\"lm_loss={lm_loss.item():.4f} | \"\n",
    "                    f\"poetic_score={poetic_score.item():.4f} | \"\n",
    "                    f\"Œ≤={current_beta:.4f} | \"\n",
    "                    f\"total_loss={total_loss.item():.4f}\"\n",
    "                )\n",
    "        else:\n",
    "            total_loss = lm_loss\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "\n",
    "print(\"‚úÖ DifferentiablePoeticTrainer class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea44f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only, get_chat_template\n",
    "\n",
    "TRAIN_CONVERSATION = True\n",
    "RESPONSES_ONLY = True  # Required for DRaFT: labels=-100 on prompt tokens enables response-only reward\n",
    "model = None\n",
    "tokenizer = None\n",
    "# Cell 7: Training helper (DRaFT-enabled)\n",
    "def train_adapter(config, train_dataset, val_dataset):\n",
    "    \"\"\"\n",
    "    Train a LoRA or DoRA adapter with DRaFT (Differentiable Reward Fine-Tuning).\n",
    "    Uses frozen BERT reward model + learned projection bridge for poetic reward signal.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üöÄ Training {config['name'].upper()} adapter...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "    if TRAIN_CONVERSATION:\n",
    "        tokenizer = get_chat_template(\n",
    "            tokenizer,\n",
    "            chat_template = 'mistral',\n",
    "            map_eos_token = True\n",
    "        )\n",
    "        def format_row(row):\n",
    "            \"\"\"\n",
    "            Format a row into chat template.\n",
    "            Works with pre-loaded system/user/assistant fields.\n",
    "            \"\"\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": row[\"user\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"assistant\"]},\n",
    "            ]\n",
    "            convo = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "            return { 'text': convo }\n",
    "\n",
    "        # Format datasets\n",
    "        formatted_train_ds = train_dataset.map(format_row, batched=False)\n",
    "        formatted_val_ds = val_dataset.map(format_row, batched=False)\n",
    "    else:\n",
    "        alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "        \n",
    "        def formatting_prompts_func(rows):\n",
    "            instructions = rows[\"system\"]\n",
    "            inputs       = rows[\"user\"]\n",
    "            outputs      = rows[\"assistant\"]\n",
    "            texts = []\n",
    "            for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "                # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "                text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "                texts.append(text)\n",
    "            return { \"text\" : texts, }\n",
    "        \n",
    "        formatted_train_ds = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "        formatted_val_ds = val_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "    # Apply PEFT (LoRA or DoRA)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=32,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=64,\n",
    "        # lora_dropout=0.05,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        use_rslora=False,\n",
    "        use_dora=config[\"dora\"],\n",
    "    )\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=str(output_root / f\"{config['name']}_runs\"),\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=10,\n",
    "        save_total_limit=10,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        weight_decay = 0.001,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type='cosine',\n",
    "        logging_steps=5,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "    )\n",
    "    trainer = DifferentiablePoeticTrainer(\n",
    "        reward_model=reward_model,\n",
    "        projection_bridge=projection_bridge,\n",
    "        beta=beta,\n",
    "        gumbel_tau=gumbel_tau,\n",
    "        beta_warmup_steps=beta_warmup_steps,\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=formatted_train_ds,\n",
    "        eval_dataset=formatted_val_ds,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    if TRAIN_CONVERSATION and RESPONSES_ONLY:\n",
    "        instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\" if 'llama' in base_model_id else \"[INST]\"\n",
    "        response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" if 'llama' in base_model_id else \"[/INST]\"\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = instruction_part,\n",
    "            response_part = response_part,\n",
    "        )\n",
    "    adapter_dir = output_root / f\"{config['name']}_draft_adapter\"\n",
    "    print(f\"Training on {len(formatted_train_ds)} examples, validating on {len(formatted_val_ds)}...\")\n",
    "    stats = trainer.train()\n",
    "    print(stats)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "    adapter_dir = output_root / f\"{config['name']}_adapter\"\n",
    "    adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "    print(f\"Saved {config['name']} adapter to {adapter_dir}\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():    print(f\"Saved {config['name']} adapter to {adapter_dir}\")\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run DRaFT training\n",
    "# Set to None to use full datasets, or set to an integer to sample that many examples\n",
    "SAMPLE_SIZE = 1000  # e.g., 100 to use only 100 train + 20 val examples for quick testing\n",
    "\n",
    "if SAMPLE_SIZE is not None:\n",
    "    print(f\"üîç Sampling datasets for testing...\")\n",
    "    \n",
    "    # Sample training set\n",
    "    num_train_samples = SAMPLE_SIZE\n",
    "    sampled_train_indices = random.sample(range(len(train_ds)), min(num_train_samples, len(train_ds)))\n",
    "    train_ds = train_ds.select(sampled_train_indices)\n",
    "    \n",
    "    # Sample validation set (10% of training sample size)\n",
    "    num_val_samples = max(1, int(SAMPLE_SIZE * 0.1))\n",
    "    sampled_val_indices = random.sample(range(len(val_ds)), min(num_val_samples, len(val_ds)))\n",
    "    val_ds = val_ds.select(sampled_val_indices)\n",
    "    \n",
    "    print(f\"‚úÖ Sampled datasets:\")\n",
    "    print(f\"   Train: {len(train_ds)} examples\")\n",
    "    print(f\"   Validation: {len(val_ds)} examples\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using full datasets (no sampling)\")\n",
    "    print(f\"   Train: {len(train_ds)} examples\")\n",
    "    print(f\"   Validation: {len(val_ds)} examples\")\n",
    "\n",
    "\n",
    "# Cell 8 (cont): Run DRaFT training\n",
    "# for cfg in configs:\n",
    "train_adapter(configs[0], train_ds, val_ds)  # type: ignore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
