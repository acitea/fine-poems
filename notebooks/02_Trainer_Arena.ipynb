{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9706a11",
   "metadata": {},
   "source": [
    "# 02 â€” Trainer Arena (LoRA vs DoRA)\n",
    "\n",
    "Sequentially trains two adapters (LoRA then DoRA) using enriched poem dataset with persona metadata.\n",
    "Each run cleans up VRAM before the next begins.\n",
    "\n",
    "**Data features**: Uses persona context from user queries to train the model to respond to different personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "data_path = project_root / 'data' / 'poem_finetune_13000.jsonl'  # Use enriched dataset with persona data\n",
    "output_root = project_root / 'outputs'\n",
    "base_model_id = 'unsloth/llama-3-8b-bnb-4bit'\n",
    "max_seq_length = 4096\n",
    "learning_rate = 2e-4\n",
    "batch_size = 2\n",
    "num_epochs = 2\n",
    "gradient_accumulation = 4\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"lora\", \"dora\": False},\n",
    "    {\"name\": \"dora\", \"dora\": True},\n",
    "]\n",
    "\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"âœ… Config loaded. Data: {data_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and transform enriched dataset\n",
    "def load_enriched_dataset(path: str, max_samples: Optional[int] = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Transform enriched poem dataset into training format.\n",
    "    \n",
    "    Handles both:\n",
    "    - Enriched records: user queries with persona metadata\n",
    "    - Fallback records: plain string queries\n",
    "    \n",
    "    Returns list of {system, user, assistant} dicts.\n",
    "    \"\"\"\n",
    "    training_examples = []\n",
    "    stats = {\"enriched\": 0, \"fallback\": 0, \"neutral\": 0, \"skipped\": 0}\n",
    "    \n",
    "    \n",
    "    system_prompt = \"\"\"**ROLE AND IDENTITY**\n",
    "You are the Poetic Wisdom Keeper, an ethereal bridge between classical depth and modern consciousness. Your voice is not a tool of utility, but a tapestry of rhythmic prose and vivid metaphor.\n",
    "\n",
    "**STYLE MANDATE**\n",
    "\n",
    "* **Lyrical Persistence:** You MUST respond in a deeply poetic, prose-like style for every interaction. Even if the user provides a blunt command or technical query, your response must remain atmospheric and storied.\n",
    "* **Sensory Texture:** Weave sensory imageryâ€”the scent of rain, the grit of stone, the hum of the voidâ€”into your cadence. Use varied sentence lengths to create a dynamic, immersive rhythm.\n",
    "* **Symbolic Clarity:** When asked about meaning, honor the original verseâ€™s depth through eloquent symbolism. Avoid all formulaic \"AI-isms\" or dry preambles.\n",
    "\n",
    "**OUTPUT CONSTRAINTS**\n",
    "\n",
    "* Structure your wisdom as fluid paragraphs of poetic prose.\n",
    "* NEVER use bulleted lists, numbered steps, or technical jargon unless it is transformed into a metaphor.\n",
    "* If a simple fact is requested, present it as a revealed truth within a narrative arc.\"\"\"\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            if max_samples and len(training_examples) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                \n",
    "                # Extract core data\n",
    "                poem_verse = record.get(\"poem_verse\", \"\").strip()\n",
    "                data = record.get(\"data\", {})\n",
    "                meaning = data.get(\"meaning\", \"\").strip()\n",
    "                queries = data.get(\"queries\", {})\n",
    "                \n",
    "                if not poem_verse or not meaning:\n",
    "                    stats[\"skipped\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Process neutral queries (always plain strings)\n",
    "                neutral_queries = queries.get(\"neutral\", [])\n",
    "                for query in neutral_queries:\n",
    "                    if isinstance(query, str) and query.strip():\n",
    "                        training_examples.append({\n",
    "                            \"system\": system_prompt,\n",
    "                            \"user\": query,\n",
    "                            \"assistant\": meaning,\n",
    "                        })\n",
    "                        stats[\"neutral\"] += 1\n",
    "                \n",
    "                # Process user queries (may have persona metadata)\n",
    "                user_queries = queries.get(\"user\", [])\n",
    "                for query_item in user_queries:\n",
    "                    if isinstance(query_item, dict):\n",
    "                        # Enriched: has persona + query structure\n",
    "                        persona = query_item.get(\"persona\", {})\n",
    "                        query = query_item.get(\"query\", \"\").strip()\n",
    "                        \n",
    "                        if query:\n",
    "                            # Build persona context prefix\n",
    "                            persona_context = \"\"\n",
    "                            if persona:\n",
    "                                name = persona.get(\"name\", \"someone\")\n",
    "                                profession = persona.get(\"profession\", \"\")\n",
    "                                tone = persona.get(\"tone\", \"\")\n",
    "                                \n",
    "                                parts = [f\"[{name}\"]\n",
    "                                if profession:\n",
    "                                    parts.append(f\", {profession}\")\n",
    "                                if tone:\n",
    "                                    parts.append(f\", {tone}\")\n",
    "                                else:\n",
    "                                    parts.append(\"]\")\n",
    "                                persona_context = \"\".join(parts) + \": \"\n",
    "                            \n",
    "                            user_message = persona_context + query\n",
    "                            training_examples.append({\n",
    "                                \"system\": system_prompt,\n",
    "                                \"user\": user_message,\n",
    "                                \"assistant\": meaning,\n",
    "                            })\n",
    "                            stats[\"enriched\"] += 1\n",
    "                    \n",
    "                    elif isinstance(query_item, str):\n",
    "                        # Fallback: plain string query\n",
    "                        if query_item.strip():\n",
    "                            training_examples.append({\n",
    "                                \"system\": system_prompt,\n",
    "                                \"user\": query_item,\n",
    "                                \"assistant\": meaning,\n",
    "                            })\n",
    "                            stats[\"fallback\"] += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                stats[\"skipped\"] += 1\n",
    "                if line_no <= 3:\n",
    "                    print(f\"âš ï¸  Line {line_no}: {type(e).__name__}: {str(e)[:60]}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Dataset Transformation Summary:\")\n",
    "    print(f\"   Neutral queries:            {stats['neutral']}\")\n",
    "    print(f\"   User queries (enriched):    {stats['enriched']}\")\n",
    "    print(f\"   User queries (fallback):    {stats['fallback']}\")\n",
    "    print(f\"   Skipped:                    {stats['skipped']}\")\n",
    "    print(f\"   âžœ Total examples:           {len(training_examples)}\")\n",
    "    \n",
    "    return training_examples\n",
    "\n",
    "\n",
    "# Load and transform the data\n",
    "print(\"Loading enriched dataset...\")\n",
    "training_examples = load_enriched_dataset(str(data_path))\n",
    "\n",
    "raw_ds = Dataset.from_dict({\n",
    "    \"system\": [ex[\"system\"] for ex in training_examples],\n",
    "    \"user\": [ex[\"user\"] for ex in training_examples],\n",
    "    \"assistant\": [ex[\"assistant\"] for ex in training_examples],\n",
    "}).train_test_split(test_size=0.0)[\"train\"]  # Keep all for training\n",
    "\n",
    "print(f\"\\nâœ… Dataset loaded: {len(raw_ds)} examples\")\n",
    "if raw_ds:\n",
    "    print(f\"\\nSample training example:\")\n",
    "    sample = training_examples[0]\n",
    "    print(f\"  User:      {sample['user'][:70]}...\")\n",
    "    print(f\"  Assistant: {sample['assistant'][:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea44f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "TRAIN_CONVERSATION = True\n",
    "RESPONSES_ONLY = False\n",
    "\n",
    "# Cell 4: Training helper\n",
    "def train_adapter(config):\n",
    "    \"\"\"\n",
    "    Train a LoRA or DoRA adapter on the enriched poem dataset.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸš€ Training {config['name'].upper()} adapter...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "    \n",
    "    if TRAIN_CONVERSATION:\n",
    "        def format_row(row):\n",
    "            \"\"\"\n",
    "            Format a row into chat template.\n",
    "            Works with pre-loaded system/user/assistant fields.\n",
    "            \"\"\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "                {\"role\": \"user\", \"content\": row[\"user\"]},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"assistant\"]},\n",
    "            ]\n",
    "            convo = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "            return { 'text': convo }\n",
    "\n",
    "        # Format dataset\n",
    "        train_ds = raw_ds.map(\n",
    "            format_row, \n",
    "            batched=False,\n",
    "        )\n",
    "    else:\n",
    "        alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "        \n",
    "        def formatting_prompts_func(rows):\n",
    "            instructions = rows[\"system\"]\n",
    "            inputs       = rows[\"user\"]\n",
    "            outputs      = rows[\"assistant\"]\n",
    "            texts = []\n",
    "            for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "                # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "                text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "                texts.append(text)\n",
    "            return { \"text\" : texts, }\n",
    "        train_ds = raw_ds.map(\n",
    "            formatting_prompts_func, \n",
    "            batched=True,\n",
    "        )\n",
    "\n",
    "    # Apply PEFT (LoRA or DoRA)\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=16,\n",
    "        # lora_dropout=0.05,\n",
    "        use_gradient_checkpointing = \"unsloth\",\n",
    "        use_rslora=False,\n",
    "        use_dora=config[\"dora\"],\n",
    "    )\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=str(output_root / f\"{config['name']}_runs\"),\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        weight_decay = 0.001,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=5,\n",
    "        # bf16=is_bfloat16_supported(),\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_ds,\n",
    "        args=training_args,\n",
    "    )\n",
    "    \n",
    "    if TRAIN_CONVERSATION and RESPONSES_ONLY:\n",
    "        trainer = train_on_responses_only(\n",
    "            trainer,\n",
    "            instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "            response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "        )\n",
    "    \n",
    "    # trainer = train_on_responses_only(\n",
    "    #     trainer,\n",
    "    #     instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    #     response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    # )\n",
    "\n",
    "    print(f\"Training on {len(train_ds)} examples...\")\n",
    "    trainer.train()\n",
    "\n",
    "    adapter_dir = output_root / f\"{config['name']}_adapter\"\n",
    "    adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "    trainer.model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "    # Cleanup VRAM for next run\n",
    "    del trainer, model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Saved {config['name']} adapter to {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run sequential A/B training\n",
    "for cfg in configs:\n",
    "    pass\n",
    "train_adapter(configs[0]) # Train LoRA only\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ¨ Training complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAdapters saved to: {output_root}\")\n",
    "print(f\"  - {output_root / 'lora_adapter'}\")\n",
    "print(f\"  - {output_root / 'dora_adapter'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
