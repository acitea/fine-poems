{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9706a11",
   "metadata": {},
   "source": [
    "# 02 â€” Trainer Arena (LoRA vs DoRA)\n",
    "Sequentially trains two adapters (LoRA then DoRA) on the bardic refusal set. Each run cleans up VRAM before the next begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "data_path = project_root / 'data' / 'poetic_refusal.jsonl'\n",
    "output_root = project_root / 'outputs'\n",
    "base_model_id = 'unsloth/llama-3-8b-bnb-4bit'\n",
    "max_seq_length = 2048\n",
    "learning_rate = 2e-4\n",
    "batch_size = 2\n",
    "num_epochs = 1\n",
    "gradient_accumulation = 4\n",
    "use_wandb = False\n",
    "wandb_project = 'bardic-finetune'\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"lora\", \"dora\": False},\n",
    "    {\"name\": \"dora\", \"dora\": True},\n",
    "]\n",
    "\n",
    "output_root.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load raw dataset\n",
    "raw_ds = load_dataset('json', data_files=str(data_path))\n",
    "print(raw_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea44f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Training helper\n",
    "def train_adapter(config):\n",
    "    print(f\"\\n=== Training {config['name']} ===\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    def format_row(row):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": row[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": row[\"user\"]},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"assistant\"]},\n",
    "        ]\n",
    "        row[\"text\"] = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        return row\n",
    "\n",
    "    train_ds = raw_ds[\"train\"].map(\n",
    "        format_row, remove_columns=raw_ds[\"train\"].column_names\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,\n",
    "        target_modules=\"all-linear\",\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        use_rslora=False,\n",
    "        use_dora=config[\"dora\"],\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_root / f\"{config['name']}_runs\"),\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"wandb\" if use_wandb else \"none\",\n",
    "        bf16=is_bfloat16_supported(),\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_ds,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=True,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    adapter_dir = output_root / f\"{config['name']}_adapter\"\n",
    "    adapter_dir.mkdir(parents=True, exist_ok=True)\n",
    "    trainer.model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)\n",
    "\n",
    "    del trainer, model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Saved {config['name']} adapter to {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run sequential A/B training\n",
    "for cfg in configs:\n",
    "    train_adapter(cfg)\n",
    "\n",
    "print('Adapters done.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
