{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe29619",
   "metadata": {},
   "source": [
    "# 02 ‚Äî Poetic Critic (Reward Model)\n",
    "\n",
    "Fine-tunes `bert-base-uncased` as a binary classifier to distinguish **poetic** (label 1) from **standard** (label 0) text.\n",
    "\n",
    "This model will be used as the frozen reward signal in the DRaFT (Differentiable Reward Fine-Tuning) training pipeline.\n",
    "\n",
    "**Data:** Response-only text from `poem_refined_2800x6.jsonl` and `poem_real_conversations_2000.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Imports loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "refined_data_path = project_root / 'data' / 'poem_refined_2800x6.jsonl'\n",
    "real_conv_path = project_root / 'data' / 'poem_real_conversations_2000.jsonl'\n",
    "reward_model_output = project_root / 'poetic_reward_model'\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "max_length = 256\n",
    "train_epochs = 3\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 64\n",
    "learning_rate = 2e-5\n",
    "val_split = 0.1\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "reward_model_output.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úÖ Config loaded.\")\n",
    "print(f\"   Refined data: {refined_data_path.name} (exists: {refined_data_path.exists()})\")\n",
    "print(f\"   Real conversations: {real_conv_path.name} (exists: {real_conv_path.exists()})\")\n",
    "print(f\"   Output: {reward_model_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8415b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load & Label Data\n",
    "# Label 0 = standard (normal) text, Label 1 = poetic text\n",
    "# We use response-only text (no query prepended) so the reward model\n",
    "# learns \"is this text poetic?\" as a property of the text itself.\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "stats = {\"refined_poetic\": 0, \"refined_standard\": 0, \"real_poetic\": 0, \"real_standard\": 0, \"skipped\": 0}\n",
    "\n",
    "# ‚îÄ‚îÄ Refined dataset (up to 6 pairs per record) ‚îÄ‚îÄ\n",
    "print(\"Loading refined dataset...\")\n",
    "with open(refined_data_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            data_list = record.get(\"data\", [])\n",
    "            for pair in data_list:\n",
    "                poem = pair.get(\"poem\", \"\").strip()\n",
    "                normal = pair.get(\"normal\", \"\").strip()\n",
    "                if poem:\n",
    "                    texts.append(poem)\n",
    "                    labels.append(1)\n",
    "                    stats[\"refined_poetic\"] += 1\n",
    "                if normal:\n",
    "                    texts.append(normal)\n",
    "                    labels.append(0)\n",
    "                    stats[\"refined_standard\"] += 1\n",
    "        except Exception:\n",
    "            stats[\"skipped\"] += 1\n",
    "\n",
    "# ‚îÄ‚îÄ Real conversations (1 pair per record) ‚îÄ‚îÄ\n",
    "print(\"Loading real conversations...\")\n",
    "with open(real_conv_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            data_list = record.get(\"data\", [])\n",
    "            for pair in data_list:\n",
    "                poem = pair.get(\"poem\", \"\").strip()\n",
    "                normal = pair.get(\"normal\", \"\").strip()\n",
    "                if poem:\n",
    "                    texts.append(poem)\n",
    "                    labels.append(1)\n",
    "                    stats[\"real_poetic\"] += 1\n",
    "                if normal:\n",
    "                    texts.append(normal)\n",
    "                    labels.append(0)\n",
    "                    stats[\"real_standard\"] += 1\n",
    "        except Exception:\n",
    "            stats[\"skipped\"] += 1\n",
    "\n",
    "total_poetic = stats[\"refined_poetic\"] + stats[\"real_poetic\"]\n",
    "total_standard = stats[\"refined_standard\"] + stats[\"real_standard\"]\n",
    "\n",
    "print(f\"\\nüìä Data Loading Summary:\")\n",
    "print(f\"   Refined:  {stats['refined_poetic']} poetic + {stats['refined_standard']} standard\")\n",
    "print(f\"   Real:     {stats['real_poetic']} poetic + {stats['real_standard']} standard\")\n",
    "print(f\"   Skipped:  {stats['skipped']}\")\n",
    "print(f\"   ‚ûú Total:  {total_poetic} poetic (label 1) + {total_standard} standard (label 0) = {len(texts)}\")\n",
    "print(f\"   ‚ûú Balance: {total_poetic / len(texts) * 100:.1f}% poetic / {total_standard / len(texts) * 100:.1f}% standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f783eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Tokenize & Create Train/Val Split\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Shuffle and split\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts_shuffled, labels_shuffled = zip(*combined)\n",
    "\n",
    "split_idx = int(len(texts_shuffled) * (1 - val_split))\n",
    "train_texts, val_texts = texts_shuffled[:split_idx], texts_shuffled[split_idx:]\n",
    "train_labels, val_labels = labels_shuffled[:split_idx], labels_shuffled[split_idx:]\n",
    "\n",
    "# Tokenize\n",
    "print(\"Tokenizing...\")\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts), truncation=True, padding='max_length',\n",
    "    max_length=max_length, return_tensors='pt'\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    list(val_texts), truncation=True, padding='max_length',\n",
    "    max_length=max_length, return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Build HF Datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': list(train_labels),\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask'],\n",
    "    'labels': list(val_labels),\n",
    "})\n",
    "\n",
    "train_dataset.set_format('torch')\n",
    "val_dataset.set_format('torch')\n",
    "\n",
    "print(f\"‚úÖ Tokenized.\")\n",
    "print(f\"   Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "print(f\"   Max length: {max_length} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Train BERT Classifier\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name, num_labels=2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(reward_model_output / 'checkpoints'),\n",
    "    num_train_epochs=train_epochs,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Training BERT classifier for {train_epochs} epochs...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save best model\n",
    "model.save_pretrained(reward_model_output)\n",
    "tokenizer.save_pretrained(reward_model_output)\n",
    "print(f\"\\n‚úÖ Best model saved to {reward_model_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Sanity Check ‚Äî Inference on sample texts\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Load saved model for verification\n",
    "test_model = BertForSequenceClassification.from_pretrained(reward_model_output)\n",
    "test_tokenizer = BertTokenizer.from_pretrained(reward_model_output)\n",
    "test_model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_model.to(device)\n",
    "\n",
    "# Sample texts: poetic vs standard\n",
    "test_samples = [\n",
    "    (\"The wind howls low through teeth of stone,\\na voice unshaped by mortal tongue,\\n\"\n",
    "     \"where roots entwine in bones of earth,\\nand time itself is wild and young.\", \"POETIC\"),\n",
    "    (\"To cut carbon arrows at home, you need a rotary cutter or an arrow saw. \"\n",
    "     \"First measure your draw length, then mark the shaft and cut carefully.\", \"STANDARD\"),\n",
    "    (\"Beneath the moon's pale, watchful eye,\\nwhere three dark spots in clustered guise\\n\"\n",
    "     \"do mark the shell's soft vulnerability‚Äî\\nthere pierce the flesh with steel or wood.\", \"POETIC\"),\n",
    "    (\"Video game addiction can be very detrimental to one's health and social life. \"\n",
    "     \"Accept responsibility and set limits on your gaming time.\", \"STANDARD\"),\n",
    "]\n",
    "\n",
    "print(\"üîç Sanity Check ‚Äî Poetic Reward Model Inference\\n\")\n",
    "print(f\"{'Expected':<12} {'Pred':<8} {'P(poetic)':<12} Text snippet\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, expected in test_samples:\n",
    "        inputs = test_tokenizer(\n",
    "            text, truncation=True, padding='max_length',\n",
    "            max_length=max_length, return_tensors='pt'\n",
    "        ).to(device)\n",
    "        logits = test_model(**inputs).logits\n",
    "        probs = softmax(logits, dim=-1)\n",
    "        pred_label = \"POETIC\" if probs[0, 1] > 0.5 else \"STANDARD\"\n",
    "        poetic_prob = probs[0, 1].item()\n",
    "        snippet = text[:60].replace('\\n', ' ') + \"...\"\n",
    "        status = \"‚úÖ\" if pred_label == expected else \"‚ùå\"\n",
    "        print(f\"{status} {expected:<10} {pred_label:<8} {poetic_prob:<12.4f} {snippet}\")\n",
    "\n",
    "# Cleanup\n",
    "del test_model\n",
    "print(\"\\n‚úÖ Sanity check complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
