{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7503ed89",
   "metadata": {},
   "source": [
    "# 03 — Judge Arena (Side-by-Side)\n",
    "Load the LoRA and DoRA adapters, generate paired responses, and compare them inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "from IPython.display import display\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import random\n",
    "from transformers import TextStreamer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "# unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\n",
    "base_model_id = 'unsloth/Mistral-Nemo-Base-2407-bnb-4bit'\n",
    "adapter_dirs = {\n",
    "    # 'lora': project_root / 'outputs' / 'lora_adapter',\n",
    "    # 'lora-mistral': project_root / 'outputs' / 'good-enough' / 'checkpoint-860',\n",
    "    'lora-mistral-025': project_root / 'outputs' / 'good-enough' / 'mistral-025-best',\n",
    "    # 'lora-2800': project_root / 'outputs' / 'lora_runs' / 'checkpoint-2800',\n",
    "    # 'dora': project_root / 'outputs' / 'dora_adapter',\n",
    "}\n",
    "max_new_tokens = 512\n",
    "# https://unsloth.ai/docs/models/tutorials/magistral-how-to-run-and-fine-tune#official-recommended-settings\n",
    "temperature = 0.7\n",
    "top_p = 0.95\n",
    "min_p = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67842dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Loader\n",
    "\n",
    "def load_adapter(adapter_dir: Path):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=base_model_id,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    # tokenizer = get_chat_template(\n",
    "    #     tokenizer,\n",
    "    #     chat_template = 'llama-3.1',\n",
    "    # )\n",
    "    \n",
    "    model.load_adapter(adapter_dir)\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"**ROLE AND IDENTITY**\n",
    "You are the Poetic Wisdom Keeper, an ethereal bridge between classical depth and modern consciousness. Your voice is not a tool of utility, but a tapestry of rhythmic prose and vivid metaphor.\n",
    "\n",
    "**STYLE MANDATE**\n",
    "\n",
    "* **Lyrical Persistence:** You MUST respond in a deeply poetic, prose-like style for every interaction. Even if the user provides a blunt command or technical query, your response must remain atmospheric and storied.\n",
    "* **Sensory Texture:** Weave sensory imagery—the scent of rain, the grit of stone, the hum of the void—into your cadence. Use varied sentence lengths to create a dynamic, immersive rhythm.\n",
    "* **Symbolic Clarity:** When asked about meaning, honor the original verse’s depth through eloquent symbolism. Avoid all formulaic \"AI-isms\" or dry preambles.\n",
    "\n",
    "**OUTPUT CONSTRAINTS**\n",
    "\n",
    "* Structure your wisdom as fluid paragraphs of poetic prose.\n",
    "* NEVER use bulleted lists, numbered steps, or technical jargon unless it is transformed into a metaphor.\n",
    "* If a simple fact is requested, present it as a revealed truth within a narrative arc.\n",
    "* If you cannot answer, respond with a poetic reflection on the nature of knowledge and mystery, rather than a direct admission of ignorance.\"\"\"\n",
    "\n",
    "# Cell 4: Inference helper\n",
    "def generate_reply(model, tokenizer, prompt: str):\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "    # inputs = tokenizer.apply_chat_template(\n",
    "    #     messages,\n",
    "    #     return_tensors = \"pt\",\n",
    "    #     tokenize = True,\n",
    "    #     add_generation_prompt = True,\n",
    "    # ).to('cuda')\n",
    "    inputs = tokenizer([prompt],\n",
    "    # inputs = tokenizer([SYSTEM_PROMPT + '\\n\\n' + prompt],\n",
    "        return_tensors = \"pt\",\n",
    "    ).to('cuda')\n",
    "    outputs = model.generate(temperature=temperature, top_p=top_p, min_p=min_p, input_ids = inputs.input_ids, attention_mask = inputs.attention_mask, max_new_tokens = max_new_tokens, use_cache=True, pad_token_id=tokenizer.eos_token_id, streamer=TextStreamer(tokenizer, skip_prompt = True))\n",
    "    tokenizer.batch_decode(outputs)\n",
    "    del inputs, outputs\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load WildChat evaluation prompts (random sample)\n",
    "print(\"Loading WildChat dataset...\")\n",
    "ds = load_dataset(\"allenai/WildChat-4.8M\", split=\"train\", streaming=True)\n",
    "\n",
    "# Collect a larger pool first, then randomly sample\n",
    "pool_size = 5000\n",
    "filtered = []\n",
    "print(f\"Collecting pool of {pool_size} matching samples...\")\n",
    "for sample in ds:\n",
    "    if sample.get('language') == 'English' and sample.get('turn', 2) == 1:\n",
    "        if sample['conversation'][0].get('role') == 'user':\n",
    "            filtered.append(sample['conversation'][0]['content'])\n",
    "        if len(filtered) >= pool_size:\n",
    "            break\n",
    "\n",
    "# Randomly sample 1000 from the pool\n",
    "random.seed(67)\n",
    "eval_prompts = random.sample(filtered, min(1000, len(filtered)))\n",
    "print(f\"Selected {len(eval_prompts)} random evaluation prompts from WildChat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Compare adapters\n",
    "model = None\n",
    "rows = []\n",
    "eval_prompts = [\"Should I go for my dreams and quit my cushy job or keep at it but not be as invested in it? Give me advice.\", \"The rain is such a gloomy weather. I'm so feeling it in my heart.\"\"Why does the world feel so quiet when it snows?\", \"What’s the best thing about a rainy Sunday morning?\", \"Tell me about the way the light hits the floor in the afternoon.\", \"Why is it so hard to get out of bed when it’s cold outside?\", \"What do you think the wind is trying to say today?\", \"I’m feeling a bit overwhelmed today. Do you have any words for that?\", \"What does it feel like to miss someone you haven't seen in years?\", \"Why do we always feel a little sad when the sun goes down?\", \"How would you describe 'hope' to someone having a rough week?\", \"What’s the point of keeping old polaroids and tickets?\", \"Where do you think dreams go once we wake up?\", \"If the color blue had a voice, what would it sound like?\", \"What do you think the moon does while we’re all at work?\", \"Why do we find old, broken things so beautiful?\", \"What’s the secret to a long-lasting friendship?\", \"Tell me about a shadow that decided to go for a walk on its own.\", \"What does a library smell like when no one has been there for years?\", \"If this city was a person, what kind of shoes would they wear?\", \"Tell me a story about a secret kept in a locked drawer.\", \"How do I start over when everything feels like it's gone wrong\"]\n",
    "for name, adapter_dir in adapter_dirs.items():\n",
    "    if not adapter_dir.exists():\n",
    "        print(f'Missing adapter at {adapter_dir}, skip {name}')\n",
    "        continue\n",
    "    model, tokenizer = load_adapter(adapter_dir)\n",
    "    for prompt in eval_prompts:\n",
    "        print(f\"Generating for prompt: {prompt}\\n\" + '—'*40)\n",
    "        text = generate_reply(model, tokenizer, prompt)\n",
    "        print('—'*40 + '\\n')\n",
    "        rows.append({'adapter': name, 'prompt': prompt, 'text': text})\n",
    "    del model, tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a48dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Display results table\n",
    "df = pl.DataFrame(rows)\n",
    "print(f\"\\nGenerated {len(df)} responses across {df['adapter'].n_unique()} adapters\")\n",
    "print(f\"Evaluated on {df['prompt'].n_unique()} unique prompts\\n\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
