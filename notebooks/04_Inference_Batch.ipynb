{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6bee50",
   "metadata": {},
   "source": [
    "# 04 â€” Batch Inference with QLoRA Adapter\n",
    "Load a HuggingFace model with QLoRA adapter and run batch inference on a dataset.\n",
    "\n",
    "**Note:** vLLM has limited LoRA support. We'll merge the adapter first for vLLM, or use transformers directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fb4c8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.10.0 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
      "W0206 20:44:31.637000 74924 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 20:44:32 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Optional: for vLLM\n",
    "try:\n",
    "    # Add vLLM installation path\n",
    "    vllm_path = \"/Users/a.darryl/Desktop/Y4S2/finetuning/.venv/bin/vllm\"\n",
    "    sys.path.append(os.path.dirname(vllm_path))\n",
    "    from vllm import LLM, SamplingParams\n",
    "    VLLM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VLLM_AVAILABLE = False\n",
    "    print(\"vLLM not installed. Will use transformers for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407a78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Config\n",
    "project_root = Path('..').resolve()\n",
    "dataset_path = project_root / 'data' / 'poem_condense.csv'\n",
    "output_path = project_root / 'data' / 'poem_condense_annotated.jsonl'\n",
    "\n",
    "# Model configuration\n",
    "adapter_model_id = \"prettyvampire/poemma\"  # HF model with QLoRA adapter\n",
    "base_model_name = None  # Will auto-detect from adapter config\n",
    "\n",
    "# Inference settings\n",
    "use_vllm = True  # Set True to merge adapter and use vLLM (faster for large batches)\n",
    "batch_size = 8  # For transformers inference\n",
    "max_new_tokens = 256\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "\n",
    "# Text column to process\n",
    "text_column = \"Verse\"  # Column name in your CSV\n",
    "output_column = \"analysis\"  # Column name for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1239e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  TEST MODE ENABLED - Using local settings\n",
      "âœ“ Using Apple Silicon GPU (MPS)\n",
      "ğŸ“Š Will process only 5 samples\n",
      "ğŸ”§ Batch size: 2, Max tokens: 50\n"
     ]
    }
   ],
   "source": [
    "# Cell 2b: Test mode for local macOS testing\n",
    "import platform\n",
    "\n",
    "# Toggle this for local testing\n",
    "TEST_MODE = True  # Set False for full GPU cluster run\n",
    "\n",
    "if TEST_MODE:\n",
    "    print(\"âš ï¸  TEST MODE ENABLED - Using local settings\")\n",
    "    \n",
    "    # Detect device\n",
    "    if platform.system() == \"Darwin\":  # macOS\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = \"mps\"  # Apple Silicon GPU\n",
    "            print(\"âœ“ Using Apple Silicon GPU (MPS)\")\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "            print(\"âœ“ Using CPU (Intel Mac)\")\n",
    "    else:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"âœ“ Using {device}\")\n",
    "    \n",
    "    # Override settings for testing\n",
    "    batch_size = 2  # Smaller for CPU/MPS\n",
    "    max_new_tokens = 50  # Shorter generations\n",
    "    use_vllm = True\n",
    "    \n",
    "    # Test on subset only\n",
    "    TEST_SUBSET_SIZE = 5  # Process only first N rows\n",
    "    print(f\"ğŸ“Š Will process only {TEST_SUBSET_SIZE} samples\")\n",
    "    print(f\"ğŸ”§ Batch size: {batch_size}, Max tokens: {max_new_tokens}\")\n",
    "else:\n",
    "    print(\"ğŸš€ PRODUCTION MODE - Using full cluster settings\")\n",
    "    device = \"auto\"\n",
    "    TEST_SUBSET_SIZE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e8eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š TEST MODE: Limited to 5 rows\n",
      "Columns: ['', 'Verse', 'Meter', 'char_count']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>Verse</th><th>Meter</th><th>char_count</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>&quot;ah why this boding start this â€¦</td><td>&quot;iambic&quot;</td><td>6</td></tr><tr><td>1</td><td>&quot;that wings my pulse and shootsâ€¦</td><td>&quot;iambic&quot;</td><td>6</td></tr><tr><td>2</td><td>&quot;what mean regardless of yon miâ€¦</td><td>&quot;iambic&quot;</td><td>6</td></tr><tr><td>3</td><td>&quot;these earthborn visions saddenâ€¦</td><td>&quot;iambic&quot;</td><td>6</td></tr><tr><td>4</td><td>&quot;what strange disorder prompts â€¦</td><td>&quot;iambic&quot;</td><td>6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚     â”† Verse                           â”† Meter  â”† char_count â”‚\n",
       "â”‚ --- â”† ---                             â”† ---    â”† ---        â”‚\n",
       "â”‚ i64 â”† str                             â”† str    â”† i64        â”‚\n",
       "â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 0   â”† ah why this boding start this â€¦ â”† iambic â”† 6          â”‚\n",
       "â”‚ 1   â”† that wings my pulse and shootsâ€¦ â”† iambic â”† 6          â”‚\n",
       "â”‚ 2   â”† what mean regardless of yon miâ€¦ â”† iambic â”† 6          â”‚\n",
       "â”‚ 3   â”† these earthborn visions saddenâ€¦ â”† iambic â”† 6          â”‚\n",
       "â”‚ 4   â”† what strange disorder prompts â€¦ â”† iambic â”† 6          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Load dataset\n",
    "df = pl.read_csv(dataset_path)\n",
    "\n",
    "# Apply test subset if in test mode\n",
    "if TEST_MODE and TEST_SUBSET_SIZE:\n",
    "    df = df.head(TEST_SUBSET_SIZE)\n",
    "    print(f\"ğŸ“Š TEST MODE: Limited to {len(df)} rows\")\n",
    "else:\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "print(f\"Columns: {df.columns}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fde81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Option A - Load adapter with transformers (no merge)\n",
    "if not use_vllm:\n",
    "    print(\"Loading model with adapter (transformers)...\")\n",
    "    \n",
    "    # Load adapter to get base model name\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    import json\n",
    "    \n",
    "    config_path = hf_hub_download(adapter_model_id, \"adapter_config.json\")\n",
    "    with open(config_path) as f:\n",
    "        adapter_config = json.load(f)\n",
    "        base_model_name = adapter_config.get(\"base_model_name_or_path\")\n",
    "    \n",
    "    print(f\"Base model: {base_model_name}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Determine dtype and device based on test mode\n",
    "    if TEST_MODE:\n",
    "        # For macOS/CPU testing, use float32 or float16\n",
    "        if device == \"mps\":\n",
    "            dtype = torch.float16  # MPS supports float16\n",
    "            device_map = {\"\": device}\n",
    "        else:\n",
    "            dtype = torch.float32  # CPU fallback\n",
    "            device_map = {\"\": \"cpu\"}\n",
    "        print(f\"Loading model with dtype={dtype}, device={device}\")\n",
    "    else:\n",
    "        dtype = torch.bfloat16\n",
    "        device_map = \"auto\"\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Load and apply adapter\n",
    "    model = PeftModel.from_pretrained(model, adapter_model_id)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model loaded with adapter!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9360c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging adapter for vLLM...\n",
      "Base model: meta-llama/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a108cd8f2c34e7390dc6a4dea3a894a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3e13661f8c433c9bfc9da5e133975a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872668b2af7444dca0b4827a72650d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848a27435255469e957ef523d29cd12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8c50eb839c46d0b1c3a9b3f18818cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd6d467011644ff864dab04a78f1dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307ee27557e14443b6b43770e0dc4aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dd6f1103674a11a444cc9e90174ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: I/O error: No space left on device (os error 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m merged_path = project_root / \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mpoetic_llm\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m merged_path.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\u001b[32m     36\u001b[39m tokenizer.save_pretrained(merged_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Y4S2/finetuning/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4173\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   4168\u001b[39m     gc.collect()\n\u001b[32m   4170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   4171\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   4172\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4173\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4175\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Y4S2/finetuning/.venv/lib/python3.13/site-packages/safetensors/torch.py:307\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    277\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    278\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    279\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    280\u001b[39m ):\n\u001b[32m    281\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    283\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: I/O error: No space left on device (os error 28)"
     ]
    }
   ],
   "source": [
    "# Cell 5: Option B - Merge adapter and use vLLM (faster for large batches)\n",
    "if use_vllm:\n",
    "    if not VLLM_AVAILABLE:\n",
    "        raise ImportError(\"vLLM not installed. Run: uv pip install vllm\")\n",
    "    \n",
    "    print(\"Loading and merging adapter for vLLM...\")\n",
    "    \n",
    "    # Load adapter config\n",
    "    from huggingface_hub import hf_hub_download, login\n",
    "    login(token=os.getenv(\"HUGGINGFACE_API_KEY\"))\n",
    "    config_path = hf_hub_download(adapter_model_id, \"adapter_config.json\")\n",
    "    with open(config_path) as f:\n",
    "        adapter_config = json.load(f)\n",
    "        base_model_name = adapter_config.get(\"base_model_name_or_path\")\n",
    "    \n",
    "    print(f\"Base model: {base_model_name}\")\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cpu\",  # Keep on CPU during merge\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Load and merge adapter\n",
    "    model = PeftModel.from_pretrained(model, adapter_model_id)\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model temporarily\n",
    "    merged_path = project_root / \"outputs\" / \"poetic_llm\"\n",
    "    merged_path.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(merged_path)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    tokenizer.save_pretrained(merged_path)\n",
    "    \n",
    "    # Clean up transformers model\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load with vLLM\n",
    "    llm = LLM(\n",
    "        model=str(merged_path),\n",
    "        dtype=\"bfloat16\",\n",
    "        trust_remote_code=True,\n",
    "        max_model_len=2048,\n",
    "    )\n",
    "    \n",
    "    print(\"vLLM model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376acc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Prepare prompts\n",
    "def format_prompt(text: str, system_prompt: str = \"\") -> str:\n",
    "    \"\"\"Format prompt using the tokenizer's chat template if available.\"\"\"\n",
    "    # Try to use the tokenizer's built-in chat template\n",
    "    if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template:\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": text})\n",
    "        \n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    \n",
    "    # Fallback: simple format for base models or models without chat templates\n",
    "    if system_prompt:\n",
    "        return f\"{system_prompt}\\n\\n{text}\\n\\n\"\n",
    "    return text\n",
    "\n",
    "# Extract texts from dataset\n",
    "if text_column not in df.columns:\n",
    "    raise ValueError(f\"Column '{text_column}' not found. Available: {df.columns}\")\n",
    "\n",
    "texts = df[text_column].cast(str).to_list()\n",
    "\n",
    "# Optional: add a system prompt for instruction-following\n",
    "system_prompt = \"\"  # Add your system prompt here if needed\n",
    "prompts = [format_prompt(text, system_prompt) for text in texts]\n",
    "\n",
    "print(f\"Prepared {len(prompts)} prompts\")\n",
    "print(f\"Example prompt:\\n{prompts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6b: Verify chat template format\n",
    "print(\"Chat template detection:\")\n",
    "if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template:\n",
    "    print(\"âœ“ Using tokenizer's built-in chat template (Llama 3.1 format)\")\n",
    "    # Show example\n",
    "    test_message = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    example = tokenizer.apply_chat_template(test_message, tokenize=False, add_generation_prompt=True)\n",
    "    print(f\"\\nExample format:\\n{example}\")\n",
    "else:\n",
    "    print(\"âš  No chat template found, using plain text format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run inference with transformers\n",
    "if not use_vllm:\n",
    "    print(\"Running batch inference with transformers...\")\n",
    "    outputs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "        ).to(model.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        batch_outputs = tokenizer.batch_decode(\n",
    "            generated_ids[:, inputs.input_ids.shape[1]:],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        outputs.extend(batch_outputs)\n",
    "    \n",
    "    print(f\"Generated {len(outputs)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Run inference with vLLM\n",
    "if use_vllm:\n",
    "    print(\"Running batch inference with vLLM...\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_new_tokens,\n",
    "    )\n",
    "    \n",
    "    vllm_outputs = llm.generate(prompts, sampling_params)\n",
    "    outputs = [output.outputs[0].text for output in vllm_outputs]\n",
    "    \n",
    "    print(f\"Generated {len(outputs)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save results\n",
    "# Add outputs to dataframe\n",
    "df_with_outputs = df.with_columns(\n",
    "    pl.Series(name=output_column, values=outputs)\n",
    ")\n",
    "\n",
    "# Save as JSONL\n",
    "df_with_outputs.write_ndjson(output_path)\n",
    "print(f\"Saved {len(df_with_outputs)} rows to {output_path}\")\n",
    "\n",
    "# Preview\n",
    "df_with_outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Preview sample outputs\n",
    "print(\"Sample outputs:\\n\")\n",
    "for i in range(min(3, len(outputs))):\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"Input: {texts[i][:100]}...\")\n",
    "    print(f\"Output: {outputs[i][:200]}...\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
