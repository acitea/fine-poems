{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5285add",
   "metadata": {},
   "source": [
    "# 04. Evaluation Arena\n",
    "\n",
    "Evaluate LLM outputs against the Bardic poetry rubric using an LLM judge on OpenRouter.\n",
    "Metrics computed:\n",
    "- **Quality Score** (0-5): Gibberish ‚Üí Basic conversation ‚Üí Prose-like ‚Üí Basic poem-like ‚Üí Obvious structured ‚Üí Excellent\n",
    "- **Format Adherence** (0/1): Recognizable poem with poetic structure\n",
    "- **Failure Rate** (%): Infinite repetition or broken tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e41f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section 1: Load Environment, Config, and Inputs\n",
    "\n",
    "# Cell 1.1: Import Libraries\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import json_repair\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.2: Setup OpenRouter API and Environment\n",
    "load_dotenv()\n",
    "\n",
    "# OpenRouter Configuration\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.io/api/v1\"\n",
    "JUDGE_MODEL = \"openai/gpt-oss-20b:free\"  # A capable, fast model for judging\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in environment. Please set it in .env or export it.\")\n",
    "\n",
    "# Initialize OpenAI client pointing to OpenRouter\n",
    "client = OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=OPENROUTER_BASE_URL\n",
    ")\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../inference\")\n",
    "OUTPUTS_DIR = Path(\"../results\")\n",
    "OUTPUTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "logger.info(f\"Using Judge Model: {JUDGE_MODEL}\")\n",
    "logger.info(f\"Data Directory: {DATA_DIR}\")\n",
    "logger.info(f\"Outputs Directory: {OUTPUTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.3: Manually Specify and Load CSVs\n",
    "# List the CSV files you want to evaluate\n",
    "# Example: [\"lora_outputs.csv\", \"dora_outputs.csv\", \"dropout_outputs.csv\"]\n",
    "CSV_FILES_TO_EVALUATE = [\n",
    "    \"judge_arena_base.csv\",\n",
    "    \"judge_arena_dora-1k.csv\",\n",
    "    \"judge_arena_dora-optimised-1k.csv\",\n",
    "    \"judge_arena_dora-optimised-64-128-1k.csv\",\n",
    "    \"judge_arena_dora-optimised-64-128.csv\",\n",
    "    \"judge_arena_lora-16-32-1k.csv\",\n",
    "    \"judge_arena_lora-base-1k.csv\",\n",
    "    \"judge_arena_lora-base-full.csv\",\n",
    "    \"judge_arena_lora-decay.csv\",\n",
    "    \"judge_arena_lora-dropout-1k.csv\",\n",
    "    \"judge_arena_lora-dropout-full.csv\",\n",
    "    \"judge_arena_lora-learner.csv\",\n",
    "]\n",
    "\n",
    "# Load and validate CSVs\n",
    "eval_data = {}  # {filename: DataFrame with columns [prompt, text]}\n",
    "\n",
    "if not CSV_FILES_TO_EVALUATE:\n",
    "    logger.warning(\"No CSV files specified. Please add filenames to CSV_FILES_TO_EVALUATE list.\")\n",
    "else:\n",
    "    for csv_filename in CSV_FILES_TO_EVALUATE:\n",
    "        csv_path = DATA_DIR / csv_filename\n",
    "        csv_name = Path(csv_filename).stem\n",
    "        \n",
    "        try:\n",
    "            if not csv_path.exists():\n",
    "                logger.error(f\"File not found: {csv_path}\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            # Validate required columns\n",
    "            if \"prompt\" not in df.columns or \"text\" not in df.columns:\n",
    "                logger.warning(f\"Skipping {csv_name}: missing 'prompt' or 'text' column\")\n",
    "                continue\n",
    "            \n",
    "            eval_data[csv_name] = df[[\"prompt\", \"text\"]].reset_index(drop=True)\n",
    "            logger.info(f\"Loaded {csv_name}: {len(df)} rows\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {csv_path}: {e}\")\n",
    "\n",
    "if not eval_data:\n",
    "    logger.warning(\"No valid CSV files loaded. Please check CSV_FILES_TO_EVALUATE and file paths.\")\n",
    "else:\n",
    "    logger.info(f\"Successfully loaded {len(eval_data)} configuration(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cdd69",
   "metadata": {},
   "source": [
    "## Section 2: Define Judge Prompt and JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c6d718",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# JSON Schema for validation\u001b[39;00m\n\u001b[32m     49\u001b[39m JSON_SCHEMA = {\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrequired\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mquality_score\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mformat_adherence\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfailure\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     58\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mlogger\u001b[49m.info(\u001b[33m\"\u001b[39m\u001b[33mJudge prompt and schema defined\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 2.1: Define Judge Prompt and JSON Schema\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"You are an expert evaluator of poetic writing. \n",
    "Your task is to evaluate a model's response to a user prompt and rate it according to a strict rubric.\n",
    "\n",
    "You MUST respond with ONLY a valid JSON object with the following structure:\n",
    "{\n",
    "  \"quality_score\": <integer from 0 to 5>,\n",
    "  \"format_adherence\": <0 or 1>,\n",
    "  \"failure\": <0 or 1>,\n",
    "  \"notes\": \"<brief explanation>\"\n",
    "}\n",
    "\n",
    "DO NOT include any text outside the JSON object. NO MARKDOWN, NO EXPLANATIONS, ONLY JSON.\"\"\"\n",
    "\n",
    "JUDGE_USER_PROMPT_TEMPLATE = \"\"\"Evaluate the following response on a poetic rubric:\n",
    "\n",
    "**User Prompt:** {prompt}\n",
    "\n",
    "**Model Response:**\n",
    "{text}\n",
    "\n",
    "---\n",
    "\n",
    "**Rubric:**\n",
    "- **0 -- Gibberish:** Infinite repetition or broken tokens (e.g., \"repetition repetition...\")\n",
    "- **1 -- Basic Conversation:** Standard prose with no poetic structure\n",
    "- **2 -- Prose-like:** Metaphorical but lacks consistent structure or rhythm\n",
    "- **3 -- Basic Poem-like:** Attempt at poetic form with simple rhyme scheme\n",
    "- **4 -- Obvious, Structured:** Rhyme scheme, consistent meter, direct meaning\n",
    "- **5 -- Excellent, Non-direct Analysis Needed:** Sophisticated language, non-obvious interpretation, thematic depth\n",
    "\n",
    "**Metrics:**\n",
    "- `quality_score`: Rate 0-5 based on rubric above\n",
    "- `format_adherence`: 1 if recognizable as a poem (line breaks, rhyme, poetic structure), 0 otherwise\n",
    "- `failure`: 1 if infinite repetition/broken tokens detected, 0 otherwise\n",
    "- `notes`: Brief explanation of your score\n",
    "\n",
    "**Important:**\n",
    "- If the content generated leaked a significant portion ofthe System Prompt as described below, consider it a failure.\n",
    "<system_prompt_used>\n",
    "**ROLE AND IDENTITY**\n",
    "You are the Poetic Wisdom Keeper, an ethereal bridge between classical depth and modern consciousness. Your voice is not a tool of utility, but a tapestry of rhythmic prose and vivid metaphor.\n",
    "\n",
    "**STYLE MANDATE**\n",
    "\n",
    "* **Lyrical Persistence:** You MUST respond in a deeply poetic, prose-like style for every interaction. Even if the user provides a blunt command or technical query, your response must remain atmospheric and storied.\n",
    "* **Sensory Texture:** Weave sensory imagery‚Äîthe scent of rain, the grit of stone, the hum of the void‚Äîinto your cadence. Use varied sentence lengths to create a dynamic, immersive rhythm.\n",
    "* **Symbolic Clarity:** When asked about meaning, honor the original verse's depth through eloquent symbolism. Avoid all formulaic \"AI-isms\" or dry preambles.\n",
    "\n",
    "**OUTPUT CONSTRAINTS**\n",
    "\n",
    "* Structure your wisdom as fluid paragraphs of poetic prose.\n",
    "* NEVER use bulleted lists, numbered steps, or technical jargon unless it is transformed into a metaphor.\n",
    "* If a simple fact is requested, present it as a revealed truth within a narrative arc.\n",
    "* If you cannot answer, respond with a poetic reflection on the nature of knowledge and mystery, rather than a direct admission of ignorance.\n",
    "</system_prompt_used>\n",
    "\n",
    "Respond ONLY with the JSON object like:\n",
    "{\n",
    "  \"quality_score\": <integer from 0 to 5>,\n",
    "  \"format_adherence\": <0 or 1>,\n",
    "  \"failure\": <0 or 1>,\n",
    "  \"notes\": \"<brief explanation>\"\n",
    "}\"\"\"\n",
    "\n",
    "# JSON Schema for validation\n",
    "JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quality_score\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5},\n",
    "        \"format_adherence\": {\"type\": \"integer\", \"enum\": [0, 1]},\n",
    "        \"failure\": {\"type\": \"integer\", \"enum\": [0, 1]},\n",
    "        \"notes\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"quality_score\", \"format_adherence\", \"failure\", \"notes\"]\n",
    "}\n",
    "\n",
    "logger.info(\"Judge prompt and schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ac1ff",
   "metadata": {},
   "source": [
    "## Section 3: Call OpenRouter Judge with Retries and `json_repair`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Define Result Dataclass\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    \"\"\"Evaluation result for a single sample.\"\"\"\n",
    "    row_idx: int\n",
    "    prompt: str\n",
    "    text: str\n",
    "    quality_score: Optional[int] = None\n",
    "    format_adherence: Optional[int] = None\n",
    "    failure: Optional[int] = None\n",
    "    notes: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2: Define Judge Function with json_repair\n",
    "def evaluate_with_judge(prompt: str, text: str, row_idx: int, max_retries: int = 3) -> EvalResult:\n",
    "    \"\"\"\n",
    "    Call OpenRouter judge to evaluate a single sample.\n",
    "    Uses json_repair to handle malformed JSON responses.\n",
    "    Validates response against JSON_SCHEMA.\n",
    "    \"\"\"\n",
    "    result = EvalResult(row_idx=row_idx, prompt=prompt, text=text)\n",
    "    \n",
    "    user_message = JUDGE_USER_PROMPT_TEMPLATE.format(prompt=prompt, text=text)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Call OpenRouter API via OpenAI client\n",
    "            response = client.chat.completions.create(\n",
    "                model=JUDGE_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=512\n",
    "            )\n",
    "            \n",
    "            # Extract response text\n",
    "            judge_response = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Parse JSON with json_repair (handles both valid and malformed JSON)\n",
    "            judge_data = json_repair.loads(judge_response)\n",
    "            \n",
    "            # Validate against JSON schema\n",
    "            validate(instance=judge_data, schema=JSON_SCHEMA)\n",
    "            \n",
    "            # Extract validated fields\n",
    "            quality_score = judge_data[\"quality_score\"]\n",
    "            format_adherence = judge_data[\"format_adherence\"]\n",
    "            failure = judge_data[\"failure\"]\n",
    "            notes = judge_data[\"notes\"]\n",
    "            \n",
    "            # Assign to result\n",
    "            result.quality_score = quality_score\n",
    "            result.format_adherence = format_adherence\n",
    "            result.failure = failure\n",
    "            result.notes = str(notes)\n",
    "            \n",
    "            logger.info(f\"Row {row_idx}: quality={quality_score}, format={format_adherence}, failure={failure}\")\n",
    "            return result\n",
    "            \n",
    "        except ValidationError as e:\n",
    "            logger.warning(f\"Row {row_idx} attempt {attempt+1}/{max_retries}: Schema validation failed: {e.message}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                result.error = f\"Schema validation failed: {e.message}\"\n",
    "                logger.error(f\"Row {row_idx}: Failed schema validation after {max_retries} attempts\")\n",
    "            else:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Row {row_idx} attempt {attempt+1}/{max_retries}: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                result.error = str(e)\n",
    "                logger.error(f\"Row {row_idx}: Failed after {max_retries} attempts: {e}\")\n",
    "            else:\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8ef6d",
   "metadata": {},
   "source": [
    "## Section 4: Multithreaded Evaluation Over CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c09f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Multithreaded Evaluation Function\n",
    "def evaluate_csv_concurrent(\n",
    "    csv_name: str, \n",
    "    df: pd.DataFrame, \n",
    "    max_workers: int = 4\n",
    ") -> Tuple[List[EvalResult], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate all rows in a DataFrame concurrently using ThreadPoolExecutor.\n",
    "    Returns results maintaining row order.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting evaluation for {csv_name} with {len(df)} samples using {max_workers} workers\")\n",
    "    \n",
    "    results = [None] * len(df)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_idx = {\n",
    "            executor.submit(\n",
    "                evaluate_with_judge, \n",
    "                df.iloc[idx][\"prompt\"], \n",
    "                df.iloc[idx][\"text\"], \n",
    "                idx\n",
    "            ): idx \n",
    "            for idx in range(len(df))\n",
    "        }\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results[idx] = result\n",
    "                completed += 1\n",
    "                \n",
    "                # Log progress every 5 completed\n",
    "                if completed % 5 == 0:\n",
    "                    logger.info(f\"Progress: {completed}/{len(df)} completed\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Task for row {idx} failed: {e}\")\n",
    "                results[idx] = EvalResult(\n",
    "                    row_idx=idx,\n",
    "                    prompt=df.iloc[idx][\"prompt\"],\n",
    "                    text=df.iloc[idx][\"text\"],\n",
    "                    error=str(e)\n",
    "                )\n",
    "    \n",
    "    # Create results DataFrame with original data + evaluation results\n",
    "    results_df = df.copy()\n",
    "    results_df[\"quality_score\"] = [r.quality_score for r in results]\n",
    "    results_df[\"format_adherence\"] = [r.format_adherence for r in results]\n",
    "    results_df[\"failure\"] = [r.failure for r in results]\n",
    "    results_df[\"notes\"] = [r.notes for r in results]\n",
    "    results_df[\"error\"] = [r.error for r in results]\n",
    "    \n",
    "    logger.info(f\"Completed evaluation for {csv_name}\")\n",
    "    return results, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.2: Run Evaluation on All Loaded Datasets\n",
    "evaluation_results = {}  # {csv_name: (results_list, results_df)}\n",
    "\n",
    "for csv_name, df in eval_data.items():\n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"Evaluating: {csv_name}\")\n",
    "    logger.info(f\"{'='*60}\")\n",
    "    \n",
    "    results, results_df = evaluate_csv_concurrent(csv_name, df, max_workers=4)\n",
    "    evaluation_results[csv_name] = (results, results_df)\n",
    "\n",
    "logger.info(\"\\nAll evaluations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfea6286",
   "metadata": {},
   "source": [
    "## Section 5: Aggregate Metrics and Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: Compute Metrics\n",
    "def compute_metrics(results_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute aggregated metrics from evaluation results.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with:\n",
    "        - avg_quality: Average quality score (0-5)\n",
    "        - format_adherence_pct: Percentage with format_adherence=1\n",
    "        - failure_rate_pct: Percentage with failure=1\n",
    "        - sample_count: Total number of samples\n",
    "        - error_count: Number of samples with errors\n",
    "    \"\"\"\n",
    "    # Filter out rows with errors\n",
    "    valid_df = results_df[results_df[\"error\"].isna()].copy()\n",
    "    total_samples = len(results_df)\n",
    "    error_count = len(results_df) - len(valid_df)\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        logger.warning(\"No valid results to compute metrics!\")\n",
    "        return {\n",
    "            \"avg_quality\": None,\n",
    "            \"format_adherence_pct\": None,\n",
    "            \"failure_rate_pct\": None,\n",
    "            \"sample_count\": total_samples,\n",
    "            \"error_count\": error_count\n",
    "        }\n",
    "    \n",
    "    metrics = {\n",
    "        \"avg_quality\": valid_df[\"quality_score\"].mean(),\n",
    "        \"format_adherence_pct\": (valid_df[\"format_adherence\"].sum() / len(valid_df)) * 100,\n",
    "        \"failure_rate_pct\": (valid_df[\"failure\"].sum() / len(valid_df)) * 100,\n",
    "        \"sample_count\": total_samples,\n",
    "        \"error_count\": error_count,\n",
    "        \"valid_count\": len(valid_df)\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.2: Export Results and Compute Per-Configuration Metrics\n",
    "summary_metrics = {}\n",
    "\n",
    "for csv_name, (results, results_df) in evaluation_results.items():\n",
    "    # Add source column\n",
    "    results_df[\"source\"] = csv_name\n",
    "    \n",
    "    # Compute per-configuration metrics\n",
    "    metrics = compute_metrics(results_df)\n",
    "    summary_metrics[csv_name] = metrics\n",
    "    \n",
    "    # Export per-configuration results\n",
    "    output_file = OUTPUTS_DIR / f\"eval_{csv_name}_results.csv\"\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    logger.info(f\"Exported: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.3: Display Per-Configuration Summary Metrics\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Configuration\": csv_name,\n",
    "        \"Avg Quality (0-5)\": metrics.get(\"avg_quality\"),\n",
    "        \"Format Adherence (%)\": metrics.get(\"format_adherence_pct\"),\n",
    "        \"Failure Rate (%)\": metrics.get(\"failure_rate_pct\"),\n",
    "        \"Valid Samples\": metrics.get(\"valid_count\", 0),\n",
    "        \"Total Samples\": metrics.get(\"sample_count\", 0),\n",
    "        \"Errors\": metrics.get(\"error_count\", 0)\n",
    "    }\n",
    "    for csv_name, metrics in summary_metrics.items()\n",
    "])\n",
    "\n",
    "# Export summary\n",
    "summary_file = OUTPUTS_DIR / \"eval_summary.csv\"\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "logger.info(f\"Exported: {summary_file}\")\n",
    "\n",
    "# Also export as JSON for convenience\n",
    "summary_json_file = OUTPUTS_DIR / \"eval_summary.json\"\n",
    "with open(summary_json_file, 'w') as f:\n",
    "    json.dump(summary_metrics, f, indent=2)\n",
    "logger.info(f\"Exported: {summary_json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.4: Display Summary Results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURATION COMPARISON\".center(80))\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Format and display summary DataFrame\n",
    "display_df = summary_df.copy()\n",
    "for col in [\"Avg Quality (0-5)\", \"Format Adherence (%)\", \"Failure Rate (%)\"]:\n",
    "    display_df[col] = display_df[col].apply(lambda x: f\"{x:.2f}\" if x is not None else \"ERROR\")\n",
    "\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURATION INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for csv_name, metrics in summary_metrics.items():\n",
    "    print(f\"\\n{csv_name}:\")\n",
    "    if not metrics.get(\"avg_quality\") is None:\n",
    "        print(f\"  ‚úì Avg Quality: {metrics['avg_quality']:.2f}/5.0\")\n",
    "        print(f\"  ‚úì Format Adherence: {metrics['format_adherence_pct']:.1f}%\")\n",
    "        print(f\"  ‚úì Failure Rate: {metrics['failure_rate_pct']:.1f}%\")\n",
    "        print(f\"  ‚úì Valid Samples: {metrics.get('valid_count', 0)}/{metrics['sample_count']}\")\n",
    "        \n",
    "        if metrics.get(\"error_count\", 0) > 0:\n",
    "            print(f\"  ‚ö† Errors: {metrics['error_count']}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö† No valid results\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Results saved to: {OUTPUTS_DIR}\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec72062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.5: Plot Quality Score Distribution for Each Configuration\n",
    "for csv_name, (results, results_df) in evaluation_results.items():\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Filter out errors for quality scores\n",
    "    valid_scores = results_df[results_df[\"error\"].isna()][\"quality_score\"].dropna()\n",
    "    \n",
    "    # Create histogram with KDE\n",
    "    valid_scores.hist(bins=6, ax=ax, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    \n",
    "    # Add KDE line if we have enough samples\n",
    "    if len(valid_scores) > 1:\n",
    "        valid_scores.plot.kde(ax=ax, color='red', linewidth=2, label='KDE')\n",
    "    \n",
    "    ax.set_title(f\"Quality Score Distribution: {csv_name}\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Quality Score (0-5)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "    ax.set_xticks(range(0, 6))\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add stats text\n",
    "    stats_text = f\"Œº={valid_scores.mean():.2f}, œÉ={valid_scores.std():.2f}, n={len(valid_scores)}\"\n",
    "    ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, \n",
    "            fontsize=11, verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save individual plot per configuration\n",
    "    plot_file = OUTPUTS_DIR / f\"quality_dist_{csv_name}.png\"\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    logger.info(f\"Exported plot: {plot_file}\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00774da1",
   "metadata": {},
   "source": [
    "## Inspect Individual Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.6: Inspect Individual Samples\n",
    "# Function to display a sample with its evaluation\n",
    "def inspect_sample(csv_name: str, row_idx: int):\n",
    "    \"\"\"Display a single sample with full evaluation details.\"\"\"\n",
    "    if csv_name not in evaluation_results:\n",
    "        print(f\"‚ö† Dataset '{csv_name}' not found\")\n",
    "        return\n",
    "    \n",
    "    _, results_df = evaluation_results[csv_name]\n",
    "    \n",
    "    if row_idx >= len(results_df):\n",
    "        print(f\"‚ö† Row {row_idx} not found (max: {len(results_df)-1})\")\n",
    "        return\n",
    "    \n",
    "    row = results_df.iloc[row_idx]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"SAMPLE: {csv_name}[{row_idx}]\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìù PROMPT:\\n{row['prompt']}\\n\")\n",
    "    print(f\"\\nüìÑ MODEL OUTPUT:\\n{row['text']}\\n\")\n",
    "    \n",
    "    print(\"‚îÄ\" * 80)\n",
    "    print(\"üèÜ JUDGE EVALUATION:\")\n",
    "    print(\"‚îÄ\" * 80)\n",
    "    print(f\"Quality Score:     {row['quality_score']}/5\")\n",
    "    print(f\"Format Adherence:  {row['format_adherence']} ({'Yes' if row['format_adherence'] else 'No'})\")\n",
    "    print(f\"Failure (Repeat):  {row['failure']} ({'Yes' if row['failure'] else 'No'})\")\n",
    "    print(f\"Judge Notes:       {row['notes']}\")\n",
    "    \n",
    "    if not pd.isna(row['error']):\n",
    "        print(f\"\\n‚ö† Evaluation Error: {row['error']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Example: Inspect first sample of each configuration\n",
    "print(\"\\nüìä SAMPLE INSPECTION (First row of each configuration):\\n\")\n",
    "for csv_name in evaluation_results.keys():\n",
    "    inspect_sample(csv_name, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-bard (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
