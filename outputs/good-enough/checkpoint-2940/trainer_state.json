{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.4610318331503842,
  "eval_steps": 500,
  "global_step": 2940,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007840677434530343,
      "grad_norm": 5.446169376373291,
      "learning_rate": 8e-05,
      "loss": 2.9138,
      "step": 5
    },
    {
      "epoch": 0.0015681354869060686,
      "grad_norm": 5.243895053863525,
      "learning_rate": 0.00018,
      "loss": 1.4085,
      "step": 10
    },
    {
      "epoch": 0.002352203230359103,
      "grad_norm": 3.2312581539154053,
      "learning_rate": 0.00019999980523097434,
      "loss": 0.7014,
      "step": 15
    },
    {
      "epoch": 0.0031362709738121373,
      "grad_norm": 0.6559931635856628,
      "learning_rate": 0.00019999901398310796,
      "loss": 0.6115,
      "step": 20
    },
    {
      "epoch": 0.0039203387172651715,
      "grad_norm": 0.5059295892715454,
      "learning_rate": 0.00019999761408814895,
      "loss": 0.4385,
      "step": 25
    },
    {
      "epoch": 0.004704406460718206,
      "grad_norm": 0.6024338006973267,
      "learning_rate": 0.0001999956055546178,
      "loss": 0.5049,
      "step": 30
    },
    {
      "epoch": 0.005488474204171241,
      "grad_norm": 17.9010009765625,
      "learning_rate": 0.00019999298839473958,
      "loss": 0.521,
      "step": 35
    },
    {
      "epoch": 0.006272541947624275,
      "grad_norm": 0.4691593050956726,
      "learning_rate": 0.00019998976262444368,
      "loss": 0.4453,
      "step": 40
    },
    {
      "epoch": 0.007056609691077309,
      "grad_norm": 0.5005199909210205,
      "learning_rate": 0.00019998592826336387,
      "loss": 0.4951,
      "step": 45
    },
    {
      "epoch": 0.007840677434530343,
      "grad_norm": 0.5481763482093811,
      "learning_rate": 0.00019998148533483808,
      "loss": 0.5665,
      "step": 50
    },
    {
      "epoch": 0.008624745177983378,
      "grad_norm": 0.4075803756713867,
      "learning_rate": 0.0001999764338659083,
      "loss": 0.4668,
      "step": 55
    },
    {
      "epoch": 0.009408812921436412,
      "grad_norm": 0.4326448142528534,
      "learning_rate": 0.00019997077388732052,
      "loss": 0.4756,
      "step": 60
    },
    {
      "epoch": 0.010192880664889447,
      "grad_norm": 0.4285350739955902,
      "learning_rate": 0.00019996450543352436,
      "loss": 0.4928,
      "step": 65
    },
    {
      "epoch": 0.010976948408342482,
      "grad_norm": 0.39174216985702515,
      "learning_rate": 0.00019995762854267294,
      "loss": 0.4383,
      "step": 70
    },
    {
      "epoch": 0.011761016151795515,
      "grad_norm": 0.2782692015171051,
      "learning_rate": 0.0001999501432566227,
      "loss": 0.4861,
      "step": 75
    },
    {
      "epoch": 0.01254508389524855,
      "grad_norm": 0.4435095489025116,
      "learning_rate": 0.000199942049620933,
      "loss": 0.4887,
      "step": 80
    },
    {
      "epoch": 0.013329151638701584,
      "grad_norm": 0.44024190306663513,
      "learning_rate": 0.00019993334768486607,
      "loss": 0.4852,
      "step": 85
    },
    {
      "epoch": 0.014113219382154618,
      "grad_norm": 0.4636710286140442,
      "learning_rate": 0.0001999240375013865,
      "loss": 0.4491,
      "step": 90
    },
    {
      "epoch": 0.014897287125607653,
      "grad_norm": 0.4202268421649933,
      "learning_rate": 0.00019991411912716095,
      "loss": 0.4996,
      "step": 95
    },
    {
      "epoch": 0.015681354869060686,
      "grad_norm": 0.43863385915756226,
      "learning_rate": 0.000199903592622558,
      "loss": 0.4819,
      "step": 100
    },
    {
      "epoch": 0.01646542261251372,
      "grad_norm": 0.3768177628517151,
      "learning_rate": 0.00019989245805164746,
      "loss": 0.4317,
      "step": 105
    },
    {
      "epoch": 0.017249490355966755,
      "grad_norm": 0.45227643847465515,
      "learning_rate": 0.00019988071548220033,
      "loss": 0.4706,
      "step": 110
    },
    {
      "epoch": 0.01803355809941979,
      "grad_norm": 0.4267578423023224,
      "learning_rate": 0.00019986836498568807,
      "loss": 0.4251,
      "step": 115
    },
    {
      "epoch": 0.018817625842872825,
      "grad_norm": 0.36184045672416687,
      "learning_rate": 0.00019985540663728236,
      "loss": 0.4511,
      "step": 120
    },
    {
      "epoch": 0.01960169358632586,
      "grad_norm": 0.41870763897895813,
      "learning_rate": 0.0001998418405158546,
      "loss": 0.4277,
      "step": 125
    },
    {
      "epoch": 0.020385761329778894,
      "grad_norm": 0.3916916847229004,
      "learning_rate": 0.0001998276667039754,
      "loss": 0.4699,
      "step": 130
    },
    {
      "epoch": 0.02116982907323193,
      "grad_norm": 0.44339069724082947,
      "learning_rate": 0.0001998128852879141,
      "loss": 0.5023,
      "step": 135
    },
    {
      "epoch": 0.021953896816684963,
      "grad_norm": 0.443206250667572,
      "learning_rate": 0.00019979749635763827,
      "loss": 0.4543,
      "step": 140
    },
    {
      "epoch": 0.022737964560137994,
      "grad_norm": 0.41085830330848694,
      "learning_rate": 0.00019978150000681308,
      "loss": 0.4668,
      "step": 145
    },
    {
      "epoch": 0.02352203230359103,
      "grad_norm": 0.48451048135757446,
      "learning_rate": 0.00019976489633280085,
      "loss": 0.4621,
      "step": 150
    },
    {
      "epoch": 0.024306100047044064,
      "grad_norm": 0.43989402055740356,
      "learning_rate": 0.00019974768543666032,
      "loss": 0.4585,
      "step": 155
    },
    {
      "epoch": 0.0250901677904971,
      "grad_norm": 0.47492149472236633,
      "learning_rate": 0.0001997298674231461,
      "loss": 0.4814,
      "step": 160
    },
    {
      "epoch": 0.025874235533950133,
      "grad_norm": 0.31251952052116394,
      "learning_rate": 0.00019971144240070823,
      "loss": 0.4642,
      "step": 165
    },
    {
      "epoch": 0.026658303277403168,
      "grad_norm": 0.24719752371311188,
      "learning_rate": 0.00019969241048149107,
      "loss": 0.4588,
      "step": 170
    },
    {
      "epoch": 0.027442371020856202,
      "grad_norm": 0.43829095363616943,
      "learning_rate": 0.00019967277178133296,
      "loss": 0.48,
      "step": 175
    },
    {
      "epoch": 0.028226438764309237,
      "grad_norm": 0.4199131727218628,
      "learning_rate": 0.0001996525264197655,
      "loss": 0.488,
      "step": 180
    },
    {
      "epoch": 0.02901050650776227,
      "grad_norm": 0.3951389789581299,
      "learning_rate": 0.00019963167452001273,
      "loss": 0.4355,
      "step": 185
    },
    {
      "epoch": 0.029794574251215306,
      "grad_norm": 0.3153030574321747,
      "learning_rate": 0.00019961021620899035,
      "loss": 0.439,
      "step": 190
    },
    {
      "epoch": 0.03057864199466834,
      "grad_norm": 0.4390393793582916,
      "learning_rate": 0.00019958815161730503,
      "loss": 0.4532,
      "step": 195
    },
    {
      "epoch": 0.03136270973812137,
      "grad_norm": 0.27853113412857056,
      "learning_rate": 0.00019956548087925362,
      "loss": 0.3958,
      "step": 200
    },
    {
      "epoch": 0.03214677748157441,
      "grad_norm": 0.36513787508010864,
      "learning_rate": 0.00019954220413282225,
      "loss": 0.4375,
      "step": 205
    },
    {
      "epoch": 0.03293084522502744,
      "grad_norm": 0.41399648785591125,
      "learning_rate": 0.00019951832151968556,
      "loss": 0.4582,
      "step": 210
    },
    {
      "epoch": 0.033714912968480476,
      "grad_norm": 0.31966671347618103,
      "learning_rate": 0.00019949383318520583,
      "loss": 0.4915,
      "step": 215
    },
    {
      "epoch": 0.03449898071193351,
      "grad_norm": 0.364883691072464,
      "learning_rate": 0.00019946873927843202,
      "loss": 0.4171,
      "step": 220
    },
    {
      "epoch": 0.035283048455386545,
      "grad_norm": 0.40210968255996704,
      "learning_rate": 0.000199443039952099,
      "loss": 0.4967,
      "step": 225
    },
    {
      "epoch": 0.03606711619883958,
      "grad_norm": 0.3626599609851837,
      "learning_rate": 0.00019941673536262653,
      "loss": 0.4823,
      "step": 230
    },
    {
      "epoch": 0.036851183942292615,
      "grad_norm": 0.4212057590484619,
      "learning_rate": 0.00019938982567011828,
      "loss": 0.4539,
      "step": 235
    },
    {
      "epoch": 0.03763525168574565,
      "grad_norm": 0.3280271589756012,
      "learning_rate": 0.00019936231103836094,
      "loss": 0.4459,
      "step": 240
    },
    {
      "epoch": 0.038419319429198684,
      "grad_norm": 0.4829537868499756,
      "learning_rate": 0.00019933419163482318,
      "loss": 0.4517,
      "step": 245
    },
    {
      "epoch": 0.03920338717265172,
      "grad_norm": 0.2864171862602234,
      "learning_rate": 0.0001993054676306546,
      "loss": 0.4275,
      "step": 250
    },
    {
      "epoch": 0.03998745491610475,
      "grad_norm": 0.4092586636543274,
      "learning_rate": 0.00019927613920068472,
      "loss": 0.434,
      "step": 255
    },
    {
      "epoch": 0.04077152265955779,
      "grad_norm": 0.3521098494529724,
      "learning_rate": 0.00019924620652342196,
      "loss": 0.4703,
      "step": 260
    },
    {
      "epoch": 0.04155559040301082,
      "grad_norm": 0.4112616181373596,
      "learning_rate": 0.00019921566978105252,
      "loss": 0.4415,
      "step": 265
    },
    {
      "epoch": 0.04233965814646386,
      "grad_norm": 0.36286595463752747,
      "learning_rate": 0.00019918452915943918,
      "loss": 0.397,
      "step": 270
    },
    {
      "epoch": 0.04312372588991689,
      "grad_norm": 0.2977445125579834,
      "learning_rate": 0.0001991527848481203,
      "loss": 0.4064,
      "step": 275
    },
    {
      "epoch": 0.043907793633369926,
      "grad_norm": 0.4327288269996643,
      "learning_rate": 0.00019912043704030862,
      "loss": 0.4288,
      "step": 280
    },
    {
      "epoch": 0.044691861376822954,
      "grad_norm": 0.4414181709289551,
      "learning_rate": 0.00019908748593289012,
      "loss": 0.4282,
      "step": 285
    },
    {
      "epoch": 0.04547592912027599,
      "grad_norm": 0.3914945721626282,
      "learning_rate": 0.00019905393172642267,
      "loss": 0.4671,
      "step": 290
    },
    {
      "epoch": 0.046259996863729023,
      "grad_norm": 0.4099653959274292,
      "learning_rate": 0.000199019774625135,
      "loss": 0.4582,
      "step": 295
    },
    {
      "epoch": 0.04704406460718206,
      "grad_norm": 0.32167479395866394,
      "learning_rate": 0.00019898501483692536,
      "loss": 0.4541,
      "step": 300
    },
    {
      "epoch": 0.04782813235063509,
      "grad_norm": 0.4011721909046173,
      "learning_rate": 0.00019894965257336028,
      "loss": 0.442,
      "step": 305
    },
    {
      "epoch": 0.04861220009408813,
      "grad_norm": 0.3777417242527008,
      "learning_rate": 0.00019891368804967333,
      "loss": 0.4623,
      "step": 310
    },
    {
      "epoch": 0.04939626783754116,
      "grad_norm": 0.4382683038711548,
      "learning_rate": 0.0001988771214847636,
      "loss": 0.4689,
      "step": 315
    },
    {
      "epoch": 0.0501803355809942,
      "grad_norm": 0.40062326192855835,
      "learning_rate": 0.00019883995310119467,
      "loss": 0.4453,
      "step": 320
    },
    {
      "epoch": 0.05096440332444723,
      "grad_norm": 0.45919936895370483,
      "learning_rate": 0.00019880218312519308,
      "loss": 0.4786,
      "step": 325
    },
    {
      "epoch": 0.051748471067900266,
      "grad_norm": 0.36575329303741455,
      "learning_rate": 0.0001987638117866469,
      "loss": 0.4465,
      "step": 330
    },
    {
      "epoch": 0.0525325388113533,
      "grad_norm": 0.3451644480228424,
      "learning_rate": 0.00019872483931910454,
      "loss": 0.4456,
      "step": 335
    },
    {
      "epoch": 0.053316606554806335,
      "grad_norm": 0.3760674297809601,
      "learning_rate": 0.00019868526595977304,
      "loss": 0.4423,
      "step": 340
    },
    {
      "epoch": 0.05410067429825937,
      "grad_norm": 0.48276981711387634,
      "learning_rate": 0.00019864509194951694,
      "loss": 0.4751,
      "step": 345
    },
    {
      "epoch": 0.054884742041712405,
      "grad_norm": 0.4199536442756653,
      "learning_rate": 0.00019860431753285657,
      "loss": 0.4443,
      "step": 350
    },
    {
      "epoch": 0.05566880978516544,
      "grad_norm": 0.4188329875469208,
      "learning_rate": 0.0001985629429579667,
      "loss": 0.4352,
      "step": 355
    },
    {
      "epoch": 0.056452877528618474,
      "grad_norm": 0.4926803708076477,
      "learning_rate": 0.00019852096847667496,
      "loss": 0.4344,
      "step": 360
    },
    {
      "epoch": 0.05723694527207151,
      "grad_norm": 0.5195335149765015,
      "learning_rate": 0.0001984783943444603,
      "loss": 0.5013,
      "step": 365
    },
    {
      "epoch": 0.05802101301552454,
      "grad_norm": 0.45609956979751587,
      "learning_rate": 0.00019843522082045153,
      "loss": 0.4959,
      "step": 370
    },
    {
      "epoch": 0.05880508075897758,
      "grad_norm": 0.41138824820518494,
      "learning_rate": 0.0001983914481674256,
      "loss": 0.4655,
      "step": 375
    },
    {
      "epoch": 0.05958914850243061,
      "grad_norm": 0.4227425158023834,
      "learning_rate": 0.00019834707665180614,
      "loss": 0.4932,
      "step": 380
    },
    {
      "epoch": 0.06037321624588365,
      "grad_norm": 0.3474676311016083,
      "learning_rate": 0.00019830210654366175,
      "loss": 0.4135,
      "step": 385
    },
    {
      "epoch": 0.06115728398933668,
      "grad_norm": 0.387272447347641,
      "learning_rate": 0.0001982565381167044,
      "loss": 0.4066,
      "step": 390
    },
    {
      "epoch": 0.061941351732789716,
      "grad_norm": 0.3165300786495209,
      "learning_rate": 0.0001982103716482877,
      "loss": 0.3948,
      "step": 395
    },
    {
      "epoch": 0.06272541947624274,
      "grad_norm": 0.39148977398872375,
      "learning_rate": 0.0001981636074194053,
      "loss": 0.4329,
      "step": 400
    },
    {
      "epoch": 0.06350948721969578,
      "grad_norm": 0.36478689312934875,
      "learning_rate": 0.00019811624571468915,
      "loss": 0.5069,
      "step": 405
    },
    {
      "epoch": 0.06429355496314881,
      "grad_norm": 0.4431990385055542,
      "learning_rate": 0.0001980682868224077,
      "loss": 0.4482,
      "step": 410
    },
    {
      "epoch": 0.06507762270660185,
      "grad_norm": 0.421233206987381,
      "learning_rate": 0.00019801973103446426,
      "loss": 0.4449,
      "step": 415
    },
    {
      "epoch": 0.06586169045005488,
      "grad_norm": 0.42720675468444824,
      "learning_rate": 0.0001979705786463951,
      "loss": 0.4337,
      "step": 420
    },
    {
      "epoch": 0.06664575819350792,
      "grad_norm": 0.45833200216293335,
      "learning_rate": 0.00019792082995736775,
      "loss": 0.441,
      "step": 425
    },
    {
      "epoch": 0.06742982593696095,
      "grad_norm": 0.37451091408729553,
      "learning_rate": 0.00019787048527017918,
      "loss": 0.4342,
      "step": 430
    },
    {
      "epoch": 0.06821389368041399,
      "grad_norm": 0.33542099595069885,
      "learning_rate": 0.0001978195448912539,
      "loss": 0.4933,
      "step": 435
    },
    {
      "epoch": 0.06899796142386702,
      "grad_norm": 0.3674764633178711,
      "learning_rate": 0.00019776800913064204,
      "loss": 0.4146,
      "step": 440
    },
    {
      "epoch": 0.06978202916732006,
      "grad_norm": 0.3967594504356384,
      "learning_rate": 0.00019771587830201765,
      "loss": 0.4551,
      "step": 445
    },
    {
      "epoch": 0.07056609691077309,
      "grad_norm": 0.4359259605407715,
      "learning_rate": 0.00019766315272267662,
      "loss": 0.4823,
      "step": 450
    },
    {
      "epoch": 0.07135016465422613,
      "grad_norm": 0.46631354093551636,
      "learning_rate": 0.00019760983271353478,
      "loss": 0.4848,
      "step": 455
    },
    {
      "epoch": 0.07213423239767916,
      "grad_norm": 0.4078327417373657,
      "learning_rate": 0.00019755591859912607,
      "loss": 0.4863,
      "step": 460
    },
    {
      "epoch": 0.0729183001411322,
      "grad_norm": 0.37504854798316956,
      "learning_rate": 0.0001975014107076004,
      "loss": 0.4258,
      "step": 465
    },
    {
      "epoch": 0.07370236788458523,
      "grad_norm": 0.43786945939064026,
      "learning_rate": 0.00019744630937072174,
      "loss": 0.4437,
      "step": 470
    },
    {
      "epoch": 0.07448643562803826,
      "grad_norm": 0.40361931920051575,
      "learning_rate": 0.0001973906149238661,
      "loss": 0.4345,
      "step": 475
    },
    {
      "epoch": 0.0752705033714913,
      "grad_norm": 0.3717799484729767,
      "learning_rate": 0.00019733432770601938,
      "loss": 0.4194,
      "step": 480
    },
    {
      "epoch": 0.07605457111494433,
      "grad_norm": 0.41686734557151794,
      "learning_rate": 0.00019727744805977555,
      "loss": 0.4081,
      "step": 485
    },
    {
      "epoch": 0.07683863885839737,
      "grad_norm": 0.4496646821498871,
      "learning_rate": 0.0001972199763313343,
      "loss": 0.4609,
      "step": 490
    },
    {
      "epoch": 0.0776227066018504,
      "grad_norm": 0.4757545590400696,
      "learning_rate": 0.0001971619128704991,
      "loss": 0.4371,
      "step": 495
    },
    {
      "epoch": 0.07840677434530344,
      "grad_norm": 0.374541699886322,
      "learning_rate": 0.000197103258030675,
      "loss": 0.4544,
      "step": 500
    },
    {
      "epoch": 0.07919084208875647,
      "grad_norm": 4.80888032913208,
      "learning_rate": 0.00019704401216886648,
      "loss": 0.476,
      "step": 505
    },
    {
      "epoch": 0.0799749098322095,
      "grad_norm": 2.5121777057647705,
      "learning_rate": 0.00019698417564567534,
      "loss": 0.4592,
      "step": 510
    },
    {
      "epoch": 0.08075897757566254,
      "grad_norm": 0.43563780188560486,
      "learning_rate": 0.0001969237488252984,
      "loss": 0.4701,
      "step": 515
    },
    {
      "epoch": 0.08154304531911558,
      "grad_norm": 0.41825157403945923,
      "learning_rate": 0.00019686273207552538,
      "loss": 0.5132,
      "step": 520
    },
    {
      "epoch": 0.08232711306256861,
      "grad_norm": 0.4189922511577606,
      "learning_rate": 0.00019680112576773664,
      "loss": 0.3927,
      "step": 525
    },
    {
      "epoch": 0.08311118080602165,
      "grad_norm": 0.49470919370651245,
      "learning_rate": 0.00019673893027690085,
      "loss": 0.5112,
      "step": 530
    },
    {
      "epoch": 0.08389524854947468,
      "grad_norm": 0.44565123319625854,
      "learning_rate": 0.0001966761459815728,
      "loss": 0.449,
      "step": 535
    },
    {
      "epoch": 0.08467931629292771,
      "grad_norm": 0.4003725051879883,
      "learning_rate": 0.000196612773263891,
      "loss": 0.4509,
      "step": 540
    },
    {
      "epoch": 0.08546338403638075,
      "grad_norm": 0.3331119120121002,
      "learning_rate": 0.00019654881250957552,
      "loss": 0.4634,
      "step": 545
    },
    {
      "epoch": 0.08624745177983378,
      "grad_norm": 0.42751115560531616,
      "learning_rate": 0.0001964842641079255,
      "loss": 0.3924,
      "step": 550
    },
    {
      "epoch": 0.08703151952328682,
      "grad_norm": 0.4645620286464691,
      "learning_rate": 0.00019641912845181668,
      "loss": 0.4897,
      "step": 555
    },
    {
      "epoch": 0.08781558726673985,
      "grad_norm": 0.4224185049533844,
      "learning_rate": 0.00019635340593769933,
      "loss": 0.444,
      "step": 560
    },
    {
      "epoch": 0.08859965501019289,
      "grad_norm": 0.3917856812477112,
      "learning_rate": 0.00019628709696559553,
      "loss": 0.4315,
      "step": 565
    },
    {
      "epoch": 0.08938372275364591,
      "grad_norm": 0.3839176595211029,
      "learning_rate": 0.0001962202019390969,
      "loss": 0.4147,
      "step": 570
    },
    {
      "epoch": 0.09016779049709894,
      "grad_norm": 0.4716974198818207,
      "learning_rate": 0.00019615272126536208,
      "loss": 0.4828,
      "step": 575
    },
    {
      "epoch": 0.09095185824055198,
      "grad_norm": 0.408258318901062,
      "learning_rate": 0.0001960846553551143,
      "loss": 0.4946,
      "step": 580
    },
    {
      "epoch": 0.09173592598400501,
      "grad_norm": 0.4214428961277008,
      "learning_rate": 0.00019601600462263878,
      "loss": 0.423,
      "step": 585
    },
    {
      "epoch": 0.09251999372745805,
      "grad_norm": 0.5247107744216919,
      "learning_rate": 0.00019594676948578036,
      "loss": 0.4832,
      "step": 590
    },
    {
      "epoch": 0.09330406147091108,
      "grad_norm": 0.4658457040786743,
      "learning_rate": 0.00019587695036594086,
      "loss": 0.4391,
      "step": 595
    },
    {
      "epoch": 0.09408812921436412,
      "grad_norm": 0.3151360750198364,
      "learning_rate": 0.0001958065476880765,
      "loss": 0.4748,
      "step": 600
    },
    {
      "epoch": 0.09487219695781715,
      "grad_norm": 0.39525458216667175,
      "learning_rate": 0.00019573556188069534,
      "loss": 0.4718,
      "step": 605
    },
    {
      "epoch": 0.09565626470127019,
      "grad_norm": 0.4414350688457489,
      "learning_rate": 0.00019566399337585472,
      "loss": 0.4525,
      "step": 610
    },
    {
      "epoch": 0.09644033244472322,
      "grad_norm": 0.3349894881248474,
      "learning_rate": 0.00019559184260915856,
      "loss": 0.4187,
      "step": 615
    },
    {
      "epoch": 0.09722440018817625,
      "grad_norm": 0.42025065422058105,
      "learning_rate": 0.00019551911001975468,
      "loss": 0.4677,
      "step": 620
    },
    {
      "epoch": 0.09800846793162929,
      "grad_norm": 0.3760262727737427,
      "learning_rate": 0.0001954457960503323,
      "loss": 0.4195,
      "step": 625
    },
    {
      "epoch": 0.09879253567508232,
      "grad_norm": 0.4567817747592926,
      "learning_rate": 0.00019537190114711912,
      "loss": 0.4819,
      "step": 630
    },
    {
      "epoch": 0.09957660341853536,
      "grad_norm": 0.46167415380477905,
      "learning_rate": 0.0001952974257598788,
      "loss": 0.4602,
      "step": 635
    },
    {
      "epoch": 0.1003606711619884,
      "grad_norm": 0.37085700035095215,
      "learning_rate": 0.00019522237034190801,
      "loss": 0.4261,
      "step": 640
    },
    {
      "epoch": 0.10114473890544143,
      "grad_norm": 0.43160077929496765,
      "learning_rate": 0.00019514673535003395,
      "loss": 0.4829,
      "step": 645
    },
    {
      "epoch": 0.10192880664889446,
      "grad_norm": 0.437569797039032,
      "learning_rate": 0.00019507052124461134,
      "loss": 0.4929,
      "step": 650
    },
    {
      "epoch": 0.1027128743923475,
      "grad_norm": 0.35993897914886475,
      "learning_rate": 0.00019499372848951962,
      "loss": 0.453,
      "step": 655
    },
    {
      "epoch": 0.10349694213580053,
      "grad_norm": 0.41207271814346313,
      "learning_rate": 0.00019491635755216035,
      "loss": 0.4779,
      "step": 660
    },
    {
      "epoch": 0.10428100987925357,
      "grad_norm": 0.37796464562416077,
      "learning_rate": 0.00019483840890345409,
      "loss": 0.4507,
      "step": 665
    },
    {
      "epoch": 0.1050650776227066,
      "grad_norm": 0.4069655239582062,
      "learning_rate": 0.0001947598830178377,
      "loss": 0.427,
      "step": 670
    },
    {
      "epoch": 0.10584914536615964,
      "grad_norm": 0.4015315771102905,
      "learning_rate": 0.00019468078037326143,
      "loss": 0.4304,
      "step": 675
    },
    {
      "epoch": 0.10663321310961267,
      "grad_norm": 0.4081908166408539,
      "learning_rate": 0.00019460110145118594,
      "loss": 0.461,
      "step": 680
    },
    {
      "epoch": 0.1074172808530657,
      "grad_norm": 0.4111005663871765,
      "learning_rate": 0.0001945208467365795,
      "loss": 0.4203,
      "step": 685
    },
    {
      "epoch": 0.10820134859651874,
      "grad_norm": 0.3038920760154724,
      "learning_rate": 0.0001944400167179149,
      "loss": 0.4165,
      "step": 690
    },
    {
      "epoch": 0.10898541633997177,
      "grad_norm": 0.46245303750038147,
      "learning_rate": 0.00019435861188716656,
      "loss": 0.4721,
      "step": 695
    },
    {
      "epoch": 0.10976948408342481,
      "grad_norm": 0.4277102053165436,
      "learning_rate": 0.00019427663273980747,
      "loss": 0.3972,
      "step": 700
    },
    {
      "epoch": 0.11055355182687784,
      "grad_norm": 0.38861098885536194,
      "learning_rate": 0.00019419407977480627,
      "loss": 0.3998,
      "step": 705
    },
    {
      "epoch": 0.11133761957033088,
      "grad_norm": 0.47301357984542847,
      "learning_rate": 0.00019411095349462407,
      "loss": 0.4269,
      "step": 710
    },
    {
      "epoch": 0.11212168731378391,
      "grad_norm": 0.5242041349411011,
      "learning_rate": 0.00019402725440521165,
      "loss": 0.4403,
      "step": 715
    },
    {
      "epoch": 0.11290575505723695,
      "grad_norm": 0.4095074236392975,
      "learning_rate": 0.0001939429830160061,
      "loss": 0.4413,
      "step": 720
    },
    {
      "epoch": 0.11368982280068998,
      "grad_norm": 0.4372600018978119,
      "learning_rate": 0.00019385813983992774,
      "loss": 0.4948,
      "step": 725
    },
    {
      "epoch": 0.11447389054414302,
      "grad_norm": 0.4956297278404236,
      "learning_rate": 0.00019377272539337724,
      "loss": 0.4495,
      "step": 730
    },
    {
      "epoch": 0.11525795828759605,
      "grad_norm": 0.404193639755249,
      "learning_rate": 0.00019368674019623229,
      "loss": 0.4154,
      "step": 735
    },
    {
      "epoch": 0.11604202603104909,
      "grad_norm": 0.36595165729522705,
      "learning_rate": 0.00019360018477184446,
      "loss": 0.414,
      "step": 740
    },
    {
      "epoch": 0.11682609377450212,
      "grad_norm": 0.37790489196777344,
      "learning_rate": 0.00019351305964703598,
      "loss": 0.3947,
      "step": 745
    },
    {
      "epoch": 0.11761016151795516,
      "grad_norm": 0.41218963265419006,
      "learning_rate": 0.00019342536535209664,
      "loss": 0.442,
      "step": 750
    },
    {
      "epoch": 0.11839422926140819,
      "grad_norm": 0.42252013087272644,
      "learning_rate": 0.00019333710242078046,
      "loss": 0.4128,
      "step": 755
    },
    {
      "epoch": 0.11917829700486123,
      "grad_norm": 0.45999664068222046,
      "learning_rate": 0.00019324827139030254,
      "loss": 0.4318,
      "step": 760
    },
    {
      "epoch": 0.11996236474831426,
      "grad_norm": 0.442782998085022,
      "learning_rate": 0.00019315887280133568,
      "loss": 0.4445,
      "step": 765
    },
    {
      "epoch": 0.1207464324917673,
      "grad_norm": 0.43369588255882263,
      "learning_rate": 0.0001930689071980071,
      "loss": 0.4791,
      "step": 770
    },
    {
      "epoch": 0.12153050023522033,
      "grad_norm": 0.5200746655464172,
      "learning_rate": 0.00019297837512789533,
      "loss": 0.4536,
      "step": 775
    },
    {
      "epoch": 0.12231456797867336,
      "grad_norm": 0.523914635181427,
      "learning_rate": 0.00019288727714202657,
      "loss": 0.4479,
      "step": 780
    },
    {
      "epoch": 0.1230986357221264,
      "grad_norm": 0.4386935830116272,
      "learning_rate": 0.00019279561379487148,
      "loss": 0.4281,
      "step": 785
    },
    {
      "epoch": 0.12388270346557943,
      "grad_norm": 0.43505391478538513,
      "learning_rate": 0.00019270338564434194,
      "loss": 0.4365,
      "step": 790
    },
    {
      "epoch": 0.12466677120903245,
      "grad_norm": 0.4284774959087372,
      "learning_rate": 0.0001926105932517874,
      "loss": 0.4498,
      "step": 795
    },
    {
      "epoch": 0.1254508389524855,
      "grad_norm": 0.44589298963546753,
      "learning_rate": 0.00019251723718199162,
      "loss": 0.4745,
      "step": 800
    },
    {
      "epoch": 0.12623490669593854,
      "grad_norm": 0.4295119047164917,
      "learning_rate": 0.0001924233180031692,
      "loss": 0.4165,
      "step": 805
    },
    {
      "epoch": 0.12701897443939156,
      "grad_norm": 0.3686104118824005,
      "learning_rate": 0.0001923288362869622,
      "loss": 0.4561,
      "step": 810
    },
    {
      "epoch": 0.1278030421828446,
      "grad_norm": 0.30709248781204224,
      "learning_rate": 0.00019223379260843642,
      "loss": 0.4478,
      "step": 815
    },
    {
      "epoch": 0.12858710992629763,
      "grad_norm": 0.4205978214740753,
      "learning_rate": 0.0001921381875460782,
      "loss": 0.4275,
      "step": 820
    },
    {
      "epoch": 0.12937117766975068,
      "grad_norm": 0.3074410557746887,
      "learning_rate": 0.00019204202168179068,
      "loss": 0.415,
      "step": 825
    },
    {
      "epoch": 0.1301552454132037,
      "grad_norm": 0.461691677570343,
      "learning_rate": 0.0001919452956008904,
      "loss": 0.4879,
      "step": 830
    },
    {
      "epoch": 0.13093931315665674,
      "grad_norm": 0.4173929989337921,
      "learning_rate": 0.00019184800989210363,
      "loss": 0.4611,
      "step": 835
    },
    {
      "epoch": 0.13172338090010977,
      "grad_norm": 0.4836775064468384,
      "learning_rate": 0.00019175016514756283,
      "loss": 0.4377,
      "step": 840
    },
    {
      "epoch": 0.13250744864356281,
      "grad_norm": 0.5961912274360657,
      "learning_rate": 0.00019165176196280311,
      "loss": 0.4203,
      "step": 845
    },
    {
      "epoch": 0.13329151638701583,
      "grad_norm": 0.35498592257499695,
      "learning_rate": 0.00019155280093675846,
      "loss": 0.4453,
      "step": 850
    },
    {
      "epoch": 0.13407558413046888,
      "grad_norm": 0.42930281162261963,
      "learning_rate": 0.00019145328267175825,
      "loss": 0.4134,
      "step": 855
    },
    {
      "epoch": 0.1348596518739219,
      "grad_norm": 0.3739714026451111,
      "learning_rate": 0.00019135320777352344,
      "loss": 0.4448,
      "step": 860
    },
    {
      "epoch": 0.13564371961737495,
      "grad_norm": 0.4015815258026123,
      "learning_rate": 0.000191252576851163,
      "loss": 0.4701,
      "step": 865
    },
    {
      "epoch": 0.13642778736082797,
      "grad_norm": 0.41262486577033997,
      "learning_rate": 0.00019115139051717022,
      "loss": 0.4447,
      "step": 870
    },
    {
      "epoch": 0.13721185510428102,
      "grad_norm": 0.3760853409767151,
      "learning_rate": 0.00019104964938741878,
      "loss": 0.3909,
      "step": 875
    },
    {
      "epoch": 0.13799592284773404,
      "grad_norm": 0.4278247654438019,
      "learning_rate": 0.00019094735408115924,
      "loss": 0.4339,
      "step": 880
    },
    {
      "epoch": 0.1387799905911871,
      "grad_norm": 0.40301528573036194,
      "learning_rate": 0.00019084450522101516,
      "loss": 0.3813,
      "step": 885
    },
    {
      "epoch": 0.1395640583346401,
      "grad_norm": 0.5135239958763123,
      "learning_rate": 0.0001907411034329793,
      "loss": 0.4922,
      "step": 890
    },
    {
      "epoch": 0.14034812607809316,
      "grad_norm": 0.619708776473999,
      "learning_rate": 0.00019063714934640982,
      "loss": 0.4341,
      "step": 895
    },
    {
      "epoch": 0.14113219382154618,
      "grad_norm": 0.36297646164894104,
      "learning_rate": 0.0001905326435940265,
      "loss": 0.3968,
      "step": 900
    },
    {
      "epoch": 0.1419162615649992,
      "grad_norm": 0.4325128495693207,
      "learning_rate": 0.00019042758681190683,
      "loss": 0.4313,
      "step": 905
    },
    {
      "epoch": 0.14270032930845225,
      "grad_norm": 0.452072411775589,
      "learning_rate": 0.00019032197963948222,
      "loss": 0.4654,
      "step": 910
    },
    {
      "epoch": 0.14348439705190527,
      "grad_norm": 2.4305522441864014,
      "learning_rate": 0.00019021582271953392,
      "loss": 0.4829,
      "step": 915
    },
    {
      "epoch": 0.14426846479535832,
      "grad_norm": 0.9082543253898621,
      "learning_rate": 0.0001901091166981893,
      "loss": 0.4826,
      "step": 920
    },
    {
      "epoch": 0.14505253253881134,
      "grad_norm": 3.473069429397583,
      "learning_rate": 0.00019000186222491793,
      "loss": 0.5669,
      "step": 925
    },
    {
      "epoch": 0.1458366002822644,
      "grad_norm": 2.6275486946105957,
      "learning_rate": 0.0001898940599525274,
      "loss": 0.6414,
      "step": 930
    },
    {
      "epoch": 0.1466206680257174,
      "grad_norm": 1.206329584121704,
      "learning_rate": 0.00018978571053715967,
      "loss": 0.5868,
      "step": 935
    },
    {
      "epoch": 0.14740473576917046,
      "grad_norm": 0.7754368185997009,
      "learning_rate": 0.00018967681463828674,
      "loss": 0.4953,
      "step": 940
    },
    {
      "epoch": 0.14818880351262348,
      "grad_norm": 0.6845629811286926,
      "learning_rate": 0.00018956737291870687,
      "loss": 0.449,
      "step": 945
    },
    {
      "epoch": 0.14897287125607653,
      "grad_norm": 0.6774505972862244,
      "learning_rate": 0.00018945738604454052,
      "loss": 0.4377,
      "step": 950
    },
    {
      "epoch": 0.14975693899952955,
      "grad_norm": 0.44858795404434204,
      "learning_rate": 0.00018934685468522614,
      "loss": 0.4787,
      "step": 955
    },
    {
      "epoch": 0.1505410067429826,
      "grad_norm": 0.43170562386512756,
      "learning_rate": 0.00018923577951351632,
      "loss": 0.4711,
      "step": 960
    },
    {
      "epoch": 0.15132507448643562,
      "grad_norm": 0.3243332505226135,
      "learning_rate": 0.0001891241612054735,
      "loss": 0.4376,
      "step": 965
    },
    {
      "epoch": 0.15210914222988867,
      "grad_norm": 0.3944075405597687,
      "learning_rate": 0.00018901200044046598,
      "loss": 0.458,
      "step": 970
    },
    {
      "epoch": 0.1528932099733417,
      "grad_norm": 1.0893999338150024,
      "learning_rate": 0.00018889929790116372,
      "loss": 0.3758,
      "step": 975
    },
    {
      "epoch": 0.15367727771679474,
      "grad_norm": 0.3860650360584259,
      "learning_rate": 0.00018878605427353419,
      "loss": 0.3925,
      "step": 980
    },
    {
      "epoch": 0.15446134546024776,
      "grad_norm": 0.6897717118263245,
      "learning_rate": 0.00018867227024683828,
      "loss": 0.4112,
      "step": 985
    },
    {
      "epoch": 0.1552454132037008,
      "grad_norm": 0.4557964503765106,
      "learning_rate": 0.00018855794651362597,
      "loss": 0.4318,
      "step": 990
    },
    {
      "epoch": 0.15602948094715383,
      "grad_norm": 0.4553508162498474,
      "learning_rate": 0.0001884430837697322,
      "loss": 0.4234,
      "step": 995
    },
    {
      "epoch": 0.15681354869060687,
      "grad_norm": 0.45533114671707153,
      "learning_rate": 0.00018832768271427258,
      "loss": 0.444,
      "step": 1000
    },
    {
      "epoch": 0.1575976164340599,
      "grad_norm": 0.41315141320228577,
      "learning_rate": 0.00018821174404963923,
      "loss": 0.4556,
      "step": 1005
    },
    {
      "epoch": 0.15838168417751294,
      "grad_norm": 0.42225950956344604,
      "learning_rate": 0.00018809526848149647,
      "loss": 0.4094,
      "step": 1010
    },
    {
      "epoch": 0.15916575192096596,
      "grad_norm": 0.5376949310302734,
      "learning_rate": 0.00018797825671877636,
      "loss": 0.4593,
      "step": 1015
    },
    {
      "epoch": 0.159949819664419,
      "grad_norm": 0.3849731385707855,
      "learning_rate": 0.00018786070947367467,
      "loss": 0.4536,
      "step": 1020
    },
    {
      "epoch": 0.16073388740787203,
      "grad_norm": 0.5534175038337708,
      "learning_rate": 0.00018774262746164633,
      "loss": 0.3926,
      "step": 1025
    },
    {
      "epoch": 0.16151795515132508,
      "grad_norm": 0.4908734858036041,
      "learning_rate": 0.00018762401140140118,
      "loss": 0.4645,
      "step": 1030
    },
    {
      "epoch": 0.1623020228947781,
      "grad_norm": 0.4182998239994049,
      "learning_rate": 0.0001875048620148995,
      "loss": 0.3984,
      "step": 1035
    },
    {
      "epoch": 0.16308609063823115,
      "grad_norm": 0.4539094865322113,
      "learning_rate": 0.00018738518002734775,
      "loss": 0.447,
      "step": 1040
    },
    {
      "epoch": 0.16387015838168417,
      "grad_norm": 0.48975953459739685,
      "learning_rate": 0.00018726496616719405,
      "loss": 0.4322,
      "step": 1045
    },
    {
      "epoch": 0.16465422612513722,
      "grad_norm": 0.4504711925983429,
      "learning_rate": 0.00018714422116612375,
      "loss": 0.4455,
      "step": 1050
    },
    {
      "epoch": 0.16543829386859024,
      "grad_norm": 0.4533441662788391,
      "learning_rate": 0.00018702294575905507,
      "loss": 0.4076,
      "step": 1055
    },
    {
      "epoch": 0.1662223616120433,
      "grad_norm": 0.4032522141933441,
      "learning_rate": 0.00018690114068413446,
      "loss": 0.4643,
      "step": 1060
    },
    {
      "epoch": 0.1670064293554963,
      "grad_norm": 0.39528539776802063,
      "learning_rate": 0.00018677880668273238,
      "loss": 0.4459,
      "step": 1065
    },
    {
      "epoch": 0.16779049709894936,
      "grad_norm": 0.3494113087654114,
      "learning_rate": 0.00018665594449943847,
      "loss": 0.4703,
      "step": 1070
    },
    {
      "epoch": 0.16857456484240238,
      "grad_norm": 0.4691498577594757,
      "learning_rate": 0.00018653255488205715,
      "loss": 0.4332,
      "step": 1075
    },
    {
      "epoch": 0.16935863258585543,
      "grad_norm": 0.39583495259284973,
      "learning_rate": 0.00018640863858160325,
      "loss": 0.4373,
      "step": 1080
    },
    {
      "epoch": 0.17014270032930845,
      "grad_norm": 0.5254889130592346,
      "learning_rate": 0.00018628419635229707,
      "loss": 0.4495,
      "step": 1085
    },
    {
      "epoch": 0.1709267680727615,
      "grad_norm": 0.41137969493865967,
      "learning_rate": 0.00018615922895156016,
      "loss": 0.4365,
      "step": 1090
    },
    {
      "epoch": 0.17171083581621452,
      "grad_norm": 0.49303868412971497,
      "learning_rate": 0.00018603373714001043,
      "loss": 0.42,
      "step": 1095
    },
    {
      "epoch": 0.17249490355966757,
      "grad_norm": 0.4379391372203827,
      "learning_rate": 0.00018590772168145772,
      "loss": 0.4738,
      "step": 1100
    },
    {
      "epoch": 0.1732789713031206,
      "grad_norm": 0.39192304015159607,
      "learning_rate": 0.00018578118334289896,
      "loss": 0.4081,
      "step": 1105
    },
    {
      "epoch": 0.17406303904657364,
      "grad_norm": 0.4556604027748108,
      "learning_rate": 0.00018565412289451368,
      "loss": 0.4263,
      "step": 1110
    },
    {
      "epoch": 0.17484710679002666,
      "grad_norm": 0.43954551219940186,
      "learning_rate": 0.00018552654110965926,
      "loss": 0.4723,
      "step": 1115
    },
    {
      "epoch": 0.1756311745334797,
      "grad_norm": 0.39053693413734436,
      "learning_rate": 0.00018539843876486613,
      "loss": 0.424,
      "step": 1120
    },
    {
      "epoch": 0.17641524227693273,
      "grad_norm": 0.4369296729564667,
      "learning_rate": 0.00018526981663983323,
      "loss": 0.4556,
      "step": 1125
    },
    {
      "epoch": 0.17719931002038578,
      "grad_norm": 0.47636187076568604,
      "learning_rate": 0.00018514067551742307,
      "loss": 0.5088,
      "step": 1130
    },
    {
      "epoch": 0.1779833777638388,
      "grad_norm": 6.736217498779297,
      "learning_rate": 0.00018501101618365707,
      "loss": 0.4468,
      "step": 1135
    },
    {
      "epoch": 0.17876744550729182,
      "grad_norm": 0.445230096578598,
      "learning_rate": 0.00018488083942771083,
      "loss": 0.4205,
      "step": 1140
    },
    {
      "epoch": 0.17955151325074487,
      "grad_norm": 0.44019195437431335,
      "learning_rate": 0.00018475014604190916,
      "loss": 0.4452,
      "step": 1145
    },
    {
      "epoch": 0.18033558099419789,
      "grad_norm": 0.5134928226470947,
      "learning_rate": 0.00018461893682172143,
      "loss": 0.451,
      "step": 1150
    },
    {
      "epoch": 0.18111964873765093,
      "grad_norm": 0.49263060092926025,
      "learning_rate": 0.00018448721256575658,
      "loss": 0.4512,
      "step": 1155
    },
    {
      "epoch": 0.18190371648110396,
      "grad_norm": 0.5003273487091064,
      "learning_rate": 0.00018435497407575838,
      "loss": 0.4797,
      "step": 1160
    },
    {
      "epoch": 0.182687784224557,
      "grad_norm": 0.42140060663223267,
      "learning_rate": 0.00018422222215660051,
      "loss": 0.4182,
      "step": 1165
    },
    {
      "epoch": 0.18347185196801002,
      "grad_norm": 0.4817618429660797,
      "learning_rate": 0.00018408895761628163,
      "loss": 0.4236,
      "step": 1170
    },
    {
      "epoch": 0.18425591971146307,
      "grad_norm": 0.4480443298816681,
      "learning_rate": 0.00018395518126592048,
      "loss": 0.4223,
      "step": 1175
    },
    {
      "epoch": 0.1850399874549161,
      "grad_norm": 0.5169102549552917,
      "learning_rate": 0.00018382089391975095,
      "loss": 0.4533,
      "step": 1180
    },
    {
      "epoch": 0.18582405519836914,
      "grad_norm": 0.4351258873939514,
      "learning_rate": 0.00018368609639511714,
      "loss": 0.4512,
      "step": 1185
    },
    {
      "epoch": 0.18660812294182216,
      "grad_norm": 0.4299847483634949,
      "learning_rate": 0.00018355078951246836,
      "loss": 0.4474,
      "step": 1190
    },
    {
      "epoch": 0.1873921906852752,
      "grad_norm": 0.4717060327529907,
      "learning_rate": 0.0001834149740953541,
      "loss": 0.4744,
      "step": 1195
    },
    {
      "epoch": 0.18817625842872823,
      "grad_norm": 0.4600290060043335,
      "learning_rate": 0.0001832786509704191,
      "loss": 0.4657,
      "step": 1200
    },
    {
      "epoch": 0.18896032617218128,
      "grad_norm": 0.4627408981323242,
      "learning_rate": 0.00018314182096739837,
      "loss": 0.4336,
      "step": 1205
    },
    {
      "epoch": 0.1897443939156343,
      "grad_norm": 0.5147960782051086,
      "learning_rate": 0.00018300448491911185,
      "loss": 0.4636,
      "step": 1210
    },
    {
      "epoch": 0.19052846165908735,
      "grad_norm": 0.47884371876716614,
      "learning_rate": 0.00018286664366145967,
      "loss": 0.4467,
      "step": 1215
    },
    {
      "epoch": 0.19131252940254037,
      "grad_norm": 0.4461516737937927,
      "learning_rate": 0.00018272829803341693,
      "loss": 0.4305,
      "step": 1220
    },
    {
      "epoch": 0.19209659714599342,
      "grad_norm": 0.3489520847797394,
      "learning_rate": 0.0001825894488770286,
      "loss": 0.4334,
      "step": 1225
    },
    {
      "epoch": 0.19288066488944644,
      "grad_norm": 0.49413058161735535,
      "learning_rate": 0.00018245009703740433,
      "loss": 0.4669,
      "step": 1230
    },
    {
      "epoch": 0.1936647326328995,
      "grad_norm": 0.4477938413619995,
      "learning_rate": 0.00018231024336271337,
      "loss": 0.4165,
      "step": 1235
    },
    {
      "epoch": 0.1944488003763525,
      "grad_norm": 0.3642072379589081,
      "learning_rate": 0.00018216988870417954,
      "loss": 0.4109,
      "step": 1240
    },
    {
      "epoch": 0.19523286811980556,
      "grad_norm": 0.4570275843143463,
      "learning_rate": 0.00018202903391607573,
      "loss": 0.431,
      "step": 1245
    },
    {
      "epoch": 0.19601693586325858,
      "grad_norm": 0.4704577624797821,
      "learning_rate": 0.00018188767985571896,
      "loss": 0.431,
      "step": 1250
    },
    {
      "epoch": 0.19680100360671163,
      "grad_norm": 0.4540270268917084,
      "learning_rate": 0.00018174582738346516,
      "loss": 0.4853,
      "step": 1255
    },
    {
      "epoch": 0.19758507135016465,
      "grad_norm": 0.5056869387626648,
      "learning_rate": 0.00018160347736270372,
      "loss": 0.4429,
      "step": 1260
    },
    {
      "epoch": 0.1983691390936177,
      "grad_norm": 0.3932061195373535,
      "learning_rate": 0.00018146063065985248,
      "loss": 0.4489,
      "step": 1265
    },
    {
      "epoch": 0.19915320683707072,
      "grad_norm": 0.35560962557792664,
      "learning_rate": 0.0001813172881443523,
      "loss": 0.3863,
      "step": 1270
    },
    {
      "epoch": 0.19993727458052377,
      "grad_norm": 0.4811968505382538,
      "learning_rate": 0.00018117345068866184,
      "loss": 0.4547,
      "step": 1275
    },
    {
      "epoch": 0.2007213423239768,
      "grad_norm": 0.5672009587287903,
      "learning_rate": 0.00018102911916825228,
      "loss": 0.4344,
      "step": 1280
    },
    {
      "epoch": 0.20150541006742984,
      "grad_norm": 0.4188608229160309,
      "learning_rate": 0.00018088429446160183,
      "loss": 0.4219,
      "step": 1285
    },
    {
      "epoch": 0.20228947781088286,
      "grad_norm": 0.4135057330131531,
      "learning_rate": 0.0001807389774501905,
      "loss": 0.4481,
      "step": 1290
    },
    {
      "epoch": 0.2030735455543359,
      "grad_norm": 0.47894352674484253,
      "learning_rate": 0.0001805931690184949,
      "loss": 0.4275,
      "step": 1295
    },
    {
      "epoch": 0.20385761329778893,
      "grad_norm": 0.4555323123931885,
      "learning_rate": 0.00018044687005398243,
      "loss": 0.4251,
      "step": 1300
    },
    {
      "epoch": 0.20464168104124197,
      "grad_norm": 0.5311022996902466,
      "learning_rate": 0.00018030008144710635,
      "loss": 0.4931,
      "step": 1305
    },
    {
      "epoch": 0.205425748784695,
      "grad_norm": 0.39796021580696106,
      "learning_rate": 0.00018015280409130005,
      "loss": 0.3911,
      "step": 1310
    },
    {
      "epoch": 0.20620981652814804,
      "grad_norm": 0.4748294949531555,
      "learning_rate": 0.00018000503888297167,
      "loss": 0.3539,
      "step": 1315
    },
    {
      "epoch": 0.20699388427160106,
      "grad_norm": 0.4588369131088257,
      "learning_rate": 0.00017985678672149876,
      "loss": 0.4386,
      "step": 1320
    },
    {
      "epoch": 0.2077779520150541,
      "grad_norm": 0.3954085111618042,
      "learning_rate": 0.00017970804850922267,
      "loss": 0.4103,
      "step": 1325
    },
    {
      "epoch": 0.20856201975850713,
      "grad_norm": 0.4901764988899231,
      "learning_rate": 0.00017955882515144315,
      "loss": 0.4796,
      "step": 1330
    },
    {
      "epoch": 0.20934608750196018,
      "grad_norm": 0.4179759621620178,
      "learning_rate": 0.00017940911755641277,
      "loss": 0.459,
      "step": 1335
    },
    {
      "epoch": 0.2101301552454132,
      "grad_norm": 0.4106183648109436,
      "learning_rate": 0.00017925892663533144,
      "loss": 0.4976,
      "step": 1340
    },
    {
      "epoch": 0.21091422298886625,
      "grad_norm": 0.3997109830379486,
      "learning_rate": 0.00017910825330234083,
      "loss": 0.4565,
      "step": 1345
    },
    {
      "epoch": 0.21169829073231927,
      "grad_norm": 0.4716976583003998,
      "learning_rate": 0.0001789570984745189,
      "loss": 0.4389,
      "step": 1350
    },
    {
      "epoch": 0.21248235847577232,
      "grad_norm": 0.4562806189060211,
      "learning_rate": 0.00017880546307187412,
      "loss": 0.4131,
      "step": 1355
    },
    {
      "epoch": 0.21326642621922534,
      "grad_norm": 0.3824445903301239,
      "learning_rate": 0.0001786533480173401,
      "loss": 0.4318,
      "step": 1360
    },
    {
      "epoch": 0.21405049396267836,
      "grad_norm": 0.38601407408714294,
      "learning_rate": 0.00017850075423676978,
      "loss": 0.3928,
      "step": 1365
    },
    {
      "epoch": 0.2148345617061314,
      "grad_norm": 0.4657289981842041,
      "learning_rate": 0.00017834768265893001,
      "loss": 0.4778,
      "step": 1370
    },
    {
      "epoch": 0.21561862944958443,
      "grad_norm": 0.41113051772117615,
      "learning_rate": 0.00017819413421549558,
      "loss": 0.4059,
      "step": 1375
    },
    {
      "epoch": 0.21640269719303748,
      "grad_norm": 0.4394676983356476,
      "learning_rate": 0.00017804010984104394,
      "loss": 0.4517,
      "step": 1380
    },
    {
      "epoch": 0.2171867649364905,
      "grad_norm": 0.3945474922657013,
      "learning_rate": 0.00017788561047304917,
      "loss": 0.4176,
      "step": 1385
    },
    {
      "epoch": 0.21797083267994355,
      "grad_norm": 0.46311917901039124,
      "learning_rate": 0.00017773063705187641,
      "loss": 0.4431,
      "step": 1390
    },
    {
      "epoch": 0.21875490042339657,
      "grad_norm": 0.3924734890460968,
      "learning_rate": 0.00017757519052077626,
      "loss": 0.4276,
      "step": 1395
    },
    {
      "epoch": 0.21953896816684962,
      "grad_norm": 0.5059096813201904,
      "learning_rate": 0.0001774192718258788,
      "loss": 0.4676,
      "step": 1400
    },
    {
      "epoch": 0.22032303591030264,
      "grad_norm": 0.610349178314209,
      "learning_rate": 0.000177262881916188,
      "loss": 0.4747,
      "step": 1405
    },
    {
      "epoch": 0.2211071036537557,
      "grad_norm": 0.3809637427330017,
      "learning_rate": 0.00017710602174357586,
      "loss": 0.426,
      "step": 1410
    },
    {
      "epoch": 0.2218911713972087,
      "grad_norm": 0.4001152515411377,
      "learning_rate": 0.00017694869226277674,
      "loss": 0.4201,
      "step": 1415
    },
    {
      "epoch": 0.22267523914066176,
      "grad_norm": 0.4575730562210083,
      "learning_rate": 0.00017679089443138127,
      "loss": 0.4359,
      "step": 1420
    },
    {
      "epoch": 0.22345930688411478,
      "grad_norm": 0.6169618368148804,
      "learning_rate": 0.000176632629209831,
      "loss": 0.4804,
      "step": 1425
    },
    {
      "epoch": 0.22424337462756783,
      "grad_norm": 0.46373748779296875,
      "learning_rate": 0.00017647389756141193,
      "loss": 0.4204,
      "step": 1430
    },
    {
      "epoch": 0.22502744237102085,
      "grad_norm": 0.4640442728996277,
      "learning_rate": 0.00017631470045224927,
      "loss": 0.474,
      "step": 1435
    },
    {
      "epoch": 0.2258115101144739,
      "grad_norm": 0.4236632287502289,
      "learning_rate": 0.00017615503885130108,
      "loss": 0.4444,
      "step": 1440
    },
    {
      "epoch": 0.22659557785792692,
      "grad_norm": 0.4239494800567627,
      "learning_rate": 0.00017599491373035267,
      "loss": 0.4417,
      "step": 1445
    },
    {
      "epoch": 0.22737964560137996,
      "grad_norm": 0.4937686622142792,
      "learning_rate": 0.00017583432606401055,
      "loss": 0.4697,
      "step": 1450
    },
    {
      "epoch": 0.22816371334483299,
      "grad_norm": 0.48164480924606323,
      "learning_rate": 0.0001756732768296965,
      "loss": 0.401,
      "step": 1455
    },
    {
      "epoch": 0.22894778108828603,
      "grad_norm": 0.4751984477043152,
      "learning_rate": 0.00017551176700764167,
      "loss": 0.4458,
      "step": 1460
    },
    {
      "epoch": 0.22973184883173906,
      "grad_norm": 0.41115036606788635,
      "learning_rate": 0.00017534979758088056,
      "loss": 0.4129,
      "step": 1465
    },
    {
      "epoch": 0.2305159165751921,
      "grad_norm": 0.4154259264469147,
      "learning_rate": 0.00017518736953524517,
      "loss": 0.4351,
      "step": 1470
    },
    {
      "epoch": 0.23129998431864512,
      "grad_norm": 0.5034992694854736,
      "learning_rate": 0.0001750244838593588,
      "loss": 0.4687,
      "step": 1475
    },
    {
      "epoch": 0.23208405206209817,
      "grad_norm": 0.5196486711502075,
      "learning_rate": 0.00017486114154463008,
      "loss": 0.416,
      "step": 1480
    },
    {
      "epoch": 0.2328681198055512,
      "grad_norm": 0.44935062527656555,
      "learning_rate": 0.00017469734358524717,
      "loss": 0.4137,
      "step": 1485
    },
    {
      "epoch": 0.23365218754900424,
      "grad_norm": 0.3732416331768036,
      "learning_rate": 0.00017453309097817133,
      "loss": 0.4591,
      "step": 1490
    },
    {
      "epoch": 0.23443625529245726,
      "grad_norm": 0.4771050214767456,
      "learning_rate": 0.0001743683847231312,
      "loss": 0.4756,
      "step": 1495
    },
    {
      "epoch": 0.2352203230359103,
      "grad_norm": 0.4467551112174988,
      "learning_rate": 0.00017420322582261646,
      "loss": 0.4387,
      "step": 1500
    },
    {
      "epoch": 0.23600439077936333,
      "grad_norm": 0.42907899618148804,
      "learning_rate": 0.0001740376152818719,
      "loss": 0.4597,
      "step": 1505
    },
    {
      "epoch": 0.23678845852281638,
      "grad_norm": 0.48936718702316284,
      "learning_rate": 0.00017387155410889115,
      "loss": 0.4411,
      "step": 1510
    },
    {
      "epoch": 0.2375725262662694,
      "grad_norm": 0.41470152139663696,
      "learning_rate": 0.00017370504331441072,
      "loss": 0.4246,
      "step": 1515
    },
    {
      "epoch": 0.23835659400972245,
      "grad_norm": 0.43099236488342285,
      "learning_rate": 0.0001735380839119037,
      "loss": 0.4389,
      "step": 1520
    },
    {
      "epoch": 0.23914066175317547,
      "grad_norm": 0.3904872238636017,
      "learning_rate": 0.00017337067691757367,
      "loss": 0.4317,
      "step": 1525
    },
    {
      "epoch": 0.23992472949662852,
      "grad_norm": 0.4525796175003052,
      "learning_rate": 0.00017320282335034847,
      "loss": 0.418,
      "step": 1530
    },
    {
      "epoch": 0.24070879724008154,
      "grad_norm": 0.43571439385414124,
      "learning_rate": 0.00017303452423187403,
      "loss": 0.4363,
      "step": 1535
    },
    {
      "epoch": 0.2414928649835346,
      "grad_norm": 0.4343824088573456,
      "learning_rate": 0.00017286578058650816,
      "loss": 0.4008,
      "step": 1540
    },
    {
      "epoch": 0.2422769327269876,
      "grad_norm": 0.46734753251075745,
      "learning_rate": 0.00017269659344131422,
      "loss": 0.4246,
      "step": 1545
    },
    {
      "epoch": 0.24306100047044066,
      "grad_norm": 0.3773515224456787,
      "learning_rate": 0.00017252696382605507,
      "loss": 0.424,
      "step": 1550
    },
    {
      "epoch": 0.24384506821389368,
      "grad_norm": 0.4286627173423767,
      "learning_rate": 0.0001723568927731866,
      "loss": 0.4399,
      "step": 1555
    },
    {
      "epoch": 0.24462913595734673,
      "grad_norm": 0.495226114988327,
      "learning_rate": 0.0001721863813178515,
      "loss": 0.4427,
      "step": 1560
    },
    {
      "epoch": 0.24541320370079975,
      "grad_norm": 0.4652136564254761,
      "learning_rate": 0.000172015430497873,
      "loss": 0.4846,
      "step": 1565
    },
    {
      "epoch": 0.2461972714442528,
      "grad_norm": 0.38543200492858887,
      "learning_rate": 0.00017184404135374862,
      "loss": 0.3981,
      "step": 1570
    },
    {
      "epoch": 0.24698133918770582,
      "grad_norm": 0.43989744782447815,
      "learning_rate": 0.00017167221492864372,
      "loss": 0.4758,
      "step": 1575
    },
    {
      "epoch": 0.24776540693115887,
      "grad_norm": 0.42274442315101624,
      "learning_rate": 0.00017149995226838506,
      "loss": 0.433,
      "step": 1580
    },
    {
      "epoch": 0.2485494746746119,
      "grad_norm": 0.46914514899253845,
      "learning_rate": 0.00017132725442145467,
      "loss": 0.3984,
      "step": 1585
    },
    {
      "epoch": 0.2493335424180649,
      "grad_norm": 0.43068253993988037,
      "learning_rate": 0.0001711541224389834,
      "loss": 0.4321,
      "step": 1590
    },
    {
      "epoch": 0.25011761016151796,
      "grad_norm": 0.49446821212768555,
      "learning_rate": 0.00017098055737474433,
      "loss": 0.4478,
      "step": 1595
    },
    {
      "epoch": 0.250901677904971,
      "grad_norm": 0.4090688228607178,
      "learning_rate": 0.00017080656028514666,
      "loss": 0.4332,
      "step": 1600
    },
    {
      "epoch": 0.251685745648424,
      "grad_norm": 0.4859888255596161,
      "learning_rate": 0.00017063213222922902,
      "loss": 0.4597,
      "step": 1605
    },
    {
      "epoch": 0.2524698133918771,
      "grad_norm": 0.4639436602592468,
      "learning_rate": 0.0001704572742686532,
      "loss": 0.4011,
      "step": 1610
    },
    {
      "epoch": 0.2532538811353301,
      "grad_norm": 0.4445328712463379,
      "learning_rate": 0.00017028198746769755,
      "loss": 0.4368,
      "step": 1615
    },
    {
      "epoch": 0.2540379488787831,
      "grad_norm": 0.37212374806404114,
      "learning_rate": 0.00017010627289325067,
      "loss": 0.4363,
      "step": 1620
    },
    {
      "epoch": 0.25482201662223614,
      "grad_norm": 0.3969605267047882,
      "learning_rate": 0.00016993013161480472,
      "loss": 0.4481,
      "step": 1625
    },
    {
      "epoch": 0.2556060843656892,
      "grad_norm": 0.46792757511138916,
      "learning_rate": 0.000169753564704449,
      "loss": 0.4396,
      "step": 1630
    },
    {
      "epoch": 0.25639015210914223,
      "grad_norm": 0.49791190028190613,
      "learning_rate": 0.00016957657323686353,
      "loss": 0.4569,
      "step": 1635
    },
    {
      "epoch": 0.25717421985259525,
      "grad_norm": 0.4189523458480835,
      "learning_rate": 0.00016939915828931233,
      "loss": 0.3937,
      "step": 1640
    },
    {
      "epoch": 0.2579582875960483,
      "grad_norm": 0.49665117263793945,
      "learning_rate": 0.00016922132094163698,
      "loss": 0.4865,
      "step": 1645
    },
    {
      "epoch": 0.25874235533950135,
      "grad_norm": 0.48225051164627075,
      "learning_rate": 0.00016904306227624998,
      "loss": 0.4156,
      "step": 1650
    },
    {
      "epoch": 0.25952642308295437,
      "grad_norm": 0.36697643995285034,
      "learning_rate": 0.00016886438337812822,
      "loss": 0.4117,
      "step": 1655
    },
    {
      "epoch": 0.2603104908264074,
      "grad_norm": 0.5010778903961182,
      "learning_rate": 0.00016868528533480634,
      "loss": 0.4088,
      "step": 1660
    },
    {
      "epoch": 0.2610945585698604,
      "grad_norm": 0.4937361180782318,
      "learning_rate": 0.00016850576923637016,
      "loss": 0.3993,
      "step": 1665
    },
    {
      "epoch": 0.2618786263133135,
      "grad_norm": 0.45135554671287537,
      "learning_rate": 0.0001683258361754499,
      "loss": 0.4217,
      "step": 1670
    },
    {
      "epoch": 0.2626626940567665,
      "grad_norm": 0.4187750518321991,
      "learning_rate": 0.00016814548724721374,
      "loss": 0.4187,
      "step": 1675
    },
    {
      "epoch": 0.26344676180021953,
      "grad_norm": 0.34371066093444824,
      "learning_rate": 0.00016796472354936102,
      "loss": 0.4031,
      "step": 1680
    },
    {
      "epoch": 0.26423082954367255,
      "grad_norm": 0.4156200885772705,
      "learning_rate": 0.00016778354618211556,
      "loss": 0.4209,
      "step": 1685
    },
    {
      "epoch": 0.26501489728712563,
      "grad_norm": 0.5756020545959473,
      "learning_rate": 0.000167601956248219,
      "loss": 0.4732,
      "step": 1690
    },
    {
      "epoch": 0.26579896503057865,
      "grad_norm": 0.4265245497226715,
      "learning_rate": 0.0001674199548529241,
      "loss": 0.407,
      "step": 1695
    },
    {
      "epoch": 0.26658303277403167,
      "grad_norm": 0.4594450294971466,
      "learning_rate": 0.00016723754310398804,
      "loss": 0.4414,
      "step": 1700
    },
    {
      "epoch": 0.2673671005174847,
      "grad_norm": 0.4984017312526703,
      "learning_rate": 0.00016705472211166552,
      "loss": 0.4312,
      "step": 1705
    },
    {
      "epoch": 0.26815116826093777,
      "grad_norm": 0.505361795425415,
      "learning_rate": 0.00016687149298870218,
      "loss": 0.4474,
      "step": 1710
    },
    {
      "epoch": 0.2689352360043908,
      "grad_norm": 0.34495556354522705,
      "learning_rate": 0.0001666878568503277,
      "loss": 0.3524,
      "step": 1715
    },
    {
      "epoch": 0.2697193037478438,
      "grad_norm": 0.4599874019622803,
      "learning_rate": 0.0001665038148142492,
      "loss": 0.4069,
      "step": 1720
    },
    {
      "epoch": 0.27050337149129683,
      "grad_norm": 0.5083295106887817,
      "learning_rate": 0.00016631936800064415,
      "loss": 0.4552,
      "step": 1725
    },
    {
      "epoch": 0.2712874392347499,
      "grad_norm": 10.928390502929688,
      "learning_rate": 0.00016613451753215384,
      "loss": 0.4311,
      "step": 1730
    },
    {
      "epoch": 0.2720715069782029,
      "grad_norm": 0.46821439266204834,
      "learning_rate": 0.00016594926453387636,
      "loss": 0.4465,
      "step": 1735
    },
    {
      "epoch": 0.27285557472165595,
      "grad_norm": 0.5331433415412903,
      "learning_rate": 0.0001657636101333598,
      "loss": 0.4151,
      "step": 1740
    },
    {
      "epoch": 0.27363964246510897,
      "grad_norm": 0.37438875436782837,
      "learning_rate": 0.00016557755546059538,
      "loss": 0.4026,
      "step": 1745
    },
    {
      "epoch": 0.27442371020856204,
      "grad_norm": 0.4326014518737793,
      "learning_rate": 0.0001653911016480107,
      "loss": 0.4395,
      "step": 1750
    },
    {
      "epoch": 0.27520777795201506,
      "grad_norm": 0.5174519419670105,
      "learning_rate": 0.0001652042498304626,
      "loss": 0.4665,
      "step": 1755
    },
    {
      "epoch": 0.2759918456954681,
      "grad_norm": 0.5045416355133057,
      "learning_rate": 0.00016501700114523047,
      "loss": 0.4132,
      "step": 1760
    },
    {
      "epoch": 0.2767759134389211,
      "grad_norm": 0.4313085377216339,
      "learning_rate": 0.00016482935673200924,
      "loss": 0.4167,
      "step": 1765
    },
    {
      "epoch": 0.2775599811823742,
      "grad_norm": 0.4285504221916199,
      "learning_rate": 0.0001646413177329024,
      "loss": 0.3849,
      "step": 1770
    },
    {
      "epoch": 0.2783440489258272,
      "grad_norm": 0.43986624479293823,
      "learning_rate": 0.00016445288529241516,
      "loss": 0.408,
      "step": 1775
    },
    {
      "epoch": 0.2791281166692802,
      "grad_norm": 0.4468976557254791,
      "learning_rate": 0.00016426406055744741,
      "loss": 0.3825,
      "step": 1780
    },
    {
      "epoch": 0.27991218441273324,
      "grad_norm": 0.4483013451099396,
      "learning_rate": 0.00016407484467728672,
      "loss": 0.4193,
      "step": 1785
    },
    {
      "epoch": 0.2806962521561863,
      "grad_norm": 0.4787501394748688,
      "learning_rate": 0.00016388523880360139,
      "loss": 0.466,
      "step": 1790
    },
    {
      "epoch": 0.28148031989963934,
      "grad_norm": 0.4895800054073334,
      "learning_rate": 0.0001636952440904335,
      "loss": 0.4217,
      "step": 1795
    },
    {
      "epoch": 0.28226438764309236,
      "grad_norm": 0.4485025107860565,
      "learning_rate": 0.00016350486169419164,
      "loss": 0.4608,
      "step": 1800
    },
    {
      "epoch": 0.2830484553865454,
      "grad_norm": 0.5011392831802368,
      "learning_rate": 0.00016331409277364425,
      "loss": 0.4448,
      "step": 1805
    },
    {
      "epoch": 0.2838325231299984,
      "grad_norm": 0.4324447810649872,
      "learning_rate": 0.00016312293848991222,
      "loss": 0.4033,
      "step": 1810
    },
    {
      "epoch": 0.2846165908734515,
      "grad_norm": 0.3968081474304199,
      "learning_rate": 0.00016293140000646207,
      "loss": 0.4406,
      "step": 1815
    },
    {
      "epoch": 0.2854006586169045,
      "grad_norm": 0.4604114294052124,
      "learning_rate": 0.00016273947848909862,
      "loss": 0.4333,
      "step": 1820
    },
    {
      "epoch": 0.2861847263603575,
      "grad_norm": 0.41123008728027344,
      "learning_rate": 0.0001625471751059582,
      "loss": 0.4166,
      "step": 1825
    },
    {
      "epoch": 0.28696879410381054,
      "grad_norm": 0.4176836609840393,
      "learning_rate": 0.0001623544910275012,
      "loss": 0.4381,
      "step": 1830
    },
    {
      "epoch": 0.2877528618472636,
      "grad_norm": 0.4229382574558258,
      "learning_rate": 0.0001621614274265054,
      "loss": 0.4107,
      "step": 1835
    },
    {
      "epoch": 0.28853692959071664,
      "grad_norm": 0.5111108422279358,
      "learning_rate": 0.0001619679854780582,
      "loss": 0.443,
      "step": 1840
    },
    {
      "epoch": 0.28932099733416966,
      "grad_norm": 0.47138911485671997,
      "learning_rate": 0.0001617741663595501,
      "loss": 0.4441,
      "step": 1845
    },
    {
      "epoch": 0.2901050650776227,
      "grad_norm": 0.4464900493621826,
      "learning_rate": 0.00016157997125066715,
      "loss": 0.4601,
      "step": 1850
    },
    {
      "epoch": 0.29088913282107576,
      "grad_norm": 0.4196482300758362,
      "learning_rate": 0.00016138540133338387,
      "loss": 0.4357,
      "step": 1855
    },
    {
      "epoch": 0.2916732005645288,
      "grad_norm": 0.47973117232322693,
      "learning_rate": 0.00016119045779195608,
      "loss": 0.4113,
      "step": 1860
    },
    {
      "epoch": 0.2924572683079818,
      "grad_norm": 0.4811786413192749,
      "learning_rate": 0.00016099514181291372,
      "loss": 0.3919,
      "step": 1865
    },
    {
      "epoch": 0.2932413360514348,
      "grad_norm": 0.48956894874572754,
      "learning_rate": 0.00016079945458505348,
      "loss": 0.4459,
      "step": 1870
    },
    {
      "epoch": 0.2940254037948879,
      "grad_norm": 0.40033867955207825,
      "learning_rate": 0.00016060339729943176,
      "loss": 0.4162,
      "step": 1875
    },
    {
      "epoch": 0.2948094715383409,
      "grad_norm": 0.5231456160545349,
      "learning_rate": 0.0001604069711493573,
      "loss": 0.4648,
      "step": 1880
    },
    {
      "epoch": 0.29559353928179394,
      "grad_norm": 0.5167508721351624,
      "learning_rate": 0.00016021017733038392,
      "loss": 0.422,
      "step": 1885
    },
    {
      "epoch": 0.29637760702524696,
      "grad_norm": 0.478291392326355,
      "learning_rate": 0.0001600130170403033,
      "loss": 0.4249,
      "step": 1890
    },
    {
      "epoch": 0.29716167476870003,
      "grad_norm": 0.4436359703540802,
      "learning_rate": 0.00015981549147913763,
      "loss": 0.4513,
      "step": 1895
    },
    {
      "epoch": 0.29794574251215306,
      "grad_norm": 0.5071510672569275,
      "learning_rate": 0.00015961760184913238,
      "loss": 0.4404,
      "step": 1900
    },
    {
      "epoch": 0.2987298102556061,
      "grad_norm": 0.38673800230026245,
      "learning_rate": 0.0001594193493547489,
      "loss": 0.4124,
      "step": 1905
    },
    {
      "epoch": 0.2995138779990591,
      "grad_norm": 0.4785563051700592,
      "learning_rate": 0.00015922073520265707,
      "loss": 0.4475,
      "step": 1910
    },
    {
      "epoch": 0.3002979457425122,
      "grad_norm": 0.43544065952301025,
      "learning_rate": 0.0001590217606017282,
      "loss": 0.4266,
      "step": 1915
    },
    {
      "epoch": 0.3010820134859652,
      "grad_norm": 0.5749733448028564,
      "learning_rate": 0.0001588224267630272,
      "loss": 0.468,
      "step": 1920
    },
    {
      "epoch": 0.3018660812294182,
      "grad_norm": 0.386924684047699,
      "learning_rate": 0.00015862273489980578,
      "loss": 0.4609,
      "step": 1925
    },
    {
      "epoch": 0.30265014897287124,
      "grad_norm": 0.5395941138267517,
      "learning_rate": 0.00015842268622749456,
      "loss": 0.4834,
      "step": 1930
    },
    {
      "epoch": 0.3034342167163243,
      "grad_norm": 0.3925447463989258,
      "learning_rate": 0.00015822228196369598,
      "loss": 0.4082,
      "step": 1935
    },
    {
      "epoch": 0.30421828445977733,
      "grad_norm": 0.5568708777427673,
      "learning_rate": 0.00015802152332817682,
      "loss": 0.4702,
      "step": 1940
    },
    {
      "epoch": 0.30500235220323035,
      "grad_norm": 0.5141738057136536,
      "learning_rate": 0.00015782041154286075,
      "loss": 0.4155,
      "step": 1945
    },
    {
      "epoch": 0.3057864199466834,
      "grad_norm": 0.3900509178638458,
      "learning_rate": 0.0001576189478318208,
      "loss": 0.4056,
      "step": 1950
    },
    {
      "epoch": 0.30657048769013645,
      "grad_norm": 0.42583462595939636,
      "learning_rate": 0.0001574171334212722,
      "loss": 0.3888,
      "step": 1955
    },
    {
      "epoch": 0.30735455543358947,
      "grad_norm": 0.430428683757782,
      "learning_rate": 0.00015721496953956444,
      "loss": 0.4556,
      "step": 1960
    },
    {
      "epoch": 0.3081386231770425,
      "grad_norm": 0.5113678574562073,
      "learning_rate": 0.0001570124574171744,
      "loss": 0.3729,
      "step": 1965
    },
    {
      "epoch": 0.3089226909204955,
      "grad_norm": 0.44289112091064453,
      "learning_rate": 0.00015680959828669826,
      "loss": 0.4762,
      "step": 1970
    },
    {
      "epoch": 0.3097067586639486,
      "grad_norm": 0.4770936667919159,
      "learning_rate": 0.00015660639338284447,
      "loss": 0.4469,
      "step": 1975
    },
    {
      "epoch": 0.3104908264074016,
      "grad_norm": 0.5946089625358582,
      "learning_rate": 0.00015640284394242594,
      "loss": 0.4751,
      "step": 1980
    },
    {
      "epoch": 0.31127489415085463,
      "grad_norm": 0.4328524172306061,
      "learning_rate": 0.00015619895120435259,
      "loss": 0.4257,
      "step": 1985
    },
    {
      "epoch": 0.31205896189430765,
      "grad_norm": 0.5715842247009277,
      "learning_rate": 0.00015599471640962394,
      "loss": 0.4368,
      "step": 1990
    },
    {
      "epoch": 0.31284302963776073,
      "grad_norm": 0.42558008432388306,
      "learning_rate": 0.00015579014080132136,
      "loss": 0.3973,
      "step": 1995
    },
    {
      "epoch": 0.31362709738121375,
      "grad_norm": 0.4622448682785034,
      "learning_rate": 0.00015558522562460065,
      "loss": 0.4421,
      "step": 2000
    },
    {
      "epoch": 0.31441116512466677,
      "grad_norm": 0.5790919661521912,
      "learning_rate": 0.00015537997212668433,
      "loss": 0.4513,
      "step": 2005
    },
    {
      "epoch": 0.3151952328681198,
      "grad_norm": 0.4854815900325775,
      "learning_rate": 0.00015517438155685423,
      "loss": 0.4368,
      "step": 2010
    },
    {
      "epoch": 0.31597930061157287,
      "grad_norm": 0.4583725333213806,
      "learning_rate": 0.00015496845516644364,
      "loss": 0.4032,
      "step": 2015
    },
    {
      "epoch": 0.3167633683550259,
      "grad_norm": 0.42419835925102234,
      "learning_rate": 0.00015476219420882995,
      "loss": 0.4677,
      "step": 2020
    },
    {
      "epoch": 0.3175474360984789,
      "grad_norm": 0.4686051607131958,
      "learning_rate": 0.00015455559993942685,
      "loss": 0.448,
      "step": 2025
    },
    {
      "epoch": 0.31833150384193193,
      "grad_norm": 0.492891401052475,
      "learning_rate": 0.00015434867361567673,
      "loss": 0.4583,
      "step": 2030
    },
    {
      "epoch": 0.319115571585385,
      "grad_norm": 0.41828399896621704,
      "learning_rate": 0.00015414141649704313,
      "loss": 0.4705,
      "step": 2035
    },
    {
      "epoch": 0.319899639328838,
      "grad_norm": 0.39593884348869324,
      "learning_rate": 0.00015393382984500283,
      "loss": 0.4121,
      "step": 2040
    },
    {
      "epoch": 0.32068370707229105,
      "grad_norm": 0.47555962204933167,
      "learning_rate": 0.00015372591492303844,
      "loss": 0.3943,
      "step": 2045
    },
    {
      "epoch": 0.32146777481574407,
      "grad_norm": 0.5041057467460632,
      "learning_rate": 0.0001535176729966306,
      "loss": 0.4288,
      "step": 2050
    },
    {
      "epoch": 0.3222518425591971,
      "grad_norm": 0.4198837876319885,
      "learning_rate": 0.00015330910533325022,
      "loss": 0.429,
      "step": 2055
    },
    {
      "epoch": 0.32303591030265016,
      "grad_norm": 0.4546828269958496,
      "learning_rate": 0.0001531002132023508,
      "loss": 0.4346,
      "step": 2060
    },
    {
      "epoch": 0.3238199780461032,
      "grad_norm": 0.36287450790405273,
      "learning_rate": 0.0001528909978753608,
      "loss": 0.4346,
      "step": 2065
    },
    {
      "epoch": 0.3246040457895562,
      "grad_norm": 0.3780982196331024,
      "learning_rate": 0.00015268146062567576,
      "loss": 0.4396,
      "step": 2070
    },
    {
      "epoch": 0.3253881135330092,
      "grad_norm": 0.43950513005256653,
      "learning_rate": 0.00015247160272865065,
      "loss": 0.4122,
      "step": 2075
    },
    {
      "epoch": 0.3261721812764623,
      "grad_norm": 0.6217437386512756,
      "learning_rate": 0.00015226142546159206,
      "loss": 0.4524,
      "step": 2080
    },
    {
      "epoch": 0.3269562490199153,
      "grad_norm": 0.5032625198364258,
      "learning_rate": 0.0001520509301037504,
      "loss": 0.4443,
      "step": 2085
    },
    {
      "epoch": 0.32774031676336834,
      "grad_norm": 0.5078793168067932,
      "learning_rate": 0.00015184011793631217,
      "loss": 0.4377,
      "step": 2090
    },
    {
      "epoch": 0.32852438450682137,
      "grad_norm": 0.5667276978492737,
      "learning_rate": 0.0001516289902423922,
      "loss": 0.4499,
      "step": 2095
    },
    {
      "epoch": 0.32930845225027444,
      "grad_norm": 0.41754236817359924,
      "learning_rate": 0.0001514175483070257,
      "loss": 0.3901,
      "step": 2100
    },
    {
      "epoch": 0.33009251999372746,
      "grad_norm": 0.46254104375839233,
      "learning_rate": 0.00015120579341716054,
      "loss": 0.439,
      "step": 2105
    },
    {
      "epoch": 0.3308765877371805,
      "grad_norm": 0.48077476024627686,
      "learning_rate": 0.00015099372686164944,
      "loss": 0.4027,
      "step": 2110
    },
    {
      "epoch": 0.3316606554806335,
      "grad_norm": 0.4670598804950714,
      "learning_rate": 0.00015078134993124206,
      "loss": 0.4272,
      "step": 2115
    },
    {
      "epoch": 0.3324447232240866,
      "grad_norm": 0.38048309087753296,
      "learning_rate": 0.00015056866391857708,
      "loss": 0.3871,
      "step": 2120
    },
    {
      "epoch": 0.3332287909675396,
      "grad_norm": 0.4457653760910034,
      "learning_rate": 0.00015035567011817457,
      "loss": 0.3981,
      "step": 2125
    },
    {
      "epoch": 0.3340128587109926,
      "grad_norm": 0.396278977394104,
      "learning_rate": 0.0001501423698264279,
      "loss": 0.3727,
      "step": 2130
    },
    {
      "epoch": 0.33479692645444564,
      "grad_norm": 0.44341397285461426,
      "learning_rate": 0.00014992876434159579,
      "loss": 0.4057,
      "step": 2135
    },
    {
      "epoch": 0.3355809941978987,
      "grad_norm": 0.483191579580307,
      "learning_rate": 0.00014971485496379473,
      "loss": 0.4309,
      "step": 2140
    },
    {
      "epoch": 0.33636506194135174,
      "grad_norm": 0.469944030046463,
      "learning_rate": 0.00014950064299499078,
      "loss": 0.3863,
      "step": 2145
    },
    {
      "epoch": 0.33714912968480476,
      "grad_norm": 0.44500336050987244,
      "learning_rate": 0.00014928612973899162,
      "loss": 0.4014,
      "step": 2150
    },
    {
      "epoch": 0.3379331974282578,
      "grad_norm": 0.5041776299476624,
      "learning_rate": 0.00014907131650143893,
      "loss": 0.4381,
      "step": 2155
    },
    {
      "epoch": 0.33871726517171086,
      "grad_norm": 0.40571290254592896,
      "learning_rate": 0.00014885620458980004,
      "loss": 0.4065,
      "step": 2160
    },
    {
      "epoch": 0.3395013329151639,
      "grad_norm": 0.3567410409450531,
      "learning_rate": 0.0001486407953133603,
      "loss": 0.4285,
      "step": 2165
    },
    {
      "epoch": 0.3402854006586169,
      "grad_norm": 0.5367431044578552,
      "learning_rate": 0.0001484250899832149,
      "loss": 0.4441,
      "step": 2170
    },
    {
      "epoch": 0.3410694684020699,
      "grad_norm": 0.4909151792526245,
      "learning_rate": 0.00014820908991226104,
      "loss": 0.4173,
      "step": 2175
    },
    {
      "epoch": 0.341853536145523,
      "grad_norm": 0.49308380484580994,
      "learning_rate": 0.00014799279641518985,
      "loss": 0.4534,
      "step": 2180
    },
    {
      "epoch": 0.342637603888976,
      "grad_norm": 0.43491554260253906,
      "learning_rate": 0.00014777621080847836,
      "loss": 0.4216,
      "step": 2185
    },
    {
      "epoch": 0.34342167163242904,
      "grad_norm": 0.4509499967098236,
      "learning_rate": 0.00014755933441038156,
      "loss": 0.3925,
      "step": 2190
    },
    {
      "epoch": 0.34420573937588206,
      "grad_norm": 0.45227381587028503,
      "learning_rate": 0.0001473421685409244,
      "loss": 0.4182,
      "step": 2195
    },
    {
      "epoch": 0.34498980711933513,
      "grad_norm": 0.44948822259902954,
      "learning_rate": 0.0001471247145218936,
      "loss": 0.3657,
      "step": 2200
    },
    {
      "epoch": 0.34577387486278816,
      "grad_norm": 0.5384871363639832,
      "learning_rate": 0.00014690697367682982,
      "loss": 0.4571,
      "step": 2205
    },
    {
      "epoch": 0.3465579426062412,
      "grad_norm": 0.6136193871498108,
      "learning_rate": 0.00014668894733101945,
      "loss": 0.4458,
      "step": 2210
    },
    {
      "epoch": 0.3473420103496942,
      "grad_norm": 0.5780574083328247,
      "learning_rate": 0.00014647063681148657,
      "loss": 0.4123,
      "step": 2215
    },
    {
      "epoch": 0.3481260780931473,
      "grad_norm": 0.4093414843082428,
      "learning_rate": 0.00014625204344698492,
      "loss": 0.3836,
      "step": 2220
    },
    {
      "epoch": 0.3489101458366003,
      "grad_norm": 0.38658496737480164,
      "learning_rate": 0.00014603316856798973,
      "loss": 0.4105,
      "step": 2225
    },
    {
      "epoch": 0.3496942135800533,
      "grad_norm": 0.5491430163383484,
      "learning_rate": 0.00014581401350668985,
      "loss": 0.3946,
      "step": 2230
    },
    {
      "epoch": 0.35047828132350634,
      "grad_norm": 0.46433699131011963,
      "learning_rate": 0.00014559457959697922,
      "loss": 0.4428,
      "step": 2235
    },
    {
      "epoch": 0.3512623490669594,
      "grad_norm": 0.4171154797077179,
      "learning_rate": 0.00014537486817444917,
      "loss": 0.3871,
      "step": 2240
    },
    {
      "epoch": 0.35204641681041243,
      "grad_norm": 0.5451834201812744,
      "learning_rate": 0.00014515488057638008,
      "loss": 0.4422,
      "step": 2245
    },
    {
      "epoch": 0.35283048455386545,
      "grad_norm": 0.5367875099182129,
      "learning_rate": 0.00014493461814173324,
      "loss": 0.4501,
      "step": 2250
    },
    {
      "epoch": 0.3536145522973185,
      "grad_norm": 0.4389900863170624,
      "learning_rate": 0.0001447140822111428,
      "loss": 0.4591,
      "step": 2255
    },
    {
      "epoch": 0.35439862004077155,
      "grad_norm": 0.5384318232536316,
      "learning_rate": 0.00014449327412690747,
      "loss": 0.444,
      "step": 2260
    },
    {
      "epoch": 0.35518268778422457,
      "grad_norm": 0.4189915359020233,
      "learning_rate": 0.0001442721952329826,
      "loss": 0.4124,
      "step": 2265
    },
    {
      "epoch": 0.3559667555276776,
      "grad_norm": 0.558387815952301,
      "learning_rate": 0.0001440508468749716,
      "loss": 0.457,
      "step": 2270
    },
    {
      "epoch": 0.3567508232711306,
      "grad_norm": 0.44807079434394836,
      "learning_rate": 0.00014382923040011817,
      "loss": 0.4017,
      "step": 2275
    },
    {
      "epoch": 0.35753489101458363,
      "grad_norm": 0.4253521263599396,
      "learning_rate": 0.00014360734715729783,
      "loss": 0.3642,
      "step": 2280
    },
    {
      "epoch": 0.3583189587580367,
      "grad_norm": 0.5197420120239258,
      "learning_rate": 0.0001433851984970098,
      "loss": 0.4243,
      "step": 2285
    },
    {
      "epoch": 0.35910302650148973,
      "grad_norm": 0.433472216129303,
      "learning_rate": 0.00014316278577136872,
      "loss": 0.4356,
      "step": 2290
    },
    {
      "epoch": 0.35988709424494275,
      "grad_norm": 0.44726771116256714,
      "learning_rate": 0.0001429401103340966,
      "loss": 0.4524,
      "step": 2295
    },
    {
      "epoch": 0.36067116198839577,
      "grad_norm": 0.4655953645706177,
      "learning_rate": 0.00014271717354051434,
      "loss": 0.4514,
      "step": 2300
    },
    {
      "epoch": 0.36145522973184885,
      "grad_norm": 0.43793565034866333,
      "learning_rate": 0.00014249397674753368,
      "loss": 0.3834,
      "step": 2305
    },
    {
      "epoch": 0.36223929747530187,
      "grad_norm": 0.41612789034843445,
      "learning_rate": 0.00014227052131364872,
      "loss": 0.4026,
      "step": 2310
    },
    {
      "epoch": 0.3630233652187549,
      "grad_norm": 0.4508545696735382,
      "learning_rate": 0.00014204680859892797,
      "loss": 0.4236,
      "step": 2315
    },
    {
      "epoch": 0.3638074329622079,
      "grad_norm": 0.4789075255393982,
      "learning_rate": 0.0001418228399650057,
      "loss": 0.4021,
      "step": 2320
    },
    {
      "epoch": 0.364591500705661,
      "grad_norm": 0.46366360783576965,
      "learning_rate": 0.000141598616775074,
      "loss": 0.3803,
      "step": 2325
    },
    {
      "epoch": 0.365375568449114,
      "grad_norm": 0.5306534767150879,
      "learning_rate": 0.0001413741403938742,
      "loss": 0.3879,
      "step": 2330
    },
    {
      "epoch": 0.36615963619256703,
      "grad_norm": 0.43085891008377075,
      "learning_rate": 0.00014114941218768877,
      "loss": 0.409,
      "step": 2335
    },
    {
      "epoch": 0.36694370393602005,
      "grad_norm": 0.39164382219314575,
      "learning_rate": 0.0001409244335243329,
      "loss": 0.4391,
      "step": 2340
    },
    {
      "epoch": 0.3677277716794731,
      "grad_norm": 0.4415305554866791,
      "learning_rate": 0.00014069920577314618,
      "loss": 0.4254,
      "step": 2345
    },
    {
      "epoch": 0.36851183942292615,
      "grad_norm": 0.4281095564365387,
      "learning_rate": 0.00014047373030498422,
      "loss": 0.4351,
      "step": 2350
    },
    {
      "epoch": 0.36929590716637917,
      "grad_norm": 0.6551634669303894,
      "learning_rate": 0.00014024800849221056,
      "loss": 0.4341,
      "step": 2355
    },
    {
      "epoch": 0.3700799749098322,
      "grad_norm": 0.4449635148048401,
      "learning_rate": 0.00014002204170868786,
      "loss": 0.4159,
      "step": 2360
    },
    {
      "epoch": 0.37086404265328526,
      "grad_norm": 0.4486696422100067,
      "learning_rate": 0.00013979583132977004,
      "loss": 0.4359,
      "step": 2365
    },
    {
      "epoch": 0.3716481103967383,
      "grad_norm": 0.5079948902130127,
      "learning_rate": 0.00013956937873229352,
      "loss": 0.4145,
      "step": 2370
    },
    {
      "epoch": 0.3724321781401913,
      "grad_norm": 0.4542016386985779,
      "learning_rate": 0.0001393426852945691,
      "loss": 0.3996,
      "step": 2375
    },
    {
      "epoch": 0.3732162458836443,
      "grad_norm": 0.5310404896736145,
      "learning_rate": 0.0001391157523963733,
      "loss": 0.4214,
      "step": 2380
    },
    {
      "epoch": 0.3740003136270974,
      "grad_norm": 0.42791783809661865,
      "learning_rate": 0.00013888858141894033,
      "loss": 0.3821,
      "step": 2385
    },
    {
      "epoch": 0.3747843813705504,
      "grad_norm": 0.4884335398674011,
      "learning_rate": 0.0001386611737449533,
      "loss": 0.4047,
      "step": 2390
    },
    {
      "epoch": 0.37556844911400344,
      "grad_norm": 0.4628289043903351,
      "learning_rate": 0.00013843353075853611,
      "loss": 0.4281,
      "step": 2395
    },
    {
      "epoch": 0.37635251685745646,
      "grad_norm": 0.45484963059425354,
      "learning_rate": 0.0001382056538452448,
      "loss": 0.4001,
      "step": 2400
    },
    {
      "epoch": 0.37713658460090954,
      "grad_norm": 0.386918842792511,
      "learning_rate": 0.00013797754439205925,
      "loss": 0.3937,
      "step": 2405
    },
    {
      "epoch": 0.37792065234436256,
      "grad_norm": 0.43956509232521057,
      "learning_rate": 0.00013774920378737478,
      "loss": 0.4092,
      "step": 2410
    },
    {
      "epoch": 0.3787047200878156,
      "grad_norm": 0.49167072772979736,
      "learning_rate": 0.00013752063342099342,
      "loss": 0.4426,
      "step": 2415
    },
    {
      "epoch": 0.3794887878312686,
      "grad_norm": 0.44001680612564087,
      "learning_rate": 0.00013729183468411585,
      "loss": 0.4165,
      "step": 2420
    },
    {
      "epoch": 0.3802728555747217,
      "grad_norm": 0.46296146512031555,
      "learning_rate": 0.0001370628089693327,
      "loss": 0.4334,
      "step": 2425
    },
    {
      "epoch": 0.3810569233181747,
      "grad_norm": 0.5237674117088318,
      "learning_rate": 0.00013683355767061598,
      "loss": 0.4838,
      "step": 2430
    },
    {
      "epoch": 0.3818409910616277,
      "grad_norm": 0.38093557953834534,
      "learning_rate": 0.00013660408218331088,
      "loss": 0.3834,
      "step": 2435
    },
    {
      "epoch": 0.38262505880508074,
      "grad_norm": 0.41690561175346375,
      "learning_rate": 0.00013637438390412703,
      "loss": 0.4047,
      "step": 2440
    },
    {
      "epoch": 0.3834091265485338,
      "grad_norm": 0.46514397859573364,
      "learning_rate": 0.0001361444642311301,
      "loss": 0.4207,
      "step": 2445
    },
    {
      "epoch": 0.38419319429198684,
      "grad_norm": 0.4534470736980438,
      "learning_rate": 0.0001359143245637333,
      "loss": 0.4383,
      "step": 2450
    },
    {
      "epoch": 0.38497726203543986,
      "grad_norm": 0.46847352385520935,
      "learning_rate": 0.00013568396630268887,
      "loss": 0.4022,
      "step": 2455
    },
    {
      "epoch": 0.3857613297788929,
      "grad_norm": 0.4562035799026489,
      "learning_rate": 0.00013545339085007944,
      "loss": 0.431,
      "step": 2460
    },
    {
      "epoch": 0.38654539752234596,
      "grad_norm": 0.5027127861976624,
      "learning_rate": 0.0001352225996093097,
      "loss": 0.4158,
      "step": 2465
    },
    {
      "epoch": 0.387329465265799,
      "grad_norm": 0.4291686415672302,
      "learning_rate": 0.0001349915939850976,
      "loss": 0.4335,
      "step": 2470
    },
    {
      "epoch": 0.388113533009252,
      "grad_norm": 0.4634084403514862,
      "learning_rate": 0.00013476037538346606,
      "loss": 0.3893,
      "step": 2475
    },
    {
      "epoch": 0.388897600752705,
      "grad_norm": 0.47961169481277466,
      "learning_rate": 0.00013452894521173426,
      "loss": 0.4317,
      "step": 2480
    },
    {
      "epoch": 0.3896816684961581,
      "grad_norm": 0.4352704882621765,
      "learning_rate": 0.00013429730487850912,
      "loss": 0.4363,
      "step": 2485
    },
    {
      "epoch": 0.3904657362396111,
      "grad_norm": 0.44600027799606323,
      "learning_rate": 0.00013406545579367664,
      "loss": 0.438,
      "step": 2490
    },
    {
      "epoch": 0.39124980398306414,
      "grad_norm": 0.47631749510765076,
      "learning_rate": 0.00013383339936839348,
      "loss": 0.4415,
      "step": 2495
    },
    {
      "epoch": 0.39203387172651716,
      "grad_norm": 0.412975937128067,
      "learning_rate": 0.00013360113701507825,
      "loss": 0.3989,
      "step": 2500
    },
    {
      "epoch": 0.3928179394699702,
      "grad_norm": 0.4891526699066162,
      "learning_rate": 0.00013336867014740296,
      "loss": 0.4093,
      "step": 2505
    },
    {
      "epoch": 0.39360200721342326,
      "grad_norm": 0.46200764179229736,
      "learning_rate": 0.00013313600018028434,
      "loss": 0.4456,
      "step": 2510
    },
    {
      "epoch": 0.3943860749568763,
      "grad_norm": 0.5124106407165527,
      "learning_rate": 0.0001329031285298754,
      "loss": 0.4047,
      "step": 2515
    },
    {
      "epoch": 0.3951701427003293,
      "grad_norm": 0.44421178102493286,
      "learning_rate": 0.00013267005661355666,
      "loss": 0.3869,
      "step": 2520
    },
    {
      "epoch": 0.3959542104437823,
      "grad_norm": 0.4882199764251709,
      "learning_rate": 0.0001324367858499275,
      "loss": 0.361,
      "step": 2525
    },
    {
      "epoch": 0.3967382781872354,
      "grad_norm": 0.4442441165447235,
      "learning_rate": 0.00013220331765879768,
      "loss": 0.4098,
      "step": 2530
    },
    {
      "epoch": 0.3975223459306884,
      "grad_norm": 0.35436832904815674,
      "learning_rate": 0.00013196965346117854,
      "loss": 0.3963,
      "step": 2535
    },
    {
      "epoch": 0.39830641367414144,
      "grad_norm": 0.5053900480270386,
      "learning_rate": 0.00013173579467927444,
      "loss": 0.4476,
      "step": 2540
    },
    {
      "epoch": 0.39909048141759446,
      "grad_norm": 0.4592802822589874,
      "learning_rate": 0.00013150174273647412,
      "loss": 0.4187,
      "step": 2545
    },
    {
      "epoch": 0.39987454916104753,
      "grad_norm": 0.4150494635105133,
      "learning_rate": 0.00013126749905734194,
      "loss": 0.414,
      "step": 2550
    },
    {
      "epoch": 0.40065861690450055,
      "grad_norm": 0.45942917466163635,
      "learning_rate": 0.00013103306506760936,
      "loss": 0.3735,
      "step": 2555
    },
    {
      "epoch": 0.4014426846479536,
      "grad_norm": 0.5727359056472778,
      "learning_rate": 0.00013079844219416603,
      "loss": 0.4223,
      "step": 2560
    },
    {
      "epoch": 0.4022267523914066,
      "grad_norm": 0.5259187817573547,
      "learning_rate": 0.0001305636318650514,
      "loss": 0.4567,
      "step": 2565
    },
    {
      "epoch": 0.40301082013485967,
      "grad_norm": 0.4618662893772125,
      "learning_rate": 0.0001303286355094458,
      "loss": 0.4072,
      "step": 2570
    },
    {
      "epoch": 0.4037948878783127,
      "grad_norm": 0.3796653747558594,
      "learning_rate": 0.00013009345455766176,
      "loss": 0.3798,
      "step": 2575
    },
    {
      "epoch": 0.4045789556217657,
      "grad_norm": 0.4078320562839508,
      "learning_rate": 0.00012985809044113543,
      "loss": 0.4153,
      "step": 2580
    },
    {
      "epoch": 0.40536302336521873,
      "grad_norm": 0.4147107005119324,
      "learning_rate": 0.00012962254459241787,
      "loss": 0.4344,
      "step": 2585
    },
    {
      "epoch": 0.4061470911086718,
      "grad_norm": 0.46461281180381775,
      "learning_rate": 0.0001293868184451661,
      "loss": 0.4257,
      "step": 2590
    },
    {
      "epoch": 0.40693115885212483,
      "grad_norm": 0.5057899951934814,
      "learning_rate": 0.0001291509134341346,
      "loss": 0.4212,
      "step": 2595
    },
    {
      "epoch": 0.40771522659557785,
      "grad_norm": 0.4374299943447113,
      "learning_rate": 0.00012891483099516658,
      "loss": 0.4112,
      "step": 2600
    },
    {
      "epoch": 0.40849929433903087,
      "grad_norm": 0.4741024374961853,
      "learning_rate": 0.00012867857256518503,
      "loss": 0.4186,
      "step": 2605
    },
    {
      "epoch": 0.40928336208248395,
      "grad_norm": 0.4222598373889923,
      "learning_rate": 0.00012844213958218426,
      "loss": 0.4051,
      "step": 2610
    },
    {
      "epoch": 0.41006742982593697,
      "grad_norm": 0.548535168170929,
      "learning_rate": 0.00012820553348522092,
      "loss": 0.4322,
      "step": 2615
    },
    {
      "epoch": 0.41085149756939,
      "grad_norm": 0.4396216571331024,
      "learning_rate": 0.00012796875571440534,
      "loss": 0.4129,
      "step": 2620
    },
    {
      "epoch": 0.411635565312843,
      "grad_norm": 0.4477290213108063,
      "learning_rate": 0.00012773180771089276,
      "loss": 0.4341,
      "step": 2625
    },
    {
      "epoch": 0.4124196330562961,
      "grad_norm": 0.5134490132331848,
      "learning_rate": 0.00012749469091687446,
      "loss": 0.382,
      "step": 2630
    },
    {
      "epoch": 0.4132037007997491,
      "grad_norm": 0.5104128122329712,
      "learning_rate": 0.00012725740677556923,
      "loss": 0.4617,
      "step": 2635
    },
    {
      "epoch": 0.41398776854320213,
      "grad_norm": 0.4579504430294037,
      "learning_rate": 0.00012701995673121435,
      "loss": 0.4463,
      "step": 2640
    },
    {
      "epoch": 0.41477183628665515,
      "grad_norm": 0.4899537265300751,
      "learning_rate": 0.00012678234222905676,
      "loss": 0.4169,
      "step": 2645
    },
    {
      "epoch": 0.4155559040301082,
      "grad_norm": 0.43586257100105286,
      "learning_rate": 0.00012654456471534455,
      "loss": 0.3844,
      "step": 2650
    },
    {
      "epoch": 0.41633997177356125,
      "grad_norm": 0.4656561613082886,
      "learning_rate": 0.00012630662563731793,
      "loss": 0.4044,
      "step": 2655
    },
    {
      "epoch": 0.41712403951701427,
      "grad_norm": 0.45274290442466736,
      "learning_rate": 0.0001260685264432004,
      "loss": 0.4358,
      "step": 2660
    },
    {
      "epoch": 0.4179081072604673,
      "grad_norm": 0.49432554841041565,
      "learning_rate": 0.00012583026858219013,
      "loss": 0.4068,
      "step": 2665
    },
    {
      "epoch": 0.41869217500392036,
      "grad_norm": 0.45811590552330017,
      "learning_rate": 0.00012559185350445094,
      "loss": 0.3598,
      "step": 2670
    },
    {
      "epoch": 0.4194762427473734,
      "grad_norm": 0.5007739663124084,
      "learning_rate": 0.00012535328266110357,
      "loss": 0.4992,
      "step": 2675
    },
    {
      "epoch": 0.4202603104908264,
      "grad_norm": 0.5585668683052063,
      "learning_rate": 0.00012511455750421685,
      "loss": 0.4021,
      "step": 2680
    },
    {
      "epoch": 0.4210443782342794,
      "grad_norm": 0.43685728311538696,
      "learning_rate": 0.0001248756794867988,
      "loss": 0.3853,
      "step": 2685
    },
    {
      "epoch": 0.4218284459777325,
      "grad_norm": 0.42427003383636475,
      "learning_rate": 0.0001246366500627879,
      "loss": 0.4614,
      "step": 2690
    },
    {
      "epoch": 0.4226125137211855,
      "grad_norm": 0.4138340651988983,
      "learning_rate": 0.00012439747068704416,
      "loss": 0.4139,
      "step": 2695
    },
    {
      "epoch": 0.42339658146463854,
      "grad_norm": 0.4996292293071747,
      "learning_rate": 0.00012415814281534017,
      "loss": 0.4206,
      "step": 2700
    },
    {
      "epoch": 0.42418064920809156,
      "grad_norm": 0.5437459349632263,
      "learning_rate": 0.00012391866790435253,
      "loss": 0.4317,
      "step": 2705
    },
    {
      "epoch": 0.42496471695154464,
      "grad_norm": 0.4692419171333313,
      "learning_rate": 0.00012367904741165259,
      "loss": 0.4275,
      "step": 2710
    },
    {
      "epoch": 0.42574878469499766,
      "grad_norm": 0.5160258412361145,
      "learning_rate": 0.00012343928279569797,
      "loss": 0.4121,
      "step": 2715
    },
    {
      "epoch": 0.4265328524384507,
      "grad_norm": 0.540435254573822,
      "learning_rate": 0.00012319937551582338,
      "loss": 0.433,
      "step": 2720
    },
    {
      "epoch": 0.4273169201819037,
      "grad_norm": 0.3812645673751831,
      "learning_rate": 0.00012295932703223193,
      "loss": 0.4117,
      "step": 2725
    },
    {
      "epoch": 0.4281009879253567,
      "grad_norm": 0.4936792552471161,
      "learning_rate": 0.0001227191388059861,
      "loss": 0.4079,
      "step": 2730
    },
    {
      "epoch": 0.4288850556688098,
      "grad_norm": 0.5081990361213684,
      "learning_rate": 0.00012247881229899904,
      "loss": 0.4372,
      "step": 2735
    },
    {
      "epoch": 0.4296691234122628,
      "grad_norm": 0.5293329358100891,
      "learning_rate": 0.00012223834897402537,
      "loss": 0.4536,
      "step": 2740
    },
    {
      "epoch": 0.43045319115571584,
      "grad_norm": 0.4881553649902344,
      "learning_rate": 0.00012199775029465265,
      "loss": 0.4269,
      "step": 2745
    },
    {
      "epoch": 0.43123725889916886,
      "grad_norm": 0.47365447878837585,
      "learning_rate": 0.0001217570177252921,
      "loss": 0.3877,
      "step": 2750
    },
    {
      "epoch": 0.43202132664262194,
      "grad_norm": 0.41903698444366455,
      "learning_rate": 0.00012151615273117003,
      "loss": 0.423,
      "step": 2755
    },
    {
      "epoch": 0.43280539438607496,
      "grad_norm": 0.4722353518009186,
      "learning_rate": 0.00012127515677831866,
      "loss": 0.4239,
      "step": 2760
    },
    {
      "epoch": 0.433589462129528,
      "grad_norm": 0.4680919349193573,
      "learning_rate": 0.00012103403133356727,
      "loss": 0.4757,
      "step": 2765
    },
    {
      "epoch": 0.434373529872981,
      "grad_norm": 0.6036903262138367,
      "learning_rate": 0.00012079277786453338,
      "loss": 0.4139,
      "step": 2770
    },
    {
      "epoch": 0.4351575976164341,
      "grad_norm": 0.5002890229225159,
      "learning_rate": 0.00012055139783961369,
      "loss": 0.417,
      "step": 2775
    },
    {
      "epoch": 0.4359416653598871,
      "grad_norm": 0.3953770697116852,
      "learning_rate": 0.00012030989272797516,
      "loss": 0.3816,
      "step": 2780
    },
    {
      "epoch": 0.4367257331033401,
      "grad_norm": 0.5487050414085388,
      "learning_rate": 0.00012006826399954619,
      "loss": 0.4597,
      "step": 2785
    },
    {
      "epoch": 0.43750980084679314,
      "grad_norm": 0.5056093335151672,
      "learning_rate": 0.00011982651312500741,
      "loss": 0.4412,
      "step": 2790
    },
    {
      "epoch": 0.4382938685902462,
      "grad_norm": 0.35286909341812134,
      "learning_rate": 0.00011958464157578309,
      "loss": 0.4177,
      "step": 2795
    },
    {
      "epoch": 0.43907793633369924,
      "grad_norm": 0.5357914566993713,
      "learning_rate": 0.00011934265082403187,
      "loss": 0.4099,
      "step": 2800
    },
    {
      "epoch": 0.43986200407715226,
      "grad_norm": 0.29388704895973206,
      "learning_rate": 0.00011910054234263792,
      "loss": 0.3544,
      "step": 2805
    },
    {
      "epoch": 0.4406460718206053,
      "grad_norm": 0.5010667443275452,
      "learning_rate": 0.00011885831760520203,
      "loss": 0.4356,
      "step": 2810
    },
    {
      "epoch": 0.44143013956405835,
      "grad_norm": 0.5561220645904541,
      "learning_rate": 0.00011861597808603258,
      "loss": 0.4643,
      "step": 2815
    },
    {
      "epoch": 0.4422142073075114,
      "grad_norm": 0.38170677423477173,
      "learning_rate": 0.00011837352526013649,
      "loss": 0.3813,
      "step": 2820
    },
    {
      "epoch": 0.4429982750509644,
      "grad_norm": 0.44931864738464355,
      "learning_rate": 0.00011813096060321043,
      "loss": 0.4186,
      "step": 2825
    },
    {
      "epoch": 0.4437823427944174,
      "grad_norm": 0.5263799428939819,
      "learning_rate": 0.0001178882855916317,
      "loss": 0.4291,
      "step": 2830
    },
    {
      "epoch": 0.4445664105378705,
      "grad_norm": 0.4793188273906708,
      "learning_rate": 0.00011764550170244924,
      "loss": 0.4266,
      "step": 2835
    },
    {
      "epoch": 0.4453504782813235,
      "grad_norm": 0.5189777612686157,
      "learning_rate": 0.0001174026104133747,
      "loss": 0.3908,
      "step": 2840
    },
    {
      "epoch": 0.44613454602477653,
      "grad_norm": 0.5019972920417786,
      "learning_rate": 0.00011715961320277344,
      "loss": 0.3788,
      "step": 2845
    },
    {
      "epoch": 0.44691861376822956,
      "grad_norm": 0.45763975381851196,
      "learning_rate": 0.0001169165115496555,
      "loss": 0.4063,
      "step": 2850
    },
    {
      "epoch": 0.44770268151168263,
      "grad_norm": 0.4419203996658325,
      "learning_rate": 0.00011667330693366663,
      "loss": 0.3941,
      "step": 2855
    },
    {
      "epoch": 0.44848674925513565,
      "grad_norm": 0.5419099926948547,
      "learning_rate": 0.0001164300008350792,
      "loss": 0.3976,
      "step": 2860
    },
    {
      "epoch": 0.4492708169985887,
      "grad_norm": 0.517630398273468,
      "learning_rate": 0.00011618659473478336,
      "loss": 0.4275,
      "step": 2865
    },
    {
      "epoch": 0.4500548847420417,
      "grad_norm": 0.4864455759525299,
      "learning_rate": 0.00011594309011427784,
      "loss": 0.4118,
      "step": 2870
    },
    {
      "epoch": 0.45083895248549477,
      "grad_norm": 0.5140098929405212,
      "learning_rate": 0.00011569948845566103,
      "loss": 0.4047,
      "step": 2875
    },
    {
      "epoch": 0.4516230202289478,
      "grad_norm": 0.5292649865150452,
      "learning_rate": 0.00011545579124162203,
      "loss": 0.4395,
      "step": 2880
    },
    {
      "epoch": 0.4524070879724008,
      "grad_norm": 0.4365848898887634,
      "learning_rate": 0.0001152119999554314,
      "loss": 0.4036,
      "step": 2885
    },
    {
      "epoch": 0.45319115571585383,
      "grad_norm": 0.49465227127075195,
      "learning_rate": 0.00011496811608093238,
      "loss": 0.4332,
      "step": 2890
    },
    {
      "epoch": 0.4539752234593069,
      "grad_norm": 0.3818402588367462,
      "learning_rate": 0.00011472414110253174,
      "loss": 0.378,
      "step": 2895
    },
    {
      "epoch": 0.45475929120275993,
      "grad_norm": 0.46619319915771484,
      "learning_rate": 0.00011448007650519068,
      "loss": 0.3651,
      "step": 2900
    },
    {
      "epoch": 0.45554335894621295,
      "grad_norm": 0.34325554966926575,
      "learning_rate": 0.00011423592377441596,
      "loss": 0.3793,
      "step": 2905
    },
    {
      "epoch": 0.45632742668966597,
      "grad_norm": 0.4615669548511505,
      "learning_rate": 0.00011399168439625075,
      "loss": 0.384,
      "step": 2910
    },
    {
      "epoch": 0.45711149443311905,
      "grad_norm": 0.46467724442481995,
      "learning_rate": 0.00011374735985726552,
      "loss": 0.394,
      "step": 2915
    },
    {
      "epoch": 0.45789556217657207,
      "grad_norm": 0.433761864900589,
      "learning_rate": 0.0001135029516445492,
      "loss": 0.4034,
      "step": 2920
    },
    {
      "epoch": 0.4586796299200251,
      "grad_norm": 0.5597785711288452,
      "learning_rate": 0.0001132584612456999,
      "loss": 0.3915,
      "step": 2925
    },
    {
      "epoch": 0.4594636976634781,
      "grad_norm": 0.40597814321517944,
      "learning_rate": 0.00011301389014881601,
      "loss": 0.3803,
      "step": 2930
    },
    {
      "epoch": 0.4602477654069312,
      "grad_norm": 0.494845449924469,
      "learning_rate": 0.00011276923984248709,
      "loss": 0.386,
      "step": 2935
    },
    {
      "epoch": 0.4610318331503842,
      "grad_norm": 0.4773111045360565,
      "learning_rate": 0.00011252451181578474,
      "loss": 0.3618,
      "step": 2940
    }
  ],
  "logging_steps": 5,
  "max_steps": 6377,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.738973614913024e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
