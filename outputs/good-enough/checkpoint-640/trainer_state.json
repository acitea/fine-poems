{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.1003606711619884,
  "eval_steps": 500,
  "global_step": 640,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007840677434530343,
      "grad_norm": 2.3289198875427246,
      "learning_rate": 8e-05,
      "loss": 2.9228,
      "step": 5
    },
    {
      "epoch": 0.0015681354869060686,
      "grad_norm": 1.458372712135315,
      "learning_rate": 0.00018,
      "loss": 1.452,
      "step": 10
    },
    {
      "epoch": 0.002352203230359103,
      "grad_norm": 2.707763195037842,
      "learning_rate": 0.00019999980523097434,
      "loss": 0.6784,
      "step": 15
    },
    {
      "epoch": 0.0031362709738121373,
      "grad_norm": 1.0996875762939453,
      "learning_rate": 0.00019999901398310796,
      "loss": 0.5909,
      "step": 20
    },
    {
      "epoch": 0.0039203387172651715,
      "grad_norm": 0.4992644488811493,
      "learning_rate": 0.00019999761408814895,
      "loss": 0.4735,
      "step": 25
    },
    {
      "epoch": 0.004704406460718206,
      "grad_norm": 1.1609508991241455,
      "learning_rate": 0.0001999956055546178,
      "loss": 0.482,
      "step": 30
    },
    {
      "epoch": 0.005488474204171241,
      "grad_norm": 0.7195559144020081,
      "learning_rate": 0.00019999298839473958,
      "loss": 0.5098,
      "step": 35
    },
    {
      "epoch": 0.006272541947624275,
      "grad_norm": 1.6587860584259033,
      "learning_rate": 0.00019998976262444368,
      "loss": 0.552,
      "step": 40
    },
    {
      "epoch": 0.007056609691077309,
      "grad_norm": 0.8686178922653198,
      "learning_rate": 0.00019998592826336387,
      "loss": 0.5122,
      "step": 45
    },
    {
      "epoch": 0.007840677434530343,
      "grad_norm": 0.5433717966079712,
      "learning_rate": 0.00019998148533483808,
      "loss": 0.4985,
      "step": 50
    },
    {
      "epoch": 0.008624745177983378,
      "grad_norm": 0.485207736492157,
      "learning_rate": 0.0001999764338659083,
      "loss": 0.4888,
      "step": 55
    },
    {
      "epoch": 0.009408812921436412,
      "grad_norm": 0.375410258769989,
      "learning_rate": 0.00019997077388732052,
      "loss": 0.5,
      "step": 60
    },
    {
      "epoch": 0.010192880664889447,
      "grad_norm": 0.5047292113304138,
      "learning_rate": 0.00019996450543352436,
      "loss": 0.5086,
      "step": 65
    },
    {
      "epoch": 0.010976948408342482,
      "grad_norm": 0.453966349363327,
      "learning_rate": 0.00019995762854267294,
      "loss": 0.4846,
      "step": 70
    },
    {
      "epoch": 0.011761016151795515,
      "grad_norm": 0.5240188241004944,
      "learning_rate": 0.0001999501432566227,
      "loss": 0.4685,
      "step": 75
    },
    {
      "epoch": 0.01254508389524855,
      "grad_norm": 0.28660473227500916,
      "learning_rate": 0.000199942049620933,
      "loss": 0.4162,
      "step": 80
    },
    {
      "epoch": 0.013329151638701584,
      "grad_norm": 0.454081654548645,
      "learning_rate": 0.00019993334768486607,
      "loss": 0.4521,
      "step": 85
    },
    {
      "epoch": 0.014113219382154618,
      "grad_norm": 0.43364715576171875,
      "learning_rate": 0.0001999240375013865,
      "loss": 0.5191,
      "step": 90
    },
    {
      "epoch": 0.014897287125607653,
      "grad_norm": 0.3220612704753876,
      "learning_rate": 0.00019991411912716095,
      "loss": 0.4448,
      "step": 95
    },
    {
      "epoch": 0.015681354869060686,
      "grad_norm": 0.45167744159698486,
      "learning_rate": 0.000199903592622558,
      "loss": 0.4698,
      "step": 100
    },
    {
      "epoch": 0.01646542261251372,
      "grad_norm": 0.34006062150001526,
      "learning_rate": 0.00019989245805164746,
      "loss": 0.4392,
      "step": 105
    },
    {
      "epoch": 0.017249490355966755,
      "grad_norm": 0.38721418380737305,
      "learning_rate": 0.00019988071548220033,
      "loss": 0.4708,
      "step": 110
    },
    {
      "epoch": 0.01803355809941979,
      "grad_norm": 0.4192662239074707,
      "learning_rate": 0.00019986836498568807,
      "loss": 0.4595,
      "step": 115
    },
    {
      "epoch": 0.018817625842872825,
      "grad_norm": 0.38310176134109497,
      "learning_rate": 0.00019985540663728236,
      "loss": 0.4336,
      "step": 120
    },
    {
      "epoch": 0.01960169358632586,
      "grad_norm": 0.40751150250434875,
      "learning_rate": 0.0001998418405158546,
      "loss": 0.4577,
      "step": 125
    },
    {
      "epoch": 0.020385761329778894,
      "grad_norm": 0.4094284474849701,
      "learning_rate": 0.0001998276667039754,
      "loss": 0.4537,
      "step": 130
    },
    {
      "epoch": 0.02116982907323193,
      "grad_norm": 0.3190658390522003,
      "learning_rate": 0.0001998128852879141,
      "loss": 0.4074,
      "step": 135
    },
    {
      "epoch": 0.021953896816684963,
      "grad_norm": 0.4356575310230255,
      "learning_rate": 0.00019979749635763827,
      "loss": 0.4841,
      "step": 140
    },
    {
      "epoch": 0.022737964560137994,
      "grad_norm": 0.3538850247859955,
      "learning_rate": 0.00019978150000681308,
      "loss": 0.4524,
      "step": 145
    },
    {
      "epoch": 0.02352203230359103,
      "grad_norm": 0.3784758448600769,
      "learning_rate": 0.00019976489633280085,
      "loss": 0.5006,
      "step": 150
    },
    {
      "epoch": 0.024306100047044064,
      "grad_norm": 0.43003490567207336,
      "learning_rate": 0.00019974768543666032,
      "loss": 0.4503,
      "step": 155
    },
    {
      "epoch": 0.0250901677904971,
      "grad_norm": 0.40390828251838684,
      "learning_rate": 0.0001997298674231461,
      "loss": 0.4279,
      "step": 160
    },
    {
      "epoch": 0.025874235533950133,
      "grad_norm": 0.42742839455604553,
      "learning_rate": 0.00019971144240070823,
      "loss": 0.4526,
      "step": 165
    },
    {
      "epoch": 0.026658303277403168,
      "grad_norm": 0.3081049621105194,
      "learning_rate": 0.00019969241048149107,
      "loss": 0.5039,
      "step": 170
    },
    {
      "epoch": 0.027442371020856202,
      "grad_norm": 0.35872742533683777,
      "learning_rate": 0.00019967277178133296,
      "loss": 0.4539,
      "step": 175
    },
    {
      "epoch": 0.028226438764309237,
      "grad_norm": 0.41373229026794434,
      "learning_rate": 0.0001996525264197655,
      "loss": 0.4299,
      "step": 180
    },
    {
      "epoch": 0.02901050650776227,
      "grad_norm": 0.33151111006736755,
      "learning_rate": 0.00019963167452001273,
      "loss": 0.4441,
      "step": 185
    },
    {
      "epoch": 0.029794574251215306,
      "grad_norm": 0.47977975010871887,
      "learning_rate": 0.00019961021620899035,
      "loss": 0.4626,
      "step": 190
    },
    {
      "epoch": 0.03057864199466834,
      "grad_norm": 0.36547034978866577,
      "learning_rate": 0.00019958815161730503,
      "loss": 0.4229,
      "step": 195
    },
    {
      "epoch": 0.03136270973812137,
      "grad_norm": 0.42055875062942505,
      "learning_rate": 0.00019956548087925362,
      "loss": 0.4923,
      "step": 200
    },
    {
      "epoch": 0.03214677748157441,
      "grad_norm": 0.3825159966945648,
      "learning_rate": 0.00019954220413282225,
      "loss": 0.446,
      "step": 205
    },
    {
      "epoch": 0.03293084522502744,
      "grad_norm": 0.3520354628562927,
      "learning_rate": 0.00019951832151968556,
      "loss": 0.4669,
      "step": 210
    },
    {
      "epoch": 0.033714912968480476,
      "grad_norm": 0.3458239734172821,
      "learning_rate": 0.00019949383318520583,
      "loss": 0.4308,
      "step": 215
    },
    {
      "epoch": 0.03449898071193351,
      "grad_norm": 0.3553615212440491,
      "learning_rate": 0.00019946873927843202,
      "loss": 0.4667,
      "step": 220
    },
    {
      "epoch": 0.035283048455386545,
      "grad_norm": 0.36897915601730347,
      "learning_rate": 0.000199443039952099,
      "loss": 0.4048,
      "step": 225
    },
    {
      "epoch": 0.03606711619883958,
      "grad_norm": 0.2773493230342865,
      "learning_rate": 0.00019941673536262653,
      "loss": 0.4113,
      "step": 230
    },
    {
      "epoch": 0.036851183942292615,
      "grad_norm": 0.3920712471008301,
      "learning_rate": 0.00019938982567011828,
      "loss": 0.4636,
      "step": 235
    },
    {
      "epoch": 0.03763525168574565,
      "grad_norm": 0.32330620288848877,
      "learning_rate": 0.00019936231103836094,
      "loss": 0.4492,
      "step": 240
    },
    {
      "epoch": 0.038419319429198684,
      "grad_norm": 0.39345237612724304,
      "learning_rate": 0.00019933419163482318,
      "loss": 0.5127,
      "step": 245
    },
    {
      "epoch": 0.03920338717265172,
      "grad_norm": 0.499350905418396,
      "learning_rate": 0.0001993054676306546,
      "loss": 0.4331,
      "step": 250
    },
    {
      "epoch": 0.03998745491610475,
      "grad_norm": 0.41301843523979187,
      "learning_rate": 0.00019927613920068472,
      "loss": 0.4551,
      "step": 255
    },
    {
      "epoch": 0.04077152265955779,
      "grad_norm": 0.434244304895401,
      "learning_rate": 0.00019924620652342196,
      "loss": 0.4583,
      "step": 260
    },
    {
      "epoch": 0.04155559040301082,
      "grad_norm": 0.43543997406959534,
      "learning_rate": 0.00019921566978105252,
      "loss": 0.4464,
      "step": 265
    },
    {
      "epoch": 0.04233965814646386,
      "grad_norm": 0.41354718804359436,
      "learning_rate": 0.00019918452915943918,
      "loss": 0.4536,
      "step": 270
    },
    {
      "epoch": 0.04312372588991689,
      "grad_norm": 0.42653536796569824,
      "learning_rate": 0.0001991527848481203,
      "loss": 0.4469,
      "step": 275
    },
    {
      "epoch": 0.043907793633369926,
      "grad_norm": 0.422654390335083,
      "learning_rate": 0.00019912043704030862,
      "loss": 0.4298,
      "step": 280
    },
    {
      "epoch": 0.044691861376822954,
      "grad_norm": 0.29739266633987427,
      "learning_rate": 0.00019908748593289012,
      "loss": 0.4358,
      "step": 285
    },
    {
      "epoch": 0.04547592912027599,
      "grad_norm": 0.36249855160713196,
      "learning_rate": 0.00019905393172642267,
      "loss": 0.4218,
      "step": 290
    },
    {
      "epoch": 0.046259996863729023,
      "grad_norm": 0.417453408241272,
      "learning_rate": 0.000199019774625135,
      "loss": 0.4543,
      "step": 295
    },
    {
      "epoch": 0.04704406460718206,
      "grad_norm": 0.3730241358280182,
      "learning_rate": 0.00019898501483692536,
      "loss": 0.4941,
      "step": 300
    },
    {
      "epoch": 0.04782813235063509,
      "grad_norm": 0.4045542776584625,
      "learning_rate": 0.00019894965257336028,
      "loss": 0.452,
      "step": 305
    },
    {
      "epoch": 0.04861220009408813,
      "grad_norm": 0.37072238326072693,
      "learning_rate": 0.00019891368804967333,
      "loss": 0.4524,
      "step": 310
    },
    {
      "epoch": 0.04939626783754116,
      "grad_norm": 0.401275098323822,
      "learning_rate": 0.0001988771214847636,
      "loss": 0.4239,
      "step": 315
    },
    {
      "epoch": 0.0501803355809942,
      "grad_norm": 0.3779895603656769,
      "learning_rate": 0.00019883995310119467,
      "loss": 0.4298,
      "step": 320
    },
    {
      "epoch": 0.05096440332444723,
      "grad_norm": 0.3771805465221405,
      "learning_rate": 0.00019880218312519308,
      "loss": 0.4115,
      "step": 325
    },
    {
      "epoch": 0.051748471067900266,
      "grad_norm": 0.41461867094039917,
      "learning_rate": 0.0001987638117866469,
      "loss": 0.4568,
      "step": 330
    },
    {
      "epoch": 0.0525325388113533,
      "grad_norm": 0.4046069383621216,
      "learning_rate": 0.00019872483931910454,
      "loss": 0.4615,
      "step": 335
    },
    {
      "epoch": 0.053316606554806335,
      "grad_norm": 0.3434845805168152,
      "learning_rate": 0.00019868526595977304,
      "loss": 0.4307,
      "step": 340
    },
    {
      "epoch": 0.05410067429825937,
      "grad_norm": 0.37292349338531494,
      "learning_rate": 0.00019864509194951694,
      "loss": 0.4656,
      "step": 345
    },
    {
      "epoch": 0.054884742041712405,
      "grad_norm": 0.2738417088985443,
      "learning_rate": 0.00019860431753285657,
      "loss": 0.4632,
      "step": 350
    },
    {
      "epoch": 0.05566880978516544,
      "grad_norm": 0.3072319030761719,
      "learning_rate": 0.0001985629429579667,
      "loss": 0.3651,
      "step": 355
    },
    {
      "epoch": 0.056452877528618474,
      "grad_norm": 0.3512754738330841,
      "learning_rate": 0.00019852096847667496,
      "loss": 0.4346,
      "step": 360
    },
    {
      "epoch": 0.05723694527207151,
      "grad_norm": 0.3211585581302643,
      "learning_rate": 0.0001984783943444603,
      "loss": 0.3915,
      "step": 365
    },
    {
      "epoch": 0.05802101301552454,
      "grad_norm": 0.6580262780189514,
      "learning_rate": 0.00019843522082045153,
      "loss": 0.4575,
      "step": 370
    },
    {
      "epoch": 0.05880508075897758,
      "grad_norm": 0.39076414704322815,
      "learning_rate": 0.0001983914481674256,
      "loss": 0.4375,
      "step": 375
    },
    {
      "epoch": 0.05958914850243061,
      "grad_norm": 0.46050822734832764,
      "learning_rate": 0.00019834707665180614,
      "loss": 0.4503,
      "step": 380
    },
    {
      "epoch": 0.06037321624588365,
      "grad_norm": 0.3523910641670227,
      "learning_rate": 0.00019830210654366175,
      "loss": 0.4732,
      "step": 385
    },
    {
      "epoch": 0.06115728398933668,
      "grad_norm": 0.42035093903541565,
      "learning_rate": 0.0001982565381167044,
      "loss": 0.4956,
      "step": 390
    },
    {
      "epoch": 0.061941351732789716,
      "grad_norm": 0.4172050654888153,
      "learning_rate": 0.0001982103716482877,
      "loss": 0.3986,
      "step": 395
    },
    {
      "epoch": 0.06272541947624274,
      "grad_norm": 0.4638385474681854,
      "learning_rate": 0.0001981636074194053,
      "loss": 0.4885,
      "step": 400
    },
    {
      "epoch": 0.06350948721969578,
      "grad_norm": 0.3210528492927551,
      "learning_rate": 0.00019811624571468915,
      "loss": 0.4142,
      "step": 405
    },
    {
      "epoch": 0.06429355496314881,
      "grad_norm": 0.4444524049758911,
      "learning_rate": 0.0001980682868224077,
      "loss": 0.4399,
      "step": 410
    },
    {
      "epoch": 0.06507762270660185,
      "grad_norm": 0.41237786412239075,
      "learning_rate": 0.00019801973103446426,
      "loss": 0.4289,
      "step": 415
    },
    {
      "epoch": 0.06586169045005488,
      "grad_norm": 0.3561651110649109,
      "learning_rate": 0.0001979705786463951,
      "loss": 0.4384,
      "step": 420
    },
    {
      "epoch": 0.06664575819350792,
      "grad_norm": 0.4199094772338867,
      "learning_rate": 0.00019792082995736775,
      "loss": 0.4679,
      "step": 425
    },
    {
      "epoch": 0.06742982593696095,
      "grad_norm": 0.4171653091907501,
      "learning_rate": 0.00019787048527017918,
      "loss": 0.4326,
      "step": 430
    },
    {
      "epoch": 0.06821389368041399,
      "grad_norm": 0.35874274373054504,
      "learning_rate": 0.0001978195448912539,
      "loss": 0.4161,
      "step": 435
    },
    {
      "epoch": 0.06899796142386702,
      "grad_norm": 0.44891905784606934,
      "learning_rate": 0.00019776800913064204,
      "loss": 0.4846,
      "step": 440
    },
    {
      "epoch": 0.06978202916732006,
      "grad_norm": 0.43674665689468384,
      "learning_rate": 0.00019771587830201765,
      "loss": 0.4205,
      "step": 445
    },
    {
      "epoch": 0.07056609691077309,
      "grad_norm": 0.4157765805721283,
      "learning_rate": 0.00019766315272267662,
      "loss": 0.5208,
      "step": 450
    },
    {
      "epoch": 0.07135016465422613,
      "grad_norm": 0.4280984401702881,
      "learning_rate": 0.00019760983271353478,
      "loss": 0.4709,
      "step": 455
    },
    {
      "epoch": 0.07213423239767916,
      "grad_norm": 0.4700545370578766,
      "learning_rate": 0.00019755591859912607,
      "loss": 0.5087,
      "step": 460
    },
    {
      "epoch": 0.0729183001411322,
      "grad_norm": 0.3885340094566345,
      "learning_rate": 0.0001975014107076004,
      "loss": 0.4361,
      "step": 465
    },
    {
      "epoch": 0.07370236788458523,
      "grad_norm": 0.3809219002723694,
      "learning_rate": 0.00019744630937072174,
      "loss": 0.4203,
      "step": 470
    },
    {
      "epoch": 0.07448643562803826,
      "grad_norm": 0.36258992552757263,
      "learning_rate": 0.0001973906149238661,
      "loss": 0.4929,
      "step": 475
    },
    {
      "epoch": 0.0752705033714913,
      "grad_norm": 0.34333714842796326,
      "learning_rate": 0.00019733432770601938,
      "loss": 0.4554,
      "step": 480
    },
    {
      "epoch": 0.07605457111494433,
      "grad_norm": 0.43987563252449036,
      "learning_rate": 0.00019727744805977555,
      "loss": 0.4273,
      "step": 485
    },
    {
      "epoch": 0.07683863885839737,
      "grad_norm": 0.44000664353370667,
      "learning_rate": 0.0001972199763313343,
      "loss": 0.4286,
      "step": 490
    },
    {
      "epoch": 0.0776227066018504,
      "grad_norm": 0.44358718395233154,
      "learning_rate": 0.0001971619128704991,
      "loss": 0.4388,
      "step": 495
    },
    {
      "epoch": 0.07840677434530344,
      "grad_norm": 0.26203158497810364,
      "learning_rate": 0.000197103258030675,
      "loss": 0.3883,
      "step": 500
    },
    {
      "epoch": 0.07919084208875647,
      "grad_norm": 0.4797963798046112,
      "learning_rate": 0.00019704401216886648,
      "loss": 0.4427,
      "step": 505
    },
    {
      "epoch": 0.0799749098322095,
      "grad_norm": 0.42094534635543823,
      "learning_rate": 0.00019698417564567534,
      "loss": 0.4318,
      "step": 510
    },
    {
      "epoch": 0.08075897757566254,
      "grad_norm": 0.423238068819046,
      "learning_rate": 0.0001969237488252984,
      "loss": 0.4542,
      "step": 515
    },
    {
      "epoch": 0.08154304531911558,
      "grad_norm": 0.4854225516319275,
      "learning_rate": 0.00019686273207552538,
      "loss": 0.4568,
      "step": 520
    },
    {
      "epoch": 0.08232711306256861,
      "grad_norm": 0.3623478412628174,
      "learning_rate": 0.00019680112576773664,
      "loss": 0.4201,
      "step": 525
    },
    {
      "epoch": 0.08311118080602165,
      "grad_norm": 0.44924843311309814,
      "learning_rate": 0.00019673893027690085,
      "loss": 0.4828,
      "step": 530
    },
    {
      "epoch": 0.08389524854947468,
      "grad_norm": 0.4414058327674866,
      "learning_rate": 0.0001966761459815728,
      "loss": 0.4523,
      "step": 535
    },
    {
      "epoch": 0.08467931629292771,
      "grad_norm": 0.3431743085384369,
      "learning_rate": 0.000196612773263891,
      "loss": 0.45,
      "step": 540
    },
    {
      "epoch": 0.08546338403638075,
      "grad_norm": 0.41478145122528076,
      "learning_rate": 0.00019654881250957552,
      "loss": 0.4421,
      "step": 545
    },
    {
      "epoch": 0.08624745177983378,
      "grad_norm": 0.3098265528678894,
      "learning_rate": 0.0001964842641079255,
      "loss": 0.4599,
      "step": 550
    },
    {
      "epoch": 0.08703151952328682,
      "grad_norm": 0.39982128143310547,
      "learning_rate": 0.00019641912845181668,
      "loss": 0.4471,
      "step": 555
    },
    {
      "epoch": 0.08781558726673985,
      "grad_norm": 0.37757599353790283,
      "learning_rate": 0.00019635340593769933,
      "loss": 0.4218,
      "step": 560
    },
    {
      "epoch": 0.08859965501019289,
      "grad_norm": 0.3676394820213318,
      "learning_rate": 0.00019628709696559553,
      "loss": 0.4261,
      "step": 565
    },
    {
      "epoch": 0.08938372275364591,
      "grad_norm": 0.37905994057655334,
      "learning_rate": 0.0001962202019390969,
      "loss": 0.4109,
      "step": 570
    },
    {
      "epoch": 0.09016779049709894,
      "grad_norm": 0.39505690336227417,
      "learning_rate": 0.00019615272126536208,
      "loss": 0.4709,
      "step": 575
    },
    {
      "epoch": 0.09095185824055198,
      "grad_norm": 0.4610424041748047,
      "learning_rate": 0.0001960846553551143,
      "loss": 0.4071,
      "step": 580
    },
    {
      "epoch": 0.09173592598400501,
      "grad_norm": 0.40436193346977234,
      "learning_rate": 0.00019601600462263878,
      "loss": 0.4765,
      "step": 585
    },
    {
      "epoch": 0.09251999372745805,
      "grad_norm": 0.4708491265773773,
      "learning_rate": 0.00019594676948578036,
      "loss": 0.4383,
      "step": 590
    },
    {
      "epoch": 0.09330406147091108,
      "grad_norm": 0.37091848254203796,
      "learning_rate": 0.00019587695036594086,
      "loss": 0.4818,
      "step": 595
    },
    {
      "epoch": 0.09408812921436412,
      "grad_norm": 0.4395599961280823,
      "learning_rate": 0.0001958065476880765,
      "loss": 0.4795,
      "step": 600
    },
    {
      "epoch": 0.09487219695781715,
      "grad_norm": 0.4465431869029999,
      "learning_rate": 0.00019573556188069534,
      "loss": 0.4336,
      "step": 605
    },
    {
      "epoch": 0.09565626470127019,
      "grad_norm": 0.42856889963150024,
      "learning_rate": 0.00019566399337585472,
      "loss": 0.4257,
      "step": 610
    },
    {
      "epoch": 0.09644033244472322,
      "grad_norm": 0.415412575006485,
      "learning_rate": 0.00019559184260915856,
      "loss": 0.4796,
      "step": 615
    },
    {
      "epoch": 0.09722440018817625,
      "grad_norm": 0.3812630772590637,
      "learning_rate": 0.00019551911001975468,
      "loss": 0.4434,
      "step": 620
    },
    {
      "epoch": 0.09800846793162929,
      "grad_norm": 0.3964299261569977,
      "learning_rate": 0.0001954457960503323,
      "loss": 0.4408,
      "step": 625
    },
    {
      "epoch": 0.09879253567508232,
      "grad_norm": 0.36963802576065063,
      "learning_rate": 0.00019537190114711912,
      "loss": 0.4313,
      "step": 630
    },
    {
      "epoch": 0.09957660341853536,
      "grad_norm": 0.38398024439811707,
      "learning_rate": 0.0001952974257598788,
      "loss": 0.4724,
      "step": 635
    },
    {
      "epoch": 0.1003606711619884,
      "grad_norm": 0.3485054671764374,
      "learning_rate": 0.00019522237034190801,
      "loss": 0.3996,
      "step": 640
    }
  ],
  "logging_steps": 5,
  "max_steps": 6377,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.244945220521472e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
