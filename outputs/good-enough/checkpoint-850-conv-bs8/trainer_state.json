{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.10663321310961267,
  "eval_steps": 500,
  "global_step": 850,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006272541947624275,
      "grad_norm": 13.659195899963379,
      "learning_rate": 0.0,
      "loss": 3.0121,
      "step": 5
    },
    {
      "epoch": 0.001254508389524855,
      "grad_norm": NaN,
      "learning_rate": 4e-05,
      "loss": 2.9786,
      "step": 10
    },
    {
      "epoch": 0.0018817625842872823,
      "grad_norm": 16.540952682495117,
      "learning_rate": 0.00012,
      "loss": 2.4468,
      "step": 15
    },
    {
      "epoch": 0.00250901677904971,
      "grad_norm": 2.3874289989471436,
      "learning_rate": 0.00019999999221559546,
      "loss": 0.9872,
      "step": 20
    },
    {
      "epoch": 0.0031362709738121373,
      "grad_norm": 1.4897716045379639,
      "learning_rate": 0.0001999997197615636,
      "loss": 0.6195,
      "step": 25
    },
    {
      "epoch": 0.0037635251685745647,
      "grad_norm": 1.3095494508743286,
      "learning_rate": 0.0001999990580885164,
      "loss": 0.5292,
      "step": 30
    },
    {
      "epoch": 0.0043907793633369925,
      "grad_norm": 0.6328980326652527,
      "learning_rate": 0.0001999980071990292,
      "loss": 0.5803,
      "step": 35
    },
    {
      "epoch": 0.00501803355809942,
      "grad_norm": 1.1310646533966064,
      "learning_rate": 0.0001999965670971923,
      "loss": 0.5158,
      "step": 40
    },
    {
      "epoch": 0.005645287752861847,
      "grad_norm": 0.45717576146125793,
      "learning_rate": 0.00019999473778861083,
      "loss": 0.4779,
      "step": 45
    },
    {
      "epoch": 0.006272541947624275,
      "grad_norm": 0.4348587691783905,
      "learning_rate": 0.00019999251928040486,
      "loss": 0.4759,
      "step": 50
    },
    {
      "epoch": 0.006899796142386702,
      "grad_norm": 0.49114835262298584,
      "learning_rate": 0.0001999899115812092,
      "loss": 0.4998,
      "step": 55
    },
    {
      "epoch": 0.007527050337149129,
      "grad_norm": 0.42155203223228455,
      "learning_rate": 0.00019998691470117364,
      "loss": 0.5097,
      "step": 60
    },
    {
      "epoch": 0.008154304531911558,
      "grad_norm": 0.4590725898742676,
      "learning_rate": 0.0001999835286519626,
      "loss": 0.516,
      "step": 65
    },
    {
      "epoch": 0.008781558726673985,
      "grad_norm": 0.36183956265449524,
      "learning_rate": 0.00019997975344675527,
      "loss": 0.49,
      "step": 70
    },
    {
      "epoch": 0.009408812921436412,
      "grad_norm": 0.4047176241874695,
      "learning_rate": 0.00019997558910024547,
      "loss": 0.4975,
      "step": 75
    },
    {
      "epoch": 0.01003606711619884,
      "grad_norm": 0.3571915626525879,
      "learning_rate": 0.00019997103562864173,
      "loss": 0.4545,
      "step": 80
    },
    {
      "epoch": 0.010663321310961267,
      "grad_norm": 0.35069966316223145,
      "learning_rate": 0.000199966093049667,
      "loss": 0.4551,
      "step": 85
    },
    {
      "epoch": 0.011290575505723694,
      "grad_norm": 0.39526233077049255,
      "learning_rate": 0.00019996076138255886,
      "loss": 0.4743,
      "step": 90
    },
    {
      "epoch": 0.011917829700486122,
      "grad_norm": 0.3323369324207306,
      "learning_rate": 0.0001999550406480692,
      "loss": 0.4416,
      "step": 95
    },
    {
      "epoch": 0.01254508389524855,
      "grad_norm": 0.5120255351066589,
      "learning_rate": 0.0001999489308684643,
      "loss": 0.452,
      "step": 100
    },
    {
      "epoch": 0.013172338090010977,
      "grad_norm": 0.44912686944007874,
      "learning_rate": 0.00019994243206752463,
      "loss": 0.4901,
      "step": 105
    },
    {
      "epoch": 0.013799592284773404,
      "grad_norm": 0.4468529522418976,
      "learning_rate": 0.0001999355442705448,
      "loss": 0.4721,
      "step": 110
    },
    {
      "epoch": 0.014426846479535831,
      "grad_norm": 0.5955290198326111,
      "learning_rate": 0.00019992826750433356,
      "loss": 0.4901,
      "step": 115
    },
    {
      "epoch": 0.015054100674298259,
      "grad_norm": 0.34080928564071655,
      "learning_rate": 0.0001999206017972135,
      "loss": 0.4702,
      "step": 120
    },
    {
      "epoch": 0.015681354869060686,
      "grad_norm": 0.3532222807407379,
      "learning_rate": 0.0001999125471790211,
      "loss": 0.4527,
      "step": 125
    },
    {
      "epoch": 0.016308609063823115,
      "grad_norm": 0.36545154452323914,
      "learning_rate": 0.00019990410368110658,
      "loss": 0.4744,
      "step": 130
    },
    {
      "epoch": 0.01693586325858554,
      "grad_norm": 0.42260757088661194,
      "learning_rate": 0.0001998952713363337,
      "loss": 0.4658,
      "step": 135
    },
    {
      "epoch": 0.01756311745334797,
      "grad_norm": 0.4380159378051758,
      "learning_rate": 0.00019988605017907975,
      "loss": 0.4867,
      "step": 140
    },
    {
      "epoch": 0.018190371648110396,
      "grad_norm": 0.37973684072494507,
      "learning_rate": 0.0001998764402452353,
      "loss": 0.4692,
      "step": 145
    },
    {
      "epoch": 0.018817625842872825,
      "grad_norm": 0.44650834798812866,
      "learning_rate": 0.00019986644157220416,
      "loss": 0.5088,
      "step": 150
    },
    {
      "epoch": 0.01944488003763525,
      "grad_norm": 0.43508854508399963,
      "learning_rate": 0.0001998560541989032,
      "loss": 0.4409,
      "step": 155
    },
    {
      "epoch": 0.02007213423239768,
      "grad_norm": 0.3750479519367218,
      "learning_rate": 0.0001998452781657621,
      "loss": 0.4909,
      "step": 160
    },
    {
      "epoch": 0.020699388427160105,
      "grad_norm": 0.4156889021396637,
      "learning_rate": 0.00019983411351472344,
      "loss": 0.4461,
      "step": 165
    },
    {
      "epoch": 0.021326642621922534,
      "grad_norm": 1.0788156986236572,
      "learning_rate": 0.00019982256028924217,
      "loss": 0.4421,
      "step": 170
    },
    {
      "epoch": 0.021953896816684963,
      "grad_norm": 0.33844253420829773,
      "learning_rate": 0.00019981061853428588,
      "loss": 0.4521,
      "step": 175
    },
    {
      "epoch": 0.02258115101144739,
      "grad_norm": 0.39264944195747375,
      "learning_rate": 0.0001997982882963342,
      "loss": 0.4529,
      "step": 180
    },
    {
      "epoch": 0.023208405206209818,
      "grad_norm": 0.40720200538635254,
      "learning_rate": 0.00019978556962337894,
      "loss": 0.4941,
      "step": 185
    },
    {
      "epoch": 0.023835659400972244,
      "grad_norm": 0.3836430609226227,
      "learning_rate": 0.00019977246256492373,
      "loss": 0.465,
      "step": 190
    },
    {
      "epoch": 0.024462913595734673,
      "grad_norm": 0.3754042088985443,
      "learning_rate": 0.00019975896717198385,
      "loss": 0.4396,
      "step": 195
    },
    {
      "epoch": 0.0250901677904971,
      "grad_norm": 0.3660666048526764,
      "learning_rate": 0.00019974508349708612,
      "loss": 0.4494,
      "step": 200
    },
    {
      "epoch": 0.025717421985259527,
      "grad_norm": 0.32304835319519043,
      "learning_rate": 0.00019973081159426856,
      "loss": 0.4707,
      "step": 205
    },
    {
      "epoch": 0.026344676180021953,
      "grad_norm": 0.4389673173427582,
      "learning_rate": 0.0001997161515190803,
      "loss": 0.527,
      "step": 210
    },
    {
      "epoch": 0.026971930374784382,
      "grad_norm": 0.39373692870140076,
      "learning_rate": 0.0001997011033285813,
      "loss": 0.4871,
      "step": 215
    },
    {
      "epoch": 0.027599184569546808,
      "grad_norm": 0.38724982738494873,
      "learning_rate": 0.00019968566708134216,
      "loss": 0.5117,
      "step": 220
    },
    {
      "epoch": 0.028226438764309237,
      "grad_norm": 0.38187316060066223,
      "learning_rate": 0.0001996698428374438,
      "loss": 0.4748,
      "step": 225
    },
    {
      "epoch": 0.028853692959071663,
      "grad_norm": 0.37286514043807983,
      "learning_rate": 0.00019965363065847745,
      "loss": 0.5054,
      "step": 230
    },
    {
      "epoch": 0.02948094715383409,
      "grad_norm": 0.3965896964073181,
      "learning_rate": 0.00019963703060754407,
      "loss": 0.5137,
      "step": 235
    },
    {
      "epoch": 0.030108201348596517,
      "grad_norm": 0.34062623977661133,
      "learning_rate": 0.00019962004274925446,
      "loss": 0.4234,
      "step": 240
    },
    {
      "epoch": 0.030735455543358946,
      "grad_norm": 0.4796328544616699,
      "learning_rate": 0.00019960266714972878,
      "loss": 0.4945,
      "step": 245
    },
    {
      "epoch": 0.03136270973812137,
      "grad_norm": 0.4689682126045227,
      "learning_rate": 0.0001995849038765963,
      "loss": 0.4481,
      "step": 250
    },
    {
      "epoch": 0.0319899639328838,
      "grad_norm": 0.3271242678165436,
      "learning_rate": 0.00019956675299899533,
      "loss": 0.4428,
      "step": 255
    },
    {
      "epoch": 0.03261721812764623,
      "grad_norm": 0.4043657183647156,
      "learning_rate": 0.00019954821458757272,
      "loss": 0.4712,
      "step": 260
    },
    {
      "epoch": 0.03324447232240866,
      "grad_norm": 0.39384621381759644,
      "learning_rate": 0.00019952928871448363,
      "loss": 0.4595,
      "step": 265
    },
    {
      "epoch": 0.03387172651717108,
      "grad_norm": 0.3393091857433319,
      "learning_rate": 0.00019950997545339143,
      "loss": 0.4471,
      "step": 270
    },
    {
      "epoch": 0.03449898071193351,
      "grad_norm": 0.42079323530197144,
      "learning_rate": 0.00019949027487946717,
      "loss": 0.4724,
      "step": 275
    },
    {
      "epoch": 0.03512623490669594,
      "grad_norm": 0.24171890318393707,
      "learning_rate": 0.00019947018706938948,
      "loss": 0.4555,
      "step": 280
    },
    {
      "epoch": 0.03575348910145837,
      "grad_norm": 0.2960837781429291,
      "learning_rate": 0.0001994497121013441,
      "loss": 0.4474,
      "step": 285
    },
    {
      "epoch": 0.03638074329622079,
      "grad_norm": 0.8441790342330933,
      "learning_rate": 0.00019942885005502382,
      "loss": 0.4349,
      "step": 290
    },
    {
      "epoch": 0.03700799749098322,
      "grad_norm": 0.434327095746994,
      "learning_rate": 0.00019940760101162783,
      "loss": 0.4979,
      "step": 295
    },
    {
      "epoch": 0.03763525168574565,
      "grad_norm": 0.3743935823440552,
      "learning_rate": 0.00019938596505386172,
      "loss": 0.4508,
      "step": 300
    },
    {
      "epoch": 0.03826250588050808,
      "grad_norm": 0.4536513388156891,
      "learning_rate": 0.00019936394226593697,
      "loss": 0.4837,
      "step": 305
    },
    {
      "epoch": 0.0388897600752705,
      "grad_norm": 0.5617051720619202,
      "learning_rate": 0.00019934153273357073,
      "loss": 0.4279,
      "step": 310
    },
    {
      "epoch": 0.03951701427003293,
      "grad_norm": 0.43752121925354004,
      "learning_rate": 0.0001993187365439854,
      "loss": 0.4396,
      "step": 315
    },
    {
      "epoch": 0.04014426846479536,
      "grad_norm": 0.27474266290664673,
      "learning_rate": 0.00019929555378590824,
      "loss": 0.4313,
      "step": 320
    },
    {
      "epoch": 0.04077152265955779,
      "grad_norm": 0.44448041915893555,
      "learning_rate": 0.00019927198454957137,
      "loss": 0.4549,
      "step": 325
    },
    {
      "epoch": 0.04139877685432021,
      "grad_norm": 0.3956523835659027,
      "learning_rate": 0.00019924802892671087,
      "loss": 0.4673,
      "step": 330
    },
    {
      "epoch": 0.04202603104908264,
      "grad_norm": 0.3833705484867096,
      "learning_rate": 0.00019922368701056687,
      "loss": 0.4423,
      "step": 335
    },
    {
      "epoch": 0.04265328524384507,
      "grad_norm": 0.39429453015327454,
      "learning_rate": 0.000199198958895883,
      "loss": 0.4854,
      "step": 340
    },
    {
      "epoch": 0.0432805394386075,
      "grad_norm": 0.43289893865585327,
      "learning_rate": 0.00019917384467890608,
      "loss": 0.4568,
      "step": 345
    },
    {
      "epoch": 0.043907793633369926,
      "grad_norm": 0.3285203278064728,
      "learning_rate": 0.0001991483444573857,
      "loss": 0.481,
      "step": 350
    },
    {
      "epoch": 0.04453504782813235,
      "grad_norm": 0.4663844704627991,
      "learning_rate": 0.0001991224583305738,
      "loss": 0.5146,
      "step": 355
    },
    {
      "epoch": 0.04516230202289478,
      "grad_norm": 0.4383179247379303,
      "learning_rate": 0.00019909618639922446,
      "loss": 0.4477,
      "step": 360
    },
    {
      "epoch": 0.04578955621765721,
      "grad_norm": 0.3643399775028229,
      "learning_rate": 0.00019906952876559323,
      "loss": 0.3985,
      "step": 365
    },
    {
      "epoch": 0.046416810412419636,
      "grad_norm": 0.37050551176071167,
      "learning_rate": 0.00019904248553343702,
      "loss": 0.4602,
      "step": 370
    },
    {
      "epoch": 0.04704406460718206,
      "grad_norm": 0.4550255537033081,
      "learning_rate": 0.00019901505680801357,
      "loss": 0.4452,
      "step": 375
    },
    {
      "epoch": 0.04767131880194449,
      "grad_norm": 0.39066338539123535,
      "learning_rate": 0.00019898724269608092,
      "loss": 0.4415,
      "step": 380
    },
    {
      "epoch": 0.048298572996706916,
      "grad_norm": 0.6847397685050964,
      "learning_rate": 0.00019895904330589724,
      "loss": 0.4519,
      "step": 385
    },
    {
      "epoch": 0.048925827191469345,
      "grad_norm": 0.4254884421825409,
      "learning_rate": 0.00019893045874722022,
      "loss": 0.4311,
      "step": 390
    },
    {
      "epoch": 0.04955308138623177,
      "grad_norm": 22.49774742126465,
      "learning_rate": 0.00019890148913130672,
      "loss": 0.4406,
      "step": 395
    },
    {
      "epoch": 0.0501803355809942,
      "grad_norm": 0.28526028990745544,
      "learning_rate": 0.00019887213457091222,
      "loss": 0.4148,
      "step": 400
    },
    {
      "epoch": 0.050807589775756626,
      "grad_norm": 0.2861546277999878,
      "learning_rate": 0.00019884239518029072,
      "loss": 0.4507,
      "step": 405
    },
    {
      "epoch": 0.051434843970519055,
      "grad_norm": 0.35153791308403015,
      "learning_rate": 0.00019881227107519377,
      "loss": 0.4806,
      "step": 410
    },
    {
      "epoch": 0.05206209816528148,
      "grad_norm": 0.37314844131469727,
      "learning_rate": 0.00019878176237287054,
      "loss": 0.4486,
      "step": 415
    },
    {
      "epoch": 0.052689352360043906,
      "grad_norm": 0.4166308343410492,
      "learning_rate": 0.00019875086919206696,
      "loss": 0.5034,
      "step": 420
    },
    {
      "epoch": 0.053316606554806335,
      "grad_norm": 0.3276212811470032,
      "learning_rate": 0.00019871959165302557,
      "loss": 0.479,
      "step": 425
    },
    {
      "epoch": 0.053943860749568764,
      "grad_norm": 0.3666975200176239,
      "learning_rate": 0.00019868792987748476,
      "loss": 0.4362,
      "step": 430
    },
    {
      "epoch": 0.054571114944331194,
      "grad_norm": 0.32016077637672424,
      "learning_rate": 0.0001986558839886786,
      "loss": 0.4833,
      "step": 435
    },
    {
      "epoch": 0.055198369139093616,
      "grad_norm": 0.3588480055332184,
      "learning_rate": 0.0001986234541113361,
      "loss": 0.4243,
      "step": 440
    },
    {
      "epoch": 0.055825623333856045,
      "grad_norm": 0.37194621562957764,
      "learning_rate": 0.0001985906403716809,
      "loss": 0.4321,
      "step": 445
    },
    {
      "epoch": 0.056452877528618474,
      "grad_norm": 0.31377264857292175,
      "learning_rate": 0.00019855744289743062,
      "loss": 0.4378,
      "step": 450
    },
    {
      "epoch": 0.0570801317233809,
      "grad_norm": 0.31273943185806274,
      "learning_rate": 0.00019852386181779653,
      "loss": 0.4244,
      "step": 455
    },
    {
      "epoch": 0.057707385918143325,
      "grad_norm": 0.43559032678604126,
      "learning_rate": 0.0001984898972634829,
      "loss": 0.4099,
      "step": 460
    },
    {
      "epoch": 0.058334640112905754,
      "grad_norm": 0.3663683235645294,
      "learning_rate": 0.0001984555493666867,
      "loss": 0.4743,
      "step": 465
    },
    {
      "epoch": 0.05896189430766818,
      "grad_norm": 0.39485403895378113,
      "learning_rate": 0.00019842081826109675,
      "loss": 0.4413,
      "step": 470
    },
    {
      "epoch": 0.05958914850243061,
      "grad_norm": 0.3570019602775574,
      "learning_rate": 0.00019838570408189357,
      "loss": 0.4665,
      "step": 475
    },
    {
      "epoch": 0.060216402697193035,
      "grad_norm": 0.4352548122406006,
      "learning_rate": 0.00019835020696574857,
      "loss": 0.4491,
      "step": 480
    },
    {
      "epoch": 0.060843656891955464,
      "grad_norm": 0.3973173499107361,
      "learning_rate": 0.00019831432705082365,
      "loss": 0.4494,
      "step": 485
    },
    {
      "epoch": 0.06147091108671789,
      "grad_norm": 0.34082332253456116,
      "learning_rate": 0.00019827806447677068,
      "loss": 0.3984,
      "step": 490
    },
    {
      "epoch": 0.06209816528148032,
      "grad_norm": 0.47701844573020935,
      "learning_rate": 0.0001982414193847309,
      "loss": 0.5043,
      "step": 495
    },
    {
      "epoch": 0.06272541947624274,
      "grad_norm": 0.4604191482067108,
      "learning_rate": 0.00019820439191733434,
      "loss": 0.4799,
      "step": 500
    },
    {
      "epoch": 0.06335267367100518,
      "grad_norm": 0.37102293968200684,
      "learning_rate": 0.0001981669822186994,
      "loss": 0.4621,
      "step": 505
    },
    {
      "epoch": 0.0639799278657676,
      "grad_norm": 0.5120315551757812,
      "learning_rate": 0.00019812919043443204,
      "loss": 0.4849,
      "step": 510
    },
    {
      "epoch": 0.06460718206053002,
      "grad_norm": 0.3425748944282532,
      "learning_rate": 0.0001980910167116256,
      "loss": 0.4718,
      "step": 515
    },
    {
      "epoch": 0.06523443625529246,
      "grad_norm": 0.3916042149066925,
      "learning_rate": 0.00019805246119885986,
      "loss": 0.516,
      "step": 520
    },
    {
      "epoch": 0.06586169045005488,
      "grad_norm": 0.4513245224952698,
      "learning_rate": 0.00019801352404620056,
      "loss": 0.4479,
      "step": 525
    },
    {
      "epoch": 0.06648894464481732,
      "grad_norm": 0.4316195249557495,
      "learning_rate": 0.00019797420540519902,
      "loss": 0.4797,
      "step": 530
    },
    {
      "epoch": 0.06711619883957974,
      "grad_norm": 0.4854297339916229,
      "learning_rate": 0.00019793450542889124,
      "loss": 0.4493,
      "step": 535
    },
    {
      "epoch": 0.06774345303434216,
      "grad_norm": 0.4268428385257721,
      "learning_rate": 0.0001978944242717975,
      "loss": 0.4474,
      "step": 540
    },
    {
      "epoch": 0.0683707072291046,
      "grad_norm": 0.4027538299560547,
      "learning_rate": 0.00019785396208992176,
      "loss": 0.4738,
      "step": 545
    },
    {
      "epoch": 0.06899796142386702,
      "grad_norm": 0.47467759251594543,
      "learning_rate": 0.00019781311904075094,
      "loss": 0.448,
      "step": 550
    },
    {
      "epoch": 0.06962521561862944,
      "grad_norm": 0.42065900564193726,
      "learning_rate": 0.00019777189528325445,
      "loss": 0.4824,
      "step": 555
    },
    {
      "epoch": 0.07025246981339188,
      "grad_norm": 0.39703238010406494,
      "learning_rate": 0.00019773029097788335,
      "loss": 0.4831,
      "step": 560
    },
    {
      "epoch": 0.0708797240081543,
      "grad_norm": 0.3027372658252716,
      "learning_rate": 0.00019768830628657004,
      "loss": 0.3962,
      "step": 565
    },
    {
      "epoch": 0.07150697820291674,
      "grad_norm": 0.36780375242233276,
      "learning_rate": 0.00019764594137272736,
      "loss": 0.421,
      "step": 570
    },
    {
      "epoch": 0.07213423239767916,
      "grad_norm": 0.3902673125267029,
      "learning_rate": 0.00019760319640124805,
      "loss": 0.4466,
      "step": 575
    },
    {
      "epoch": 0.07276148659244158,
      "grad_norm": 0.5111082792282104,
      "learning_rate": 0.00019756007153850415,
      "loss": 0.4592,
      "step": 580
    },
    {
      "epoch": 0.07338874078720402,
      "grad_norm": 0.5047140717506409,
      "learning_rate": 0.0001975165669523463,
      "loss": 0.4774,
      "step": 585
    },
    {
      "epoch": 0.07401599498196644,
      "grad_norm": 0.4444524645805359,
      "learning_rate": 0.0001974726828121031,
      "loss": 0.4795,
      "step": 590
    },
    {
      "epoch": 0.07464324917672886,
      "grad_norm": 0.4263084828853607,
      "learning_rate": 0.00019742841928858048,
      "loss": 0.4796,
      "step": 595
    },
    {
      "epoch": 0.0752705033714913,
      "grad_norm": 0.45974019169807434,
      "learning_rate": 0.0001973837765540609,
      "loss": 0.4441,
      "step": 600
    },
    {
      "epoch": 0.07589775756625372,
      "grad_norm": 0.4083701968193054,
      "learning_rate": 0.00019733875478230285,
      "loss": 0.4521,
      "step": 605
    },
    {
      "epoch": 0.07652501176101616,
      "grad_norm": 0.3873724043369293,
      "learning_rate": 0.0001972933541485402,
      "loss": 0.4123,
      "step": 610
    },
    {
      "epoch": 0.07715226595577858,
      "grad_norm": 0.4273001253604889,
      "learning_rate": 0.0001972475748294813,
      "loss": 0.5465,
      "step": 615
    },
    {
      "epoch": 0.077779520150541,
      "grad_norm": 0.4352057874202728,
      "learning_rate": 0.00019720141700330847,
      "loss": 0.4612,
      "step": 620
    },
    {
      "epoch": 0.07840677434530344,
      "grad_norm": 0.3953089714050293,
      "learning_rate": 0.00019715488084967727,
      "loss": 0.4686,
      "step": 625
    },
    {
      "epoch": 0.07903402854006586,
      "grad_norm": 0.45592987537384033,
      "learning_rate": 0.0001971079665497157,
      "loss": 0.434,
      "step": 630
    },
    {
      "epoch": 0.0796612827348283,
      "grad_norm": 0.3559194505214691,
      "learning_rate": 0.00019706067428602373,
      "loss": 0.3967,
      "step": 635
    },
    {
      "epoch": 0.08028853692959072,
      "grad_norm": 0.426921010017395,
      "learning_rate": 0.00019701300424267233,
      "loss": 0.4818,
      "step": 640
    },
    {
      "epoch": 0.08091579112435314,
      "grad_norm": 0.39503583312034607,
      "learning_rate": 0.00019696495660520284,
      "loss": 0.4504,
      "step": 645
    },
    {
      "epoch": 0.08154304531911558,
      "grad_norm": 0.4176393151283264,
      "learning_rate": 0.0001969165315606264,
      "loss": 0.4507,
      "step": 650
    },
    {
      "epoch": 0.082170299513878,
      "grad_norm": 0.4038068950176239,
      "learning_rate": 0.000196867729297423,
      "loss": 0.3906,
      "step": 655
    },
    {
      "epoch": 0.08279755370864042,
      "grad_norm": 0.44040727615356445,
      "learning_rate": 0.00019681855000554084,
      "loss": 0.4348,
      "step": 660
    },
    {
      "epoch": 0.08342480790340286,
      "grad_norm": 0.4055412709712982,
      "learning_rate": 0.00019676899387639565,
      "loss": 0.4387,
      "step": 665
    },
    {
      "epoch": 0.08405206209816528,
      "grad_norm": 0.4487919211387634,
      "learning_rate": 0.0001967190611028698,
      "loss": 0.5027,
      "step": 670
    },
    {
      "epoch": 0.08467931629292771,
      "grad_norm": 0.36683306097984314,
      "learning_rate": 0.00019666875187931168,
      "loss": 0.493,
      "step": 675
    },
    {
      "epoch": 0.08530657048769014,
      "grad_norm": 0.4030819833278656,
      "learning_rate": 0.00019661806640153498,
      "loss": 0.4805,
      "step": 680
    },
    {
      "epoch": 0.08593382468245256,
      "grad_norm": 0.39927220344543457,
      "learning_rate": 0.0001965670048668177,
      "loss": 0.456,
      "step": 685
    },
    {
      "epoch": 0.086561078877215,
      "grad_norm": 0.40194639563560486,
      "learning_rate": 0.0001965155674739016,
      "loss": 0.4714,
      "step": 690
    },
    {
      "epoch": 0.08718833307197742,
      "grad_norm": 0.46935105323791504,
      "learning_rate": 0.0001964637544229914,
      "loss": 0.5039,
      "step": 695
    },
    {
      "epoch": 0.08781558726673985,
      "grad_norm": 0.42496660351753235,
      "learning_rate": 0.0001964115659157539,
      "loss": 0.4395,
      "step": 700
    },
    {
      "epoch": 0.08844284146150228,
      "grad_norm": 0.472322016954422,
      "learning_rate": 0.00019635900215531722,
      "loss": 0.5186,
      "step": 705
    },
    {
      "epoch": 0.0890700956562647,
      "grad_norm": 0.37893420457839966,
      "learning_rate": 0.0001963060633462701,
      "loss": 0.4523,
      "step": 710
    },
    {
      "epoch": 0.08969734985102713,
      "grad_norm": 0.3737439811229706,
      "learning_rate": 0.00019625274969466106,
      "loss": 0.4803,
      "step": 715
    },
    {
      "epoch": 0.09032460404578956,
      "grad_norm": 0.5093035697937012,
      "learning_rate": 0.00019619906140799757,
      "loss": 0.4569,
      "step": 720
    },
    {
      "epoch": 0.09095185824055198,
      "grad_norm": 0.40129947662353516,
      "learning_rate": 0.0001961449986952452,
      "loss": 0.4884,
      "step": 725
    },
    {
      "epoch": 0.09157911243531441,
      "grad_norm": 0.41048485040664673,
      "learning_rate": 0.00019609056176682685,
      "loss": 0.4346,
      "step": 730
    },
    {
      "epoch": 0.09220636663007684,
      "grad_norm": 0.44371986389160156,
      "learning_rate": 0.00019603575083462207,
      "loss": 0.4792,
      "step": 735
    },
    {
      "epoch": 0.09283362082483927,
      "grad_norm": 0.506888210773468,
      "learning_rate": 0.00019598056611196598,
      "loss": 0.4509,
      "step": 740
    },
    {
      "epoch": 0.0934608750196017,
      "grad_norm": 0.43010297417640686,
      "learning_rate": 0.00019592500781364866,
      "loss": 0.4432,
      "step": 745
    },
    {
      "epoch": 0.09408812921436412,
      "grad_norm": 0.42304643988609314,
      "learning_rate": 0.00019586907615591414,
      "loss": 0.449,
      "step": 750
    },
    {
      "epoch": 0.09471538340912655,
      "grad_norm": 0.4066172242164612,
      "learning_rate": 0.00019581277135645968,
      "loss": 0.47,
      "step": 755
    },
    {
      "epoch": 0.09534263760388897,
      "grad_norm": 0.41886577010154724,
      "learning_rate": 0.00019575609363443487,
      "loss": 0.4653,
      "step": 760
    },
    {
      "epoch": 0.0959698917986514,
      "grad_norm": 0.44804054498672485,
      "learning_rate": 0.00019569904321044087,
      "loss": 0.4299,
      "step": 765
    },
    {
      "epoch": 0.09659714599341383,
      "grad_norm": 0.4596683979034424,
      "learning_rate": 0.0001956416203065293,
      "loss": 0.4517,
      "step": 770
    },
    {
      "epoch": 0.09722440018817625,
      "grad_norm": 0.464821457862854,
      "learning_rate": 0.00019558382514620176,
      "loss": 0.4819,
      "step": 775
    },
    {
      "epoch": 0.09785165438293869,
      "grad_norm": 0.3574564754962921,
      "learning_rate": 0.0001955256579544085,
      "loss": 0.4505,
      "step": 780
    },
    {
      "epoch": 0.09847890857770111,
      "grad_norm": 0.4623837172985077,
      "learning_rate": 0.00019546711895754805,
      "loss": 0.4637,
      "step": 785
    },
    {
      "epoch": 0.09910616277246354,
      "grad_norm": 0.3662761151790619,
      "learning_rate": 0.00019540820838346586,
      "loss": 0.4467,
      "step": 790
    },
    {
      "epoch": 0.09973341696722597,
      "grad_norm": 0.4189079701900482,
      "learning_rate": 0.0001953489264614538,
      "loss": 0.4493,
      "step": 795
    },
    {
      "epoch": 0.1003606711619884,
      "grad_norm": 0.41791877150535583,
      "learning_rate": 0.000195289273422249,
      "loss": 0.4383,
      "step": 800
    },
    {
      "epoch": 0.10098792535675083,
      "grad_norm": 0.41431987285614014,
      "learning_rate": 0.0001952292494980331,
      "loss": 0.4541,
      "step": 805
    },
    {
      "epoch": 0.10161517955151325,
      "grad_norm": 0.4126748740673065,
      "learning_rate": 0.00019516885492243123,
      "loss": 0.4475,
      "step": 810
    },
    {
      "epoch": 0.10224243374627567,
      "grad_norm": 0.49894511699676514,
      "learning_rate": 0.0001951080899305113,
      "loss": 0.4433,
      "step": 815
    },
    {
      "epoch": 0.10286968794103811,
      "grad_norm": 0.31616851687431335,
      "learning_rate": 0.0001950469547587828,
      "loss": 0.448,
      "step": 820
    },
    {
      "epoch": 0.10349694213580053,
      "grad_norm": 0.35921424627304077,
      "learning_rate": 0.00019498544964519614,
      "loss": 0.453,
      "step": 825
    },
    {
      "epoch": 0.10412419633056295,
      "grad_norm": 0.41913530230522156,
      "learning_rate": 0.00019492357482914162,
      "loss": 0.4607,
      "step": 830
    },
    {
      "epoch": 0.10475145052532539,
      "grad_norm": 0.48450350761413574,
      "learning_rate": 0.00019486133055144839,
      "loss": 0.4665,
      "step": 835
    },
    {
      "epoch": 0.10537870472008781,
      "grad_norm": 0.39938056468963623,
      "learning_rate": 0.00019479871705438377,
      "loss": 0.4907,
      "step": 840
    },
    {
      "epoch": 0.10600595891485025,
      "grad_norm": 0.4627731144428253,
      "learning_rate": 0.00019473573458165207,
      "loss": 0.4373,
      "step": 845
    },
    {
      "epoch": 0.10663321310961267,
      "grad_norm": 0.38617271184921265,
      "learning_rate": 0.00019467238337839368,
      "loss": 0.4036,
      "step": 850
    }
  ],
  "logging_steps": 5,
  "max_steps": 7972,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6580311955770368e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
