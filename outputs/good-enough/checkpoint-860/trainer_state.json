{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.5994075622930825,
  "eval_steps": 500,
  "global_step": 860,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003484927687750479,
      "grad_norm": 2.6135387420654297,
      "learning_rate": 5.555555555555556e-06,
      "loss": 3.0267,
      "step": 5
    },
    {
      "epoch": 0.006969855375500958,
      "grad_norm": 2.7352917194366455,
      "learning_rate": 1.25e-05,
      "loss": 2.9068,
      "step": 10
    },
    {
      "epoch": 0.010454783063251438,
      "grad_norm": 2.312462329864502,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 2.4316,
      "step": 15
    },
    {
      "epoch": 0.013939710751001916,
      "grad_norm": 3.0282199382781982,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 1.8367,
      "step": 20
    },
    {
      "epoch": 0.017424638438752395,
      "grad_norm": 2.5456013679504395,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.075,
      "step": 25
    },
    {
      "epoch": 0.020909566126502875,
      "grad_norm": 0.841454267501831,
      "learning_rate": 4.027777777777778e-05,
      "loss": 0.7289,
      "step": 30
    },
    {
      "epoch": 0.024394493814253355,
      "grad_norm": 1.0773881673812866,
      "learning_rate": 4.722222222222222e-05,
      "loss": 0.6189,
      "step": 35
    },
    {
      "epoch": 0.027879421502003832,
      "grad_norm": 0.8775879144668579,
      "learning_rate": 5.4166666666666664e-05,
      "loss": 0.5508,
      "step": 40
    },
    {
      "epoch": 0.03136434918975431,
      "grad_norm": 1.3046302795410156,
      "learning_rate": 6.111111111111112e-05,
      "loss": 0.5593,
      "step": 45
    },
    {
      "epoch": 0.03484927687750479,
      "grad_norm": 0.5773324966430664,
      "learning_rate": 6.805555555555556e-05,
      "loss": 0.5311,
      "step": 50
    },
    {
      "epoch": 0.03833420456525527,
      "grad_norm": 0.6276559233665466,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.5142,
      "step": 55
    },
    {
      "epoch": 0.04181913225300575,
      "grad_norm": 0.5278290510177612,
      "learning_rate": 8.194444444444445e-05,
      "loss": 0.5197,
      "step": 60
    },
    {
      "epoch": 0.04530405994075623,
      "grad_norm": 0.5434977412223816,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.5217,
      "step": 65
    },
    {
      "epoch": 0.04878898762850671,
      "grad_norm": 0.4420661926269531,
      "learning_rate": 9.583333333333334e-05,
      "loss": 0.4584,
      "step": 70
    },
    {
      "epoch": 0.052273915316257184,
      "grad_norm": 0.5293088555335999,
      "learning_rate": 0.00010277777777777778,
      "loss": 0.5072,
      "step": 75
    },
    {
      "epoch": 0.055758843004007665,
      "grad_norm": 0.49954459071159363,
      "learning_rate": 0.00010972222222222224,
      "loss": 0.4714,
      "step": 80
    },
    {
      "epoch": 0.059243770691758145,
      "grad_norm": 0.5204141736030579,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.4724,
      "step": 85
    },
    {
      "epoch": 0.06272869837950862,
      "grad_norm": 0.49699679017066956,
      "learning_rate": 0.00012361111111111112,
      "loss": 0.4732,
      "step": 90
    },
    {
      "epoch": 0.0662136260672591,
      "grad_norm": 0.5141588449478149,
      "learning_rate": 0.00013055555555555555,
      "loss": 0.4733,
      "step": 95
    },
    {
      "epoch": 0.06969855375500958,
      "grad_norm": 0.3983529806137085,
      "learning_rate": 0.0001375,
      "loss": 0.4747,
      "step": 100
    },
    {
      "epoch": 0.07318348144276006,
      "grad_norm": 0.4677749276161194,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.4773,
      "step": 105
    },
    {
      "epoch": 0.07666840913051054,
      "grad_norm": 0.5097423195838928,
      "learning_rate": 0.0001513888888888889,
      "loss": 0.4679,
      "step": 110
    },
    {
      "epoch": 0.08015333681826102,
      "grad_norm": 0.5053136944770813,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.4669,
      "step": 115
    },
    {
      "epoch": 0.0836382645060115,
      "grad_norm": 0.44642335176467896,
      "learning_rate": 0.00016527777777777778,
      "loss": 0.4601,
      "step": 120
    },
    {
      "epoch": 0.08712319219376198,
      "grad_norm": 0.4479033946990967,
      "learning_rate": 0.00017222222222222224,
      "loss": 0.5024,
      "step": 125
    },
    {
      "epoch": 0.09060811988151246,
      "grad_norm": 0.42405033111572266,
      "learning_rate": 0.0001791666666666667,
      "loss": 0.4206,
      "step": 130
    },
    {
      "epoch": 0.09409304756926294,
      "grad_norm": 0.50856614112854,
      "learning_rate": 0.00018611111111111112,
      "loss": 0.5194,
      "step": 135
    },
    {
      "epoch": 0.09757797525701342,
      "grad_norm": 0.46089211106300354,
      "learning_rate": 0.00019305555555555558,
      "loss": 0.4703,
      "step": 140
    },
    {
      "epoch": 0.1010629029447639,
      "grad_norm": 0.4220588505268097,
      "learning_rate": 0.0002,
      "loss": 0.4348,
      "step": 145
    },
    {
      "epoch": 0.10454783063251437,
      "grad_norm": 0.5276545286178589,
      "learning_rate": 0.0001992254066615027,
      "loss": 0.4695,
      "step": 150
    },
    {
      "epoch": 0.10803275832026485,
      "grad_norm": 0.4333854019641876,
      "learning_rate": 0.00019845081332300543,
      "loss": 0.4771,
      "step": 155
    },
    {
      "epoch": 0.11151768600801533,
      "grad_norm": 0.42625126242637634,
      "learning_rate": 0.00019767621998450813,
      "loss": 0.4965,
      "step": 160
    },
    {
      "epoch": 0.11500261369576581,
      "grad_norm": 0.37970227003097534,
      "learning_rate": 0.00019690162664601086,
      "loss": 0.4591,
      "step": 165
    },
    {
      "epoch": 0.11848754138351629,
      "grad_norm": 0.372379332780838,
      "learning_rate": 0.00019612703330751355,
      "loss": 0.4729,
      "step": 170
    },
    {
      "epoch": 0.12197246907126677,
      "grad_norm": 0.39838966727256775,
      "learning_rate": 0.00019535243996901625,
      "loss": 0.4642,
      "step": 175
    },
    {
      "epoch": 0.12545739675901724,
      "grad_norm": 0.4896467626094818,
      "learning_rate": 0.00019457784663051898,
      "loss": 0.4435,
      "step": 180
    },
    {
      "epoch": 0.12894232444676773,
      "grad_norm": 0.4603949189186096,
      "learning_rate": 0.0001938032532920217,
      "loss": 0.4785,
      "step": 185
    },
    {
      "epoch": 0.1324272521345182,
      "grad_norm": 0.475354939699173,
      "learning_rate": 0.00019302865995352443,
      "loss": 0.4982,
      "step": 190
    },
    {
      "epoch": 0.1359121798222687,
      "grad_norm": 0.35663145780563354,
      "learning_rate": 0.00019225406661502713,
      "loss": 0.406,
      "step": 195
    },
    {
      "epoch": 0.13939710751001916,
      "grad_norm": 0.391207754611969,
      "learning_rate": 0.00019147947327652985,
      "loss": 0.4784,
      "step": 200
    },
    {
      "epoch": 0.14288203519776965,
      "grad_norm": 0.5269039869308472,
      "learning_rate": 0.00019070487993803255,
      "loss": 0.4326,
      "step": 205
    },
    {
      "epoch": 0.14636696288552012,
      "grad_norm": 0.395664781332016,
      "learning_rate": 0.00018993028659953525,
      "loss": 0.4642,
      "step": 210
    },
    {
      "epoch": 0.1498518905732706,
      "grad_norm": 0.367127001285553,
      "learning_rate": 0.00018915569326103797,
      "loss": 0.4298,
      "step": 215
    },
    {
      "epoch": 0.15333681826102108,
      "grad_norm": 0.3555621802806854,
      "learning_rate": 0.00018838109992254067,
      "loss": 0.4521,
      "step": 220
    },
    {
      "epoch": 0.15682174594877157,
      "grad_norm": 0.3737030625343323,
      "learning_rate": 0.0001876065065840434,
      "loss": 0.4478,
      "step": 225
    },
    {
      "epoch": 0.16030667363652204,
      "grad_norm": 0.43997722864151,
      "learning_rate": 0.0001868319132455461,
      "loss": 0.4226,
      "step": 230
    },
    {
      "epoch": 0.16379160132427253,
      "grad_norm": 0.46257084608078003,
      "learning_rate": 0.0001860573199070488,
      "loss": 0.4665,
      "step": 235
    },
    {
      "epoch": 0.167276529012023,
      "grad_norm": 0.3807833194732666,
      "learning_rate": 0.00018528272656855152,
      "loss": 0.4692,
      "step": 240
    },
    {
      "epoch": 0.17076145669977347,
      "grad_norm": 0.39236095547676086,
      "learning_rate": 0.00018450813323005422,
      "loss": 0.4677,
      "step": 245
    },
    {
      "epoch": 0.17424638438752396,
      "grad_norm": 0.4534180164337158,
      "learning_rate": 0.00018373353989155694,
      "loss": 0.4664,
      "step": 250
    },
    {
      "epoch": 0.17773131207527443,
      "grad_norm": 0.42144837975502014,
      "learning_rate": 0.00018295894655305964,
      "loss": 0.4619,
      "step": 255
    },
    {
      "epoch": 0.18121623976302492,
      "grad_norm": 0.4293971061706543,
      "learning_rate": 0.00018218435321456236,
      "loss": 0.4945,
      "step": 260
    },
    {
      "epoch": 0.1847011674507754,
      "grad_norm": 0.41162174940109253,
      "learning_rate": 0.0001814097598760651,
      "loss": 0.4494,
      "step": 265
    },
    {
      "epoch": 0.18818609513852588,
      "grad_norm": 0.45597973465919495,
      "learning_rate": 0.0001806351665375678,
      "loss": 0.5053,
      "step": 270
    },
    {
      "epoch": 0.19167102282627635,
      "grad_norm": 0.5994377732276917,
      "learning_rate": 0.0001798605731990705,
      "loss": 0.4901,
      "step": 275
    },
    {
      "epoch": 0.19515595051402684,
      "grad_norm": 0.41234976053237915,
      "learning_rate": 0.0001790859798605732,
      "loss": 0.4854,
      "step": 280
    },
    {
      "epoch": 0.1986408782017773,
      "grad_norm": 0.45196929574012756,
      "learning_rate": 0.00017831138652207594,
      "loss": 0.4332,
      "step": 285
    },
    {
      "epoch": 0.2021258058895278,
      "grad_norm": 0.4684348404407501,
      "learning_rate": 0.00017753679318357863,
      "loss": 0.4616,
      "step": 290
    },
    {
      "epoch": 0.20561073357727827,
      "grad_norm": 0.409650593996048,
      "learning_rate": 0.00017676219984508133,
      "loss": 0.4397,
      "step": 295
    },
    {
      "epoch": 0.20909566126502874,
      "grad_norm": 0.37629234790802,
      "learning_rate": 0.00017598760650658406,
      "loss": 0.4225,
      "step": 300
    },
    {
      "epoch": 0.21258058895277923,
      "grad_norm": 0.42076054215431213,
      "learning_rate": 0.00017521301316808676,
      "loss": 0.4495,
      "step": 305
    },
    {
      "epoch": 0.2160655166405297,
      "grad_norm": 0.38984978199005127,
      "learning_rate": 0.00017443841982958948,
      "loss": 0.4583,
      "step": 310
    },
    {
      "epoch": 0.2195504443282802,
      "grad_norm": 0.4552024006843567,
      "learning_rate": 0.00017366382649109218,
      "loss": 0.4361,
      "step": 315
    },
    {
      "epoch": 0.22303537201603066,
      "grad_norm": 0.4601477384567261,
      "learning_rate": 0.00017288923315259488,
      "loss": 0.5089,
      "step": 320
    },
    {
      "epoch": 0.22652029970378115,
      "grad_norm": 0.4242843687534332,
      "learning_rate": 0.0001721146398140976,
      "loss": 0.4473,
      "step": 325
    },
    {
      "epoch": 0.23000522739153162,
      "grad_norm": 0.45843207836151123,
      "learning_rate": 0.0001713400464756003,
      "loss": 0.4407,
      "step": 330
    },
    {
      "epoch": 0.2334901550792821,
      "grad_norm": 0.3724101781845093,
      "learning_rate": 0.00017056545313710303,
      "loss": 0.4382,
      "step": 335
    },
    {
      "epoch": 0.23697508276703258,
      "grad_norm": 0.41750600934028625,
      "learning_rate": 0.00016979085979860575,
      "loss": 0.4347,
      "step": 340
    },
    {
      "epoch": 0.24046001045478307,
      "grad_norm": 0.4946185052394867,
      "learning_rate": 0.00016901626646010845,
      "loss": 0.4844,
      "step": 345
    },
    {
      "epoch": 0.24394493814253354,
      "grad_norm": 0.38579437136650085,
      "learning_rate": 0.00016824167312161117,
      "loss": 0.4364,
      "step": 350
    },
    {
      "epoch": 0.24742986583028403,
      "grad_norm": 0.3891845941543579,
      "learning_rate": 0.00016746707978311387,
      "loss": 0.4479,
      "step": 355
    },
    {
      "epoch": 0.2509147935180345,
      "grad_norm": 0.3995891511440277,
      "learning_rate": 0.0001666924864446166,
      "loss": 0.4531,
      "step": 360
    },
    {
      "epoch": 0.25439972120578497,
      "grad_norm": 0.3613477647304535,
      "learning_rate": 0.0001659178931061193,
      "loss": 0.4444,
      "step": 365
    },
    {
      "epoch": 0.25788464889353546,
      "grad_norm": 0.4038073718547821,
      "learning_rate": 0.00016514329976762202,
      "loss": 0.4535,
      "step": 370
    },
    {
      "epoch": 0.26136957658128596,
      "grad_norm": 0.5262879133224487,
      "learning_rate": 0.00016436870642912472,
      "loss": 0.4751,
      "step": 375
    },
    {
      "epoch": 0.2648545042690364,
      "grad_norm": 0.4304600954055786,
      "learning_rate": 0.00016359411309062742,
      "loss": 0.4411,
      "step": 380
    },
    {
      "epoch": 0.2683394319567869,
      "grad_norm": 0.4175858497619629,
      "learning_rate": 0.00016281951975213014,
      "loss": 0.4503,
      "step": 385
    },
    {
      "epoch": 0.2718243596445374,
      "grad_norm": 0.44179007411003113,
      "learning_rate": 0.00016204492641363284,
      "loss": 0.4344,
      "step": 390
    },
    {
      "epoch": 0.2753092873322879,
      "grad_norm": 0.3987584710121155,
      "learning_rate": 0.00016127033307513557,
      "loss": 0.4119,
      "step": 395
    },
    {
      "epoch": 0.2787942150200383,
      "grad_norm": 0.40908676385879517,
      "learning_rate": 0.00016049573973663826,
      "loss": 0.4098,
      "step": 400
    },
    {
      "epoch": 0.2822791427077888,
      "grad_norm": 0.4312569499015808,
      "learning_rate": 0.00015972114639814096,
      "loss": 0.477,
      "step": 405
    },
    {
      "epoch": 0.2857640703955393,
      "grad_norm": 0.4477165639400482,
      "learning_rate": 0.0001589465530596437,
      "loss": 0.4611,
      "step": 410
    },
    {
      "epoch": 0.2892489980832898,
      "grad_norm": 0.4382442533969879,
      "learning_rate": 0.0001581719597211464,
      "loss": 0.487,
      "step": 415
    },
    {
      "epoch": 0.29273392577104024,
      "grad_norm": 0.3593069016933441,
      "learning_rate": 0.00015739736638264914,
      "loss": 0.4661,
      "step": 420
    },
    {
      "epoch": 0.29621885345879073,
      "grad_norm": 0.4405742585659027,
      "learning_rate": 0.00015662277304415184,
      "loss": 0.4502,
      "step": 425
    },
    {
      "epoch": 0.2997037811465412,
      "grad_norm": 0.45539146661758423,
      "learning_rate": 0.00015584817970565453,
      "loss": 0.445,
      "step": 430
    },
    {
      "epoch": 0.30318870883429166,
      "grad_norm": 0.48512405157089233,
      "learning_rate": 0.00015507358636715726,
      "loss": 0.4341,
      "step": 435
    },
    {
      "epoch": 0.30667363652204216,
      "grad_norm": 0.431877464056015,
      "learning_rate": 0.00015429899302865996,
      "loss": 0.4116,
      "step": 440
    },
    {
      "epoch": 0.31015856420979265,
      "grad_norm": 0.4377971291542053,
      "learning_rate": 0.00015352439969016268,
      "loss": 0.4472,
      "step": 445
    },
    {
      "epoch": 0.31364349189754315,
      "grad_norm": 0.4802955090999603,
      "learning_rate": 0.00015274980635166538,
      "loss": 0.4478,
      "step": 450
    },
    {
      "epoch": 0.3171284195852936,
      "grad_norm": 0.4213622510433197,
      "learning_rate": 0.0001519752130131681,
      "loss": 0.4282,
      "step": 455
    },
    {
      "epoch": 0.3206133472730441,
      "grad_norm": 0.3852848410606384,
      "learning_rate": 0.0001512006196746708,
      "loss": 0.3883,
      "step": 460
    },
    {
      "epoch": 0.3240982749607946,
      "grad_norm": 0.39065030217170715,
      "learning_rate": 0.0001504260263361735,
      "loss": 0.4267,
      "step": 465
    },
    {
      "epoch": 0.32758320264854507,
      "grad_norm": 0.3956261873245239,
      "learning_rate": 0.00014965143299767623,
      "loss": 0.4387,
      "step": 470
    },
    {
      "epoch": 0.3310681303362955,
      "grad_norm": 0.4563988745212555,
      "learning_rate": 0.00014887683965917893,
      "loss": 0.4387,
      "step": 475
    },
    {
      "epoch": 0.334553058024046,
      "grad_norm": 0.5253580212593079,
      "learning_rate": 0.00014810224632068165,
      "loss": 0.4369,
      "step": 480
    },
    {
      "epoch": 0.3380379857117965,
      "grad_norm": 0.4161703288555145,
      "learning_rate": 0.00014732765298218435,
      "loss": 0.4061,
      "step": 485
    },
    {
      "epoch": 0.34152291339954693,
      "grad_norm": 0.3512125313282013,
      "learning_rate": 0.00014655305964368707,
      "loss": 0.4246,
      "step": 490
    },
    {
      "epoch": 0.34500784108729743,
      "grad_norm": 0.3683735430240631,
      "learning_rate": 0.0001457784663051898,
      "loss": 0.4252,
      "step": 495
    },
    {
      "epoch": 0.3484927687750479,
      "grad_norm": 0.40833795070648193,
      "learning_rate": 0.0001450038729666925,
      "loss": 0.4254,
      "step": 500
    },
    {
      "epoch": 0.3519776964627984,
      "grad_norm": 0.4552725553512573,
      "learning_rate": 0.00014422927962819522,
      "loss": 0.4318,
      "step": 505
    },
    {
      "epoch": 0.35546262415054886,
      "grad_norm": 0.4341769516468048,
      "learning_rate": 0.00014345468628969792,
      "loss": 0.4509,
      "step": 510
    },
    {
      "epoch": 0.35894755183829935,
      "grad_norm": 0.4688941240310669,
      "learning_rate": 0.00014268009295120062,
      "loss": 0.4499,
      "step": 515
    },
    {
      "epoch": 0.36243247952604984,
      "grad_norm": 0.383304238319397,
      "learning_rate": 0.00014190549961270334,
      "loss": 0.4078,
      "step": 520
    },
    {
      "epoch": 0.36591740721380034,
      "grad_norm": 0.40450572967529297,
      "learning_rate": 0.00014113090627420604,
      "loss": 0.4681,
      "step": 525
    },
    {
      "epoch": 0.3694023349015508,
      "grad_norm": 0.4577934145927429,
      "learning_rate": 0.00014035631293570877,
      "loss": 0.48,
      "step": 530
    },
    {
      "epoch": 0.37288726258930127,
      "grad_norm": 0.38382217288017273,
      "learning_rate": 0.00013958171959721147,
      "loss": 0.4314,
      "step": 535
    },
    {
      "epoch": 0.37637219027705177,
      "grad_norm": 0.38553565740585327,
      "learning_rate": 0.00013880712625871416,
      "loss": 0.427,
      "step": 540
    },
    {
      "epoch": 0.3798571179648022,
      "grad_norm": 0.45626574754714966,
      "learning_rate": 0.0001380325329202169,
      "loss": 0.4768,
      "step": 545
    },
    {
      "epoch": 0.3833420456525527,
      "grad_norm": 0.4262070953845978,
      "learning_rate": 0.0001372579395817196,
      "loss": 0.4098,
      "step": 550
    },
    {
      "epoch": 0.3868269733403032,
      "grad_norm": 0.42798352241516113,
      "learning_rate": 0.0001364833462432223,
      "loss": 0.3942,
      "step": 555
    },
    {
      "epoch": 0.3903119010280537,
      "grad_norm": 0.4515320062637329,
      "learning_rate": 0.000135708752904725,
      "loss": 0.4813,
      "step": 560
    },
    {
      "epoch": 0.3937968287158041,
      "grad_norm": 0.4193539619445801,
      "learning_rate": 0.00013493415956622774,
      "loss": 0.4277,
      "step": 565
    },
    {
      "epoch": 0.3972817564035546,
      "grad_norm": 0.3861646354198456,
      "learning_rate": 0.00013415956622773046,
      "loss": 0.4541,
      "step": 570
    },
    {
      "epoch": 0.4007666840913051,
      "grad_norm": 0.41434556245803833,
      "learning_rate": 0.00013338497288923316,
      "loss": 0.4437,
      "step": 575
    },
    {
      "epoch": 0.4042516117790556,
      "grad_norm": 0.450277179479599,
      "learning_rate": 0.00013261037955073589,
      "loss": 0.453,
      "step": 580
    },
    {
      "epoch": 0.40773653946680605,
      "grad_norm": 0.41143998503685,
      "learning_rate": 0.00013183578621223858,
      "loss": 0.4371,
      "step": 585
    },
    {
      "epoch": 0.41122146715455654,
      "grad_norm": 0.39580756425857544,
      "learning_rate": 0.0001310611928737413,
      "loss": 0.4508,
      "step": 590
    },
    {
      "epoch": 0.41470639484230704,
      "grad_norm": 0.37452322244644165,
      "learning_rate": 0.000130286599535244,
      "loss": 0.399,
      "step": 595
    },
    {
      "epoch": 0.4181913225300575,
      "grad_norm": 0.3878813087940216,
      "learning_rate": 0.0001295120061967467,
      "loss": 0.4019,
      "step": 600
    },
    {
      "epoch": 0.42167625021780797,
      "grad_norm": 0.40777429938316345,
      "learning_rate": 0.00012873741285824943,
      "loss": 0.4095,
      "step": 605
    },
    {
      "epoch": 0.42516117790555846,
      "grad_norm": 0.47260943055152893,
      "learning_rate": 0.00012796281951975213,
      "loss": 0.4521,
      "step": 610
    },
    {
      "epoch": 0.42864610559330896,
      "grad_norm": 0.42897409200668335,
      "learning_rate": 0.00012718822618125485,
      "loss": 0.4185,
      "step": 615
    },
    {
      "epoch": 0.4321310332810594,
      "grad_norm": 0.4253702759742737,
      "learning_rate": 0.00012641363284275755,
      "loss": 0.4476,
      "step": 620
    },
    {
      "epoch": 0.4356159609688099,
      "grad_norm": 0.4692927300930023,
      "learning_rate": 0.00012563903950426025,
      "loss": 0.4612,
      "step": 625
    },
    {
      "epoch": 0.4391008886565604,
      "grad_norm": 0.46618232131004333,
      "learning_rate": 0.00012486444616576297,
      "loss": 0.4591,
      "step": 630
    },
    {
      "epoch": 0.4425858163443109,
      "grad_norm": 0.44145265221595764,
      "learning_rate": 0.00012408985282726567,
      "loss": 0.4204,
      "step": 635
    },
    {
      "epoch": 0.4460707440320613,
      "grad_norm": 0.40518414974212646,
      "learning_rate": 0.0001233152594887684,
      "loss": 0.4197,
      "step": 640
    },
    {
      "epoch": 0.4495556717198118,
      "grad_norm": 0.3878219723701477,
      "learning_rate": 0.00012254066615027112,
      "loss": 0.4005,
      "step": 645
    },
    {
      "epoch": 0.4530405994075623,
      "grad_norm": 0.3995022475719452,
      "learning_rate": 0.00012176607281177384,
      "loss": 0.3846,
      "step": 650
    },
    {
      "epoch": 0.4565255270953128,
      "grad_norm": 0.41106143593788147,
      "learning_rate": 0.00012099147947327653,
      "loss": 0.4383,
      "step": 655
    },
    {
      "epoch": 0.46001045478306324,
      "grad_norm": 0.41408494114875793,
      "learning_rate": 0.00012021688613477924,
      "loss": 0.4287,
      "step": 660
    },
    {
      "epoch": 0.46349538247081373,
      "grad_norm": 0.39672747254371643,
      "learning_rate": 0.00011944229279628196,
      "loss": 0.4482,
      "step": 665
    },
    {
      "epoch": 0.4669803101585642,
      "grad_norm": 0.3952828645706177,
      "learning_rate": 0.00011866769945778467,
      "loss": 0.4123,
      "step": 670
    },
    {
      "epoch": 0.47046523784631467,
      "grad_norm": 0.4201192557811737,
      "learning_rate": 0.0001178931061192874,
      "loss": 0.3956,
      "step": 675
    },
    {
      "epoch": 0.47395016553406516,
      "grad_norm": 0.362983375787735,
      "learning_rate": 0.00011711851278079009,
      "loss": 0.4386,
      "step": 680
    },
    {
      "epoch": 0.47743509322181565,
      "grad_norm": 0.4735147953033447,
      "learning_rate": 0.00011634391944229279,
      "loss": 0.4406,
      "step": 685
    },
    {
      "epoch": 0.48092002090956615,
      "grad_norm": 0.39701223373413086,
      "learning_rate": 0.00011556932610379551,
      "loss": 0.4128,
      "step": 690
    },
    {
      "epoch": 0.4844049485973166,
      "grad_norm": 0.42219278216362,
      "learning_rate": 0.00011479473276529821,
      "loss": 0.4091,
      "step": 695
    },
    {
      "epoch": 0.4878898762850671,
      "grad_norm": 0.3955274820327759,
      "learning_rate": 0.00011402013942680094,
      "loss": 0.4375,
      "step": 700
    },
    {
      "epoch": 0.4913748039728176,
      "grad_norm": 0.42184948921203613,
      "learning_rate": 0.00011324554608830365,
      "loss": 0.4159,
      "step": 705
    },
    {
      "epoch": 0.49485973166056807,
      "grad_norm": 0.4378323554992676,
      "learning_rate": 0.00011247095274980635,
      "loss": 0.4426,
      "step": 710
    },
    {
      "epoch": 0.4983446593483185,
      "grad_norm": 0.4183051288127899,
      "learning_rate": 0.00011169635941130907,
      "loss": 0.4211,
      "step": 715
    },
    {
      "epoch": 0.501829587036069,
      "grad_norm": 0.4302607774734497,
      "learning_rate": 0.00011092176607281177,
      "loss": 0.4406,
      "step": 720
    },
    {
      "epoch": 0.5053145147238195,
      "grad_norm": 0.44472450017929077,
      "learning_rate": 0.0001101471727343145,
      "loss": 0.4253,
      "step": 725
    },
    {
      "epoch": 0.5087994424115699,
      "grad_norm": 0.43735837936401367,
      "learning_rate": 0.0001093725793958172,
      "loss": 0.436,
      "step": 730
    },
    {
      "epoch": 0.5122843700993205,
      "grad_norm": 0.4099596440792084,
      "learning_rate": 0.00010859798605731992,
      "loss": 0.4601,
      "step": 735
    },
    {
      "epoch": 0.5157692977870709,
      "grad_norm": 0.36903154850006104,
      "learning_rate": 0.00010782339271882262,
      "loss": 0.3599,
      "step": 740
    },
    {
      "epoch": 0.5192542254748214,
      "grad_norm": 0.41885894536972046,
      "learning_rate": 0.00010704879938032533,
      "loss": 0.423,
      "step": 745
    },
    {
      "epoch": 0.5227391531625719,
      "grad_norm": 0.3674814701080322,
      "learning_rate": 0.00010627420604182806,
      "loss": 0.4129,
      "step": 750
    },
    {
      "epoch": 0.5262240808503224,
      "grad_norm": 0.3489435911178589,
      "learning_rate": 0.00010549961270333075,
      "loss": 0.4023,
      "step": 755
    },
    {
      "epoch": 0.5297090085380728,
      "grad_norm": 0.374277800321579,
      "learning_rate": 0.00010472501936483348,
      "loss": 0.4065,
      "step": 760
    },
    {
      "epoch": 0.5331939362258233,
      "grad_norm": 0.4082717001438141,
      "learning_rate": 0.00010395042602633618,
      "loss": 0.4139,
      "step": 765
    },
    {
      "epoch": 0.5366788639135738,
      "grad_norm": 0.4251544773578644,
      "learning_rate": 0.00010317583268783887,
      "loss": 0.4261,
      "step": 770
    },
    {
      "epoch": 0.5401637916013243,
      "grad_norm": 0.38464346528053284,
      "learning_rate": 0.0001024012393493416,
      "loss": 0.4001,
      "step": 775
    },
    {
      "epoch": 0.5436487192890748,
      "grad_norm": 0.40550750494003296,
      "learning_rate": 0.00010162664601084431,
      "loss": 0.4298,
      "step": 780
    },
    {
      "epoch": 0.5471336469768252,
      "grad_norm": 0.4346456527709961,
      "learning_rate": 0.00010085205267234704,
      "loss": 0.4032,
      "step": 785
    },
    {
      "epoch": 0.5506185746645758,
      "grad_norm": 0.4123024046421051,
      "learning_rate": 0.00010007745933384973,
      "loss": 0.4395,
      "step": 790
    },
    {
      "epoch": 0.5541035023523262,
      "grad_norm": 0.4838726818561554,
      "learning_rate": 9.930286599535245e-05,
      "loss": 0.4178,
      "step": 795
    },
    {
      "epoch": 0.5575884300400766,
      "grad_norm": 0.3837737441062927,
      "learning_rate": 9.852827265685516e-05,
      "loss": 0.4197,
      "step": 800
    },
    {
      "epoch": 0.5610733577278272,
      "grad_norm": 0.41552734375,
      "learning_rate": 9.775367931835787e-05,
      "loss": 0.4461,
      "step": 805
    },
    {
      "epoch": 0.5645582854155776,
      "grad_norm": 0.45105284452438354,
      "learning_rate": 9.697908597986057e-05,
      "loss": 0.4905,
      "step": 810
    },
    {
      "epoch": 0.5680432131033281,
      "grad_norm": 0.43703973293304443,
      "learning_rate": 9.620449264136328e-05,
      "loss": 0.4264,
      "step": 815
    },
    {
      "epoch": 0.5715281407910786,
      "grad_norm": 0.418729305267334,
      "learning_rate": 9.5429899302866e-05,
      "loss": 0.3814,
      "step": 820
    },
    {
      "epoch": 0.575013068478829,
      "grad_norm": 0.41087082028388977,
      "learning_rate": 9.465530596436872e-05,
      "loss": 0.4066,
      "step": 825
    },
    {
      "epoch": 0.5784979961665796,
      "grad_norm": 0.40326786041259766,
      "learning_rate": 9.388071262587143e-05,
      "loss": 0.4039,
      "step": 830
    },
    {
      "epoch": 0.58198292385433,
      "grad_norm": 0.3797869086265564,
      "learning_rate": 9.310611928737414e-05,
      "loss": 0.3867,
      "step": 835
    },
    {
      "epoch": 0.5854678515420805,
      "grad_norm": 0.3652918040752411,
      "learning_rate": 9.233152594887684e-05,
      "loss": 0.3923,
      "step": 840
    },
    {
      "epoch": 0.588952779229831,
      "grad_norm": 0.4393939971923828,
      "learning_rate": 9.155693261037955e-05,
      "loss": 0.4127,
      "step": 845
    },
    {
      "epoch": 0.5924377069175815,
      "grad_norm": 0.4717440605163574,
      "learning_rate": 9.078233927188226e-05,
      "loss": 0.4491,
      "step": 850
    },
    {
      "epoch": 0.5959226346053319,
      "grad_norm": 0.41980859637260437,
      "learning_rate": 9.000774593338497e-05,
      "loss": 0.4098,
      "step": 855
    },
    {
      "epoch": 0.5994075622930825,
      "grad_norm": 0.49591293931007385,
      "learning_rate": 8.92331525948877e-05,
      "loss": 0.3852,
      "step": 860
    }
  ],
  "logging_steps": 5,
  "max_steps": 1435,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.889154515284992e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
