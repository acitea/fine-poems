{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9835037320454117,
  "eval_steps": 500,
  "global_step": 3920,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0012544690459762905,
      "grad_norm": 2.977828025817871,
      "learning_rate": 2.0050125313283207e-06,
      "loss": 3.6058,
      "step": 5
    },
    {
      "epoch": 0.002508938091952581,
      "grad_norm": 4.12176513671875,
      "learning_rate": 4.511278195488722e-06,
      "loss": 3.9359,
      "step": 10
    },
    {
      "epoch": 0.0037634071379288717,
      "grad_norm": 3.5963032245635986,
      "learning_rate": 7.017543859649123e-06,
      "loss": 3.6852,
      "step": 15
    },
    {
      "epoch": 0.005017876183905162,
      "grad_norm": 3.4053730964660645,
      "learning_rate": 9.523809523809523e-06,
      "loss": 3.6854,
      "step": 20
    },
    {
      "epoch": 0.006272345229881453,
      "grad_norm": 1.9819560050964355,
      "learning_rate": 1.2030075187969925e-05,
      "loss": 3.5422,
      "step": 25
    },
    {
      "epoch": 0.0075268142758577435,
      "grad_norm": 2.0557074546813965,
      "learning_rate": 1.4536340852130325e-05,
      "loss": 3.1301,
      "step": 30
    },
    {
      "epoch": 0.008781283321834034,
      "grad_norm": 2.639983892440796,
      "learning_rate": 1.7042606516290727e-05,
      "loss": 2.8855,
      "step": 35
    },
    {
      "epoch": 0.010035752367810324,
      "grad_norm": 2.716599225997925,
      "learning_rate": 1.954887218045113e-05,
      "loss": 2.6694,
      "step": 40
    },
    {
      "epoch": 0.011290221413786616,
      "grad_norm": 2.1144537925720215,
      "learning_rate": 2.205513784461153e-05,
      "loss": 2.3742,
      "step": 45
    },
    {
      "epoch": 0.012544690459762906,
      "grad_norm": 1.992550015449524,
      "learning_rate": 2.456140350877193e-05,
      "loss": 2.1769,
      "step": 50
    },
    {
      "epoch": 0.013799159505739195,
      "grad_norm": 2.157470703125,
      "learning_rate": 2.706766917293233e-05,
      "loss": 2.1194,
      "step": 55
    },
    {
      "epoch": 0.015053628551715487,
      "grad_norm": 2.2851340770721436,
      "learning_rate": 2.9573934837092732e-05,
      "loss": 2.1409,
      "step": 60
    },
    {
      "epoch": 0.01630809759769178,
      "grad_norm": 1.9949098825454712,
      "learning_rate": 3.208020050125313e-05,
      "loss": 1.992,
      "step": 65
    },
    {
      "epoch": 0.01756256664366807,
      "grad_norm": 2.34540057182312,
      "learning_rate": 3.458646616541353e-05,
      "loss": 2.0704,
      "step": 70
    },
    {
      "epoch": 0.01881703568964436,
      "grad_norm": 1.9954653978347778,
      "learning_rate": 3.7092731829573934e-05,
      "loss": 1.9834,
      "step": 75
    },
    {
      "epoch": 0.020071504735620648,
      "grad_norm": 1.947283148765564,
      "learning_rate": 3.9598997493734336e-05,
      "loss": 1.943,
      "step": 80
    },
    {
      "epoch": 0.021325973781596938,
      "grad_norm": 2.2097866535186768,
      "learning_rate": 4.210526315789474e-05,
      "loss": 1.9963,
      "step": 85
    },
    {
      "epoch": 0.02258044282757323,
      "grad_norm": 1.9591258764266968,
      "learning_rate": 4.461152882205514e-05,
      "loss": 1.8799,
      "step": 90
    },
    {
      "epoch": 0.02383491187354952,
      "grad_norm": 1.9924030303955078,
      "learning_rate": 4.711779448621554e-05,
      "loss": 1.8693,
      "step": 95
    },
    {
      "epoch": 0.02508938091952581,
      "grad_norm": 1.8988404273986816,
      "learning_rate": 4.9624060150375936e-05,
      "loss": 1.819,
      "step": 100
    },
    {
      "epoch": 0.0263438499655021,
      "grad_norm": 1.8459333181381226,
      "learning_rate": 5.213032581453634e-05,
      "loss": 1.8676,
      "step": 105
    },
    {
      "epoch": 0.02759831901147839,
      "grad_norm": 1.8491073846817017,
      "learning_rate": 5.463659147869674e-05,
      "loss": 1.9216,
      "step": 110
    },
    {
      "epoch": 0.02885278805745468,
      "grad_norm": 2.4000158309936523,
      "learning_rate": 5.714285714285714e-05,
      "loss": 1.9378,
      "step": 115
    },
    {
      "epoch": 0.030107257103430974,
      "grad_norm": 1.8039873838424683,
      "learning_rate": 5.9649122807017544e-05,
      "loss": 1.7309,
      "step": 120
    },
    {
      "epoch": 0.031361726149407264,
      "grad_norm": 1.5556293725967407,
      "learning_rate": 6.215538847117795e-05,
      "loss": 1.8286,
      "step": 125
    },
    {
      "epoch": 0.03261619519538356,
      "grad_norm": 1.9234046936035156,
      "learning_rate": 6.466165413533834e-05,
      "loss": 1.844,
      "step": 130
    },
    {
      "epoch": 0.033870664241359844,
      "grad_norm": 1.9804606437683105,
      "learning_rate": 6.716791979949875e-05,
      "loss": 1.7649,
      "step": 135
    },
    {
      "epoch": 0.03512513328733614,
      "grad_norm": 1.7566927671432495,
      "learning_rate": 6.967418546365914e-05,
      "loss": 1.8214,
      "step": 140
    },
    {
      "epoch": 0.03637960233331242,
      "grad_norm": 1.7425764799118042,
      "learning_rate": 7.218045112781955e-05,
      "loss": 1.8442,
      "step": 145
    },
    {
      "epoch": 0.03763407137928872,
      "grad_norm": 1.6275922060012817,
      "learning_rate": 7.468671679197995e-05,
      "loss": 1.7949,
      "step": 150
    },
    {
      "epoch": 0.03888854042526501,
      "grad_norm": 1.7570691108703613,
      "learning_rate": 7.719298245614036e-05,
      "loss": 1.8573,
      "step": 155
    },
    {
      "epoch": 0.040143009471241296,
      "grad_norm": 1.545183539390564,
      "learning_rate": 7.969924812030075e-05,
      "loss": 1.7769,
      "step": 160
    },
    {
      "epoch": 0.04139747851721759,
      "grad_norm": 1.8928053379058838,
      "learning_rate": 8.220551378446115e-05,
      "loss": 1.9094,
      "step": 165
    },
    {
      "epoch": 0.042651947563193876,
      "grad_norm": 1.7991971969604492,
      "learning_rate": 8.471177944862155e-05,
      "loss": 1.7352,
      "step": 170
    },
    {
      "epoch": 0.04390641660917017,
      "grad_norm": 1.6996684074401855,
      "learning_rate": 8.721804511278195e-05,
      "loss": 1.8291,
      "step": 175
    },
    {
      "epoch": 0.04516088565514646,
      "grad_norm": 1.4795244932174683,
      "learning_rate": 8.972431077694236e-05,
      "loss": 1.767,
      "step": 180
    },
    {
      "epoch": 0.04641535470112275,
      "grad_norm": 1.9160369634628296,
      "learning_rate": 9.223057644110275e-05,
      "loss": 1.7511,
      "step": 185
    },
    {
      "epoch": 0.04766982374709904,
      "grad_norm": 1.683709740638733,
      "learning_rate": 9.473684210526316e-05,
      "loss": 1.7178,
      "step": 190
    },
    {
      "epoch": 0.04892429279307533,
      "grad_norm": 1.8229721784591675,
      "learning_rate": 9.724310776942356e-05,
      "loss": 1.8372,
      "step": 195
    },
    {
      "epoch": 0.05017876183905162,
      "grad_norm": 1.5110870599746704,
      "learning_rate": 9.974937343358397e-05,
      "loss": 1.7641,
      "step": 200
    },
    {
      "epoch": 0.05143323088502791,
      "grad_norm": 1.6193989515304565,
      "learning_rate": 0.00010225563909774436,
      "loss": 1.7203,
      "step": 205
    },
    {
      "epoch": 0.0526876999310042,
      "grad_norm": 1.6602553129196167,
      "learning_rate": 0.00010476190476190477,
      "loss": 1.7436,
      "step": 210
    },
    {
      "epoch": 0.053942168976980495,
      "grad_norm": 1.4300717115402222,
      "learning_rate": 0.00010726817042606516,
      "loss": 1.789,
      "step": 215
    },
    {
      "epoch": 0.05519663802295678,
      "grad_norm": 1.7966753244400024,
      "learning_rate": 0.00010977443609022557,
      "loss": 1.7548,
      "step": 220
    },
    {
      "epoch": 0.056451107068933075,
      "grad_norm": 1.6580311059951782,
      "learning_rate": 0.00011228070175438597,
      "loss": 1.7829,
      "step": 225
    },
    {
      "epoch": 0.05770557611490936,
      "grad_norm": 1.472223162651062,
      "learning_rate": 0.00011478696741854638,
      "loss": 1.8284,
      "step": 230
    },
    {
      "epoch": 0.058960045160885655,
      "grad_norm": 1.5930159091949463,
      "learning_rate": 0.00011729323308270677,
      "loss": 1.9071,
      "step": 235
    },
    {
      "epoch": 0.06021451420686195,
      "grad_norm": 1.589754343032837,
      "learning_rate": 0.00011979949874686718,
      "loss": 1.7444,
      "step": 240
    },
    {
      "epoch": 0.061468983252838234,
      "grad_norm": 1.5218851566314697,
      "learning_rate": 0.00012230576441102757,
      "loss": 1.7672,
      "step": 245
    },
    {
      "epoch": 0.06272345229881453,
      "grad_norm": 1.552700400352478,
      "learning_rate": 0.00012481203007518797,
      "loss": 1.7958,
      "step": 250
    },
    {
      "epoch": 0.06397792134479081,
      "grad_norm": 1.5872631072998047,
      "learning_rate": 0.00012731829573934836,
      "loss": 1.7854,
      "step": 255
    },
    {
      "epoch": 0.06523239039076711,
      "grad_norm": 1.4999492168426514,
      "learning_rate": 0.0001298245614035088,
      "loss": 1.7901,
      "step": 260
    },
    {
      "epoch": 0.0664868594367434,
      "grad_norm": 1.3488736152648926,
      "learning_rate": 0.00013233082706766918,
      "loss": 1.7491,
      "step": 265
    },
    {
      "epoch": 0.06774132848271969,
      "grad_norm": 1.4061459302902222,
      "learning_rate": 0.00013483709273182958,
      "loss": 1.7047,
      "step": 270
    },
    {
      "epoch": 0.06899579752869597,
      "grad_norm": 1.5539060831069946,
      "learning_rate": 0.00013734335839598997,
      "loss": 1.8813,
      "step": 275
    },
    {
      "epoch": 0.07025026657467227,
      "grad_norm": 1.3255760669708252,
      "learning_rate": 0.0001398496240601504,
      "loss": 1.7427,
      "step": 280
    },
    {
      "epoch": 0.07150473562064856,
      "grad_norm": 1.8472603559494019,
      "learning_rate": 0.0001423558897243108,
      "loss": 1.8555,
      "step": 285
    },
    {
      "epoch": 0.07275920466662485,
      "grad_norm": 1.792567253112793,
      "learning_rate": 0.00014486215538847118,
      "loss": 1.7957,
      "step": 290
    },
    {
      "epoch": 0.07401367371260115,
      "grad_norm": 1.5343375205993652,
      "learning_rate": 0.00014736842105263158,
      "loss": 1.7189,
      "step": 295
    },
    {
      "epoch": 0.07526814275857743,
      "grad_norm": 1.3406875133514404,
      "learning_rate": 0.000149874686716792,
      "loss": 1.7854,
      "step": 300
    },
    {
      "epoch": 0.07652261180455372,
      "grad_norm": 1.778550148010254,
      "learning_rate": 0.00015238095238095237,
      "loss": 1.7993,
      "step": 305
    },
    {
      "epoch": 0.07777708085053002,
      "grad_norm": 1.465734839439392,
      "learning_rate": 0.0001548872180451128,
      "loss": 1.8113,
      "step": 310
    },
    {
      "epoch": 0.0790315498965063,
      "grad_norm": 1.4441167116165161,
      "learning_rate": 0.00015739348370927319,
      "loss": 1.7809,
      "step": 315
    },
    {
      "epoch": 0.08028601894248259,
      "grad_norm": 1.6283718347549438,
      "learning_rate": 0.00015989974937343358,
      "loss": 1.7534,
      "step": 320
    },
    {
      "epoch": 0.08154048798845888,
      "grad_norm": 1.6191918849945068,
      "learning_rate": 0.00016240601503759398,
      "loss": 1.7798,
      "step": 325
    },
    {
      "epoch": 0.08279495703443518,
      "grad_norm": 1.6543523073196411,
      "learning_rate": 0.0001649122807017544,
      "loss": 1.8052,
      "step": 330
    },
    {
      "epoch": 0.08404942608041147,
      "grad_norm": 1.3812458515167236,
      "learning_rate": 0.0001674185463659148,
      "loss": 1.6911,
      "step": 335
    },
    {
      "epoch": 0.08530389512638775,
      "grad_norm": 1.5960911512374878,
      "learning_rate": 0.0001699248120300752,
      "loss": 1.8121,
      "step": 340
    },
    {
      "epoch": 0.08655836417236405,
      "grad_norm": 1.3111087083816528,
      "learning_rate": 0.00017243107769423558,
      "loss": 1.7216,
      "step": 345
    },
    {
      "epoch": 0.08781283321834034,
      "grad_norm": 1.4991482496261597,
      "learning_rate": 0.000174937343358396,
      "loss": 1.7662,
      "step": 350
    },
    {
      "epoch": 0.08906730226431663,
      "grad_norm": 1.437935471534729,
      "learning_rate": 0.0001774436090225564,
      "loss": 1.7263,
      "step": 355
    },
    {
      "epoch": 0.09032177131029293,
      "grad_norm": 1.5097156763076782,
      "learning_rate": 0.0001799498746867168,
      "loss": 1.7094,
      "step": 360
    },
    {
      "epoch": 0.09157624035626921,
      "grad_norm": 1.6728209257125854,
      "learning_rate": 0.0001824561403508772,
      "loss": 1.7229,
      "step": 365
    },
    {
      "epoch": 0.0928307094022455,
      "grad_norm": 1.3799026012420654,
      "learning_rate": 0.0001849624060150376,
      "loss": 1.8439,
      "step": 370
    },
    {
      "epoch": 0.09408517844822178,
      "grad_norm": 2.0296425819396973,
      "learning_rate": 0.00018746867167919798,
      "loss": 1.8255,
      "step": 375
    },
    {
      "epoch": 0.09533964749419808,
      "grad_norm": 1.511501669883728,
      "learning_rate": 0.0001899749373433584,
      "loss": 1.842,
      "step": 380
    },
    {
      "epoch": 0.09659411654017437,
      "grad_norm": 1.4508113861083984,
      "learning_rate": 0.0001924812030075188,
      "loss": 1.7388,
      "step": 385
    },
    {
      "epoch": 0.09784858558615066,
      "grad_norm": 1.6131987571716309,
      "learning_rate": 0.00019498746867167922,
      "loss": 1.7823,
      "step": 390
    },
    {
      "epoch": 0.09910305463212696,
      "grad_norm": 1.3647732734680176,
      "learning_rate": 0.0001974937343358396,
      "loss": 1.7105,
      "step": 395
    },
    {
      "epoch": 0.10035752367810324,
      "grad_norm": 1.4319194555282593,
      "learning_rate": 0.0002,
      "loss": 1.8352,
      "step": 400
    },
    {
      "epoch": 0.10161199272407953,
      "grad_norm": 1.3733336925506592,
      "learning_rate": 0.0001997212155004182,
      "loss": 1.7772,
      "step": 405
    },
    {
      "epoch": 0.10286646177005582,
      "grad_norm": 1.4792115688323975,
      "learning_rate": 0.00019944243100083637,
      "loss": 1.7282,
      "step": 410
    },
    {
      "epoch": 0.10412093081603212,
      "grad_norm": 1.471961498260498,
      "learning_rate": 0.00019916364650125455,
      "loss": 1.7384,
      "step": 415
    },
    {
      "epoch": 0.1053753998620084,
      "grad_norm": 1.5700044631958008,
      "learning_rate": 0.0001988848620016727,
      "loss": 1.8295,
      "step": 420
    },
    {
      "epoch": 0.10662986890798469,
      "grad_norm": 1.5195121765136719,
      "learning_rate": 0.00019860607750209092,
      "loss": 1.7881,
      "step": 425
    },
    {
      "epoch": 0.10788433795396099,
      "grad_norm": 1.5140210390090942,
      "learning_rate": 0.00019832729300250907,
      "loss": 1.8584,
      "step": 430
    },
    {
      "epoch": 0.10913880699993728,
      "grad_norm": 1.351803183555603,
      "learning_rate": 0.00019804850850292725,
      "loss": 1.7702,
      "step": 435
    },
    {
      "epoch": 0.11039327604591356,
      "grad_norm": 1.5621734857559204,
      "learning_rate": 0.0001977697240033454,
      "loss": 1.7271,
      "step": 440
    },
    {
      "epoch": 0.11164774509188986,
      "grad_norm": 1.4309163093566895,
      "learning_rate": 0.0001974909395037636,
      "loss": 1.7015,
      "step": 445
    },
    {
      "epoch": 0.11290221413786615,
      "grad_norm": 1.9489997625350952,
      "learning_rate": 0.00019721215500418177,
      "loss": 1.7818,
      "step": 450
    },
    {
      "epoch": 0.11415668318384244,
      "grad_norm": 1.4814459085464478,
      "learning_rate": 0.00019693337050459995,
      "loss": 1.7134,
      "step": 455
    },
    {
      "epoch": 0.11541115222981872,
      "grad_norm": 1.334621548652649,
      "learning_rate": 0.00019665458600501813,
      "loss": 1.7673,
      "step": 460
    },
    {
      "epoch": 0.11666562127579502,
      "grad_norm": 1.653243899345398,
      "learning_rate": 0.0001963758015054363,
      "loss": 1.7994,
      "step": 465
    },
    {
      "epoch": 0.11792009032177131,
      "grad_norm": 1.4145588874816895,
      "learning_rate": 0.00019609701700585446,
      "loss": 1.7508,
      "step": 470
    },
    {
      "epoch": 0.1191745593677476,
      "grad_norm": 1.5953185558319092,
      "learning_rate": 0.00019581823250627267,
      "loss": 1.758,
      "step": 475
    },
    {
      "epoch": 0.1204290284137239,
      "grad_norm": 1.4390195608139038,
      "learning_rate": 0.00019553944800669083,
      "loss": 1.7361,
      "step": 480
    },
    {
      "epoch": 0.12168349745970018,
      "grad_norm": 1.8018484115600586,
      "learning_rate": 0.000195260663507109,
      "loss": 1.8441,
      "step": 485
    },
    {
      "epoch": 0.12293796650567647,
      "grad_norm": 1.5973496437072754,
      "learning_rate": 0.0001949818790075272,
      "loss": 1.7609,
      "step": 490
    },
    {
      "epoch": 0.12419243555165277,
      "grad_norm": 1.5459802150726318,
      "learning_rate": 0.00019470309450794537,
      "loss": 1.7024,
      "step": 495
    },
    {
      "epoch": 0.12544690459762906,
      "grad_norm": 1.4581584930419922,
      "learning_rate": 0.00019442431000836355,
      "loss": 1.7919,
      "step": 500
    },
    {
      "epoch": 0.12670137364360534,
      "grad_norm": 1.7628216743469238,
      "learning_rate": 0.0001941455255087817,
      "loss": 1.7421,
      "step": 505
    },
    {
      "epoch": 0.12795584268958163,
      "grad_norm": 1.5366220474243164,
      "learning_rate": 0.0001938667410091999,
      "loss": 1.7585,
      "step": 510
    },
    {
      "epoch": 0.12921031173555791,
      "grad_norm": 1.5331356525421143,
      "learning_rate": 0.00019358795650961807,
      "loss": 1.7386,
      "step": 515
    },
    {
      "epoch": 0.13046478078153423,
      "grad_norm": 1.4182711839675903,
      "learning_rate": 0.00019330917201003625,
      "loss": 1.7246,
      "step": 520
    },
    {
      "epoch": 0.13171924982751052,
      "grad_norm": 1.542728066444397,
      "learning_rate": 0.00019303038751045443,
      "loss": 1.7844,
      "step": 525
    },
    {
      "epoch": 0.1329737188734868,
      "grad_norm": 1.4071030616760254,
      "learning_rate": 0.0001927516030108726,
      "loss": 1.7028,
      "step": 530
    },
    {
      "epoch": 0.1342281879194631,
      "grad_norm": 1.644376516342163,
      "learning_rate": 0.00019247281851129076,
      "loss": 1.77,
      "step": 535
    },
    {
      "epoch": 0.13548265696543937,
      "grad_norm": 1.6317068338394165,
      "learning_rate": 0.00019219403401170897,
      "loss": 1.7147,
      "step": 540
    },
    {
      "epoch": 0.13673712601141566,
      "grad_norm": 1.4396439790725708,
      "learning_rate": 0.00019191524951212713,
      "loss": 1.7894,
      "step": 545
    },
    {
      "epoch": 0.13799159505739195,
      "grad_norm": 1.4927990436553955,
      "learning_rate": 0.0001916364650125453,
      "loss": 1.8278,
      "step": 550
    },
    {
      "epoch": 0.13924606410336826,
      "grad_norm": 1.7446597814559937,
      "learning_rate": 0.0001913576805129635,
      "loss": 1.7941,
      "step": 555
    },
    {
      "epoch": 0.14050053314934455,
      "grad_norm": 1.5361045598983765,
      "learning_rate": 0.00019107889601338167,
      "loss": 1.7805,
      "step": 560
    },
    {
      "epoch": 0.14175500219532083,
      "grad_norm": 1.5425162315368652,
      "learning_rate": 0.00019080011151379985,
      "loss": 1.7622,
      "step": 565
    },
    {
      "epoch": 0.14300947124129712,
      "grad_norm": 1.6209547519683838,
      "learning_rate": 0.000190521327014218,
      "loss": 1.7468,
      "step": 570
    },
    {
      "epoch": 0.1442639402872734,
      "grad_norm": 1.5693891048431396,
      "learning_rate": 0.00019024254251463621,
      "loss": 1.766,
      "step": 575
    },
    {
      "epoch": 0.1455184093332497,
      "grad_norm": 1.7010644674301147,
      "learning_rate": 0.00018996375801505437,
      "loss": 1.7354,
      "step": 580
    },
    {
      "epoch": 0.14677287837922598,
      "grad_norm": 1.5666614770889282,
      "learning_rate": 0.00018968497351547255,
      "loss": 1.6964,
      "step": 585
    },
    {
      "epoch": 0.1480273474252023,
      "grad_norm": 1.3512639999389648,
      "learning_rate": 0.00018940618901589073,
      "loss": 1.724,
      "step": 590
    },
    {
      "epoch": 0.14928181647117858,
      "grad_norm": 1.9726330041885376,
      "learning_rate": 0.0001891274045163089,
      "loss": 1.7895,
      "step": 595
    },
    {
      "epoch": 0.15053628551715487,
      "grad_norm": 1.985482931137085,
      "learning_rate": 0.00018884862001672707,
      "loss": 1.8096,
      "step": 600
    },
    {
      "epoch": 0.15179075456313115,
      "grad_norm": 1.6905200481414795,
      "learning_rate": 0.00018856983551714527,
      "loss": 1.7819,
      "step": 605
    },
    {
      "epoch": 0.15304522360910744,
      "grad_norm": 1.6398239135742188,
      "learning_rate": 0.00018829105101756343,
      "loss": 1.7922,
      "step": 610
    },
    {
      "epoch": 0.15429969265508373,
      "grad_norm": 1.4872336387634277,
      "learning_rate": 0.0001880122665179816,
      "loss": 1.795,
      "step": 615
    },
    {
      "epoch": 0.15555416170106004,
      "grad_norm": 1.6230344772338867,
      "learning_rate": 0.00018773348201839976,
      "loss": 1.7256,
      "step": 620
    },
    {
      "epoch": 0.15680863074703633,
      "grad_norm": 1.641273021697998,
      "learning_rate": 0.00018745469751881797,
      "loss": 1.7345,
      "step": 625
    },
    {
      "epoch": 0.1580630997930126,
      "grad_norm": 1.576269268989563,
      "learning_rate": 0.00018717591301923613,
      "loss": 1.6572,
      "step": 630
    },
    {
      "epoch": 0.1593175688389889,
      "grad_norm": 1.5445375442504883,
      "learning_rate": 0.0001868971285196543,
      "loss": 1.7173,
      "step": 635
    },
    {
      "epoch": 0.16057203788496519,
      "grad_norm": 1.5789974927902222,
      "learning_rate": 0.0001866183440200725,
      "loss": 1.7548,
      "step": 640
    },
    {
      "epoch": 0.16182650693094147,
      "grad_norm": 1.4345887899398804,
      "learning_rate": 0.00018633955952049067,
      "loss": 1.7214,
      "step": 645
    },
    {
      "epoch": 0.16308097597691776,
      "grad_norm": 1.8656429052352905,
      "learning_rate": 0.00018606077502090885,
      "loss": 1.7594,
      "step": 650
    },
    {
      "epoch": 0.16433544502289407,
      "grad_norm": 1.5815279483795166,
      "learning_rate": 0.00018578199052132703,
      "loss": 1.7618,
      "step": 655
    },
    {
      "epoch": 0.16558991406887036,
      "grad_norm": 1.6873443126678467,
      "learning_rate": 0.0001855032060217452,
      "loss": 1.7245,
      "step": 660
    },
    {
      "epoch": 0.16684438311484664,
      "grad_norm": 1.7873115539550781,
      "learning_rate": 0.00018522442152216337,
      "loss": 1.7519,
      "step": 665
    },
    {
      "epoch": 0.16809885216082293,
      "grad_norm": 1.5285781621932983,
      "learning_rate": 0.00018494563702258157,
      "loss": 1.6789,
      "step": 670
    },
    {
      "epoch": 0.16935332120679922,
      "grad_norm": 1.5594218969345093,
      "learning_rate": 0.00018466685252299973,
      "loss": 1.642,
      "step": 675
    },
    {
      "epoch": 0.1706077902527755,
      "grad_norm": 1.5877432823181152,
      "learning_rate": 0.0001843880680234179,
      "loss": 1.7173,
      "step": 680
    },
    {
      "epoch": 0.1718622592987518,
      "grad_norm": 1.55059814453125,
      "learning_rate": 0.00018410928352383606,
      "loss": 1.737,
      "step": 685
    },
    {
      "epoch": 0.1731167283447281,
      "grad_norm": 1.46401846408844,
      "learning_rate": 0.00018383049902425427,
      "loss": 1.6761,
      "step": 690
    },
    {
      "epoch": 0.1743711973907044,
      "grad_norm": 1.7427974939346313,
      "learning_rate": 0.00018355171452467243,
      "loss": 1.7126,
      "step": 695
    },
    {
      "epoch": 0.17562566643668068,
      "grad_norm": 1.7048364877700806,
      "learning_rate": 0.0001832729300250906,
      "loss": 1.7001,
      "step": 700
    },
    {
      "epoch": 0.17688013548265696,
      "grad_norm": 1.4376134872436523,
      "learning_rate": 0.0001829941455255088,
      "loss": 1.7156,
      "step": 705
    },
    {
      "epoch": 0.17813460452863325,
      "grad_norm": 1.4186033010482788,
      "learning_rate": 0.00018271536102592697,
      "loss": 1.7086,
      "step": 710
    },
    {
      "epoch": 0.17938907357460954,
      "grad_norm": 1.9593678712844849,
      "learning_rate": 0.00018243657652634515,
      "loss": 1.8327,
      "step": 715
    },
    {
      "epoch": 0.18064354262058585,
      "grad_norm": 1.6261996030807495,
      "learning_rate": 0.00018215779202676333,
      "loss": 1.8076,
      "step": 720
    },
    {
      "epoch": 0.18189801166656214,
      "grad_norm": 1.5430229902267456,
      "learning_rate": 0.0001818790075271815,
      "loss": 1.7501,
      "step": 725
    },
    {
      "epoch": 0.18315248071253842,
      "grad_norm": 1.6103847026824951,
      "learning_rate": 0.00018160022302759967,
      "loss": 1.7202,
      "step": 730
    },
    {
      "epoch": 0.1844069497585147,
      "grad_norm": 1.7368253469467163,
      "learning_rate": 0.00018132143852801788,
      "loss": 1.7628,
      "step": 735
    },
    {
      "epoch": 0.185661418804491,
      "grad_norm": 1.597786784172058,
      "learning_rate": 0.00018104265402843603,
      "loss": 1.7304,
      "step": 740
    },
    {
      "epoch": 0.18691588785046728,
      "grad_norm": 1.948281168937683,
      "learning_rate": 0.0001807638695288542,
      "loss": 1.6692,
      "step": 745
    },
    {
      "epoch": 0.18817035689644357,
      "grad_norm": 1.624162197113037,
      "learning_rate": 0.00018048508502927236,
      "loss": 1.75,
      "step": 750
    },
    {
      "epoch": 0.18942482594241988,
      "grad_norm": 1.6655960083007812,
      "learning_rate": 0.00018020630052969057,
      "loss": 1.7112,
      "step": 755
    },
    {
      "epoch": 0.19067929498839617,
      "grad_norm": 1.6824384927749634,
      "learning_rate": 0.00017992751603010873,
      "loss": 1.7302,
      "step": 760
    },
    {
      "epoch": 0.19193376403437246,
      "grad_norm": 1.5126327276229858,
      "learning_rate": 0.0001796487315305269,
      "loss": 1.7155,
      "step": 765
    },
    {
      "epoch": 0.19318823308034874,
      "grad_norm": 1.7209875583648682,
      "learning_rate": 0.0001793699470309451,
      "loss": 1.6864,
      "step": 770
    },
    {
      "epoch": 0.19444270212632503,
      "grad_norm": 1.8246557712554932,
      "learning_rate": 0.00017909116253136327,
      "loss": 1.7519,
      "step": 775
    },
    {
      "epoch": 0.19569717117230132,
      "grad_norm": 1.561708688735962,
      "learning_rate": 0.00017881237803178142,
      "loss": 1.6473,
      "step": 780
    },
    {
      "epoch": 0.1969516402182776,
      "grad_norm": 1.654950737953186,
      "learning_rate": 0.00017853359353219963,
      "loss": 1.7373,
      "step": 785
    },
    {
      "epoch": 0.19820610926425392,
      "grad_norm": 1.9264365434646606,
      "learning_rate": 0.00017825480903261779,
      "loss": 1.7361,
      "step": 790
    },
    {
      "epoch": 0.1994605783102302,
      "grad_norm": 1.7251559495925903,
      "learning_rate": 0.00017797602453303597,
      "loss": 1.7561,
      "step": 795
    },
    {
      "epoch": 0.2007150473562065,
      "grad_norm": 1.6873254776000977,
      "learning_rate": 0.00017769724003345415,
      "loss": 1.699,
      "step": 800
    },
    {
      "epoch": 0.20196951640218277,
      "grad_norm": 1.578249454498291,
      "learning_rate": 0.00017741845553387233,
      "loss": 1.6658,
      "step": 805
    },
    {
      "epoch": 0.20322398544815906,
      "grad_norm": 1.908272385597229,
      "learning_rate": 0.0001771396710342905,
      "loss": 1.7795,
      "step": 810
    },
    {
      "epoch": 0.20447845449413535,
      "grad_norm": 1.3954023122787476,
      "learning_rate": 0.00017686088653470866,
      "loss": 1.6417,
      "step": 815
    },
    {
      "epoch": 0.20573292354011163,
      "grad_norm": 1.5489314794540405,
      "learning_rate": 0.00017658210203512687,
      "loss": 1.7247,
      "step": 820
    },
    {
      "epoch": 0.20698739258608795,
      "grad_norm": 1.5505293607711792,
      "learning_rate": 0.00017630331753554503,
      "loss": 1.8101,
      "step": 825
    },
    {
      "epoch": 0.20824186163206423,
      "grad_norm": 1.7525371313095093,
      "learning_rate": 0.0001760245330359632,
      "loss": 1.6621,
      "step": 830
    },
    {
      "epoch": 0.20949633067804052,
      "grad_norm": 1.5366259813308716,
      "learning_rate": 0.0001757457485363814,
      "loss": 1.6014,
      "step": 835
    },
    {
      "epoch": 0.2107507997240168,
      "grad_norm": 1.7520825862884521,
      "learning_rate": 0.00017546696403679957,
      "loss": 1.7212,
      "step": 840
    },
    {
      "epoch": 0.2120052687699931,
      "grad_norm": 1.4406347274780273,
      "learning_rate": 0.00017518817953721772,
      "loss": 1.7415,
      "step": 845
    },
    {
      "epoch": 0.21325973781596938,
      "grad_norm": 1.6416645050048828,
      "learning_rate": 0.00017490939503763593,
      "loss": 1.7693,
      "step": 850
    },
    {
      "epoch": 0.2145142068619457,
      "grad_norm": 1.6689332723617554,
      "learning_rate": 0.0001746306105380541,
      "loss": 1.7009,
      "step": 855
    },
    {
      "epoch": 0.21576867590792198,
      "grad_norm": 1.541351318359375,
      "learning_rate": 0.00017435182603847227,
      "loss": 1.7231,
      "step": 860
    },
    {
      "epoch": 0.21702314495389827,
      "grad_norm": 1.7256262302398682,
      "learning_rate": 0.00017407304153889045,
      "loss": 1.7246,
      "step": 865
    },
    {
      "epoch": 0.21827761399987455,
      "grad_norm": 2.119253396987915,
      "learning_rate": 0.00017379425703930863,
      "loss": 1.7233,
      "step": 870
    },
    {
      "epoch": 0.21953208304585084,
      "grad_norm": 1.5007317066192627,
      "learning_rate": 0.00017351547253972678,
      "loss": 1.6518,
      "step": 875
    },
    {
      "epoch": 0.22078655209182713,
      "grad_norm": 1.420782446861267,
      "learning_rate": 0.00017323668804014497,
      "loss": 1.6738,
      "step": 880
    },
    {
      "epoch": 0.2220410211378034,
      "grad_norm": 1.6109153032302856,
      "learning_rate": 0.00017295790354056315,
      "loss": 1.6938,
      "step": 885
    },
    {
      "epoch": 0.22329549018377973,
      "grad_norm": 1.662375569343567,
      "learning_rate": 0.00017267911904098133,
      "loss": 1.7105,
      "step": 890
    },
    {
      "epoch": 0.224549959229756,
      "grad_norm": 1.935225009918213,
      "learning_rate": 0.0001724003345413995,
      "loss": 1.6217,
      "step": 895
    },
    {
      "epoch": 0.2258044282757323,
      "grad_norm": 1.6429779529571533,
      "learning_rate": 0.0001721215500418177,
      "loss": 1.7316,
      "step": 900
    },
    {
      "epoch": 0.22705889732170859,
      "grad_norm": 1.6015443801879883,
      "learning_rate": 0.00017184276554223587,
      "loss": 1.7413,
      "step": 905
    },
    {
      "epoch": 0.22831336636768487,
      "grad_norm": 1.689624309539795,
      "learning_rate": 0.00017156398104265403,
      "loss": 1.6508,
      "step": 910
    },
    {
      "epoch": 0.22956783541366116,
      "grad_norm": 1.7538173198699951,
      "learning_rate": 0.00017128519654307223,
      "loss": 1.6701,
      "step": 915
    },
    {
      "epoch": 0.23082230445963745,
      "grad_norm": 1.4934544563293457,
      "learning_rate": 0.0001710064120434904,
      "loss": 1.6532,
      "step": 920
    },
    {
      "epoch": 0.23207677350561376,
      "grad_norm": 1.7957183122634888,
      "learning_rate": 0.00017072762754390857,
      "loss": 1.7037,
      "step": 925
    },
    {
      "epoch": 0.23333124255159005,
      "grad_norm": 1.6542049646377563,
      "learning_rate": 0.00017044884304432672,
      "loss": 1.7295,
      "step": 930
    },
    {
      "epoch": 0.23458571159756633,
      "grad_norm": 1.7184418439865112,
      "learning_rate": 0.00017017005854474493,
      "loss": 1.6821,
      "step": 935
    },
    {
      "epoch": 0.23584018064354262,
      "grad_norm": 1.5908560752868652,
      "learning_rate": 0.00016989127404516308,
      "loss": 1.7045,
      "step": 940
    },
    {
      "epoch": 0.2370946496895189,
      "grad_norm": 1.5656769275665283,
      "learning_rate": 0.00016961248954558127,
      "loss": 1.7513,
      "step": 945
    },
    {
      "epoch": 0.2383491187354952,
      "grad_norm": 1.630885362625122,
      "learning_rate": 0.00016933370504599945,
      "loss": 1.7102,
      "step": 950
    },
    {
      "epoch": 0.2396035877814715,
      "grad_norm": 1.6369636058807373,
      "learning_rate": 0.00016905492054641763,
      "loss": 1.7473,
      "step": 955
    },
    {
      "epoch": 0.2408580568274478,
      "grad_norm": 1.9807883501052856,
      "learning_rate": 0.0001687761360468358,
      "loss": 1.6411,
      "step": 960
    },
    {
      "epoch": 0.24211252587342408,
      "grad_norm": 1.721799612045288,
      "learning_rate": 0.000168497351547254,
      "loss": 1.666,
      "step": 965
    },
    {
      "epoch": 0.24336699491940036,
      "grad_norm": 1.6385102272033691,
      "learning_rate": 0.00016821856704767217,
      "loss": 1.7468,
      "step": 970
    },
    {
      "epoch": 0.24462146396537665,
      "grad_norm": 1.7736561298370361,
      "learning_rate": 0.00016793978254809033,
      "loss": 1.7879,
      "step": 975
    },
    {
      "epoch": 0.24587593301135294,
      "grad_norm": 1.441989541053772,
      "learning_rate": 0.00016766099804850853,
      "loss": 1.5934,
      "step": 980
    },
    {
      "epoch": 0.24713040205732922,
      "grad_norm": 1.8452686071395874,
      "learning_rate": 0.0001673822135489267,
      "loss": 1.585,
      "step": 985
    },
    {
      "epoch": 0.24838487110330554,
      "grad_norm": 1.663414478302002,
      "learning_rate": 0.00016710342904934487,
      "loss": 1.6801,
      "step": 990
    },
    {
      "epoch": 0.24963934014928182,
      "grad_norm": 1.3995325565338135,
      "learning_rate": 0.00016682464454976302,
      "loss": 1.6893,
      "step": 995
    },
    {
      "epoch": 0.2508938091952581,
      "grad_norm": 1.818676471710205,
      "learning_rate": 0.00016654586005018123,
      "loss": 1.6959,
      "step": 1000
    },
    {
      "epoch": 0.2521482782412344,
      "grad_norm": 1.6757841110229492,
      "learning_rate": 0.00016626707555059939,
      "loss": 1.6776,
      "step": 1005
    },
    {
      "epoch": 0.2534027472872107,
      "grad_norm": 1.392061710357666,
      "learning_rate": 0.00016598829105101757,
      "loss": 1.6821,
      "step": 1010
    },
    {
      "epoch": 0.254657216333187,
      "grad_norm": 1.7358981370925903,
      "learning_rate": 0.00016570950655143575,
      "loss": 1.7101,
      "step": 1015
    },
    {
      "epoch": 0.25591168537916326,
      "grad_norm": 1.4397127628326416,
      "learning_rate": 0.00016543072205185393,
      "loss": 1.6039,
      "step": 1020
    },
    {
      "epoch": 0.25716615442513957,
      "grad_norm": 1.6486618518829346,
      "learning_rate": 0.00016515193755227208,
      "loss": 1.6589,
      "step": 1025
    },
    {
      "epoch": 0.25842062347111583,
      "grad_norm": 1.640455722808838,
      "learning_rate": 0.0001648731530526903,
      "loss": 1.6505,
      "step": 1030
    },
    {
      "epoch": 0.25967509251709214,
      "grad_norm": 1.5449481010437012,
      "learning_rate": 0.00016459436855310845,
      "loss": 1.6693,
      "step": 1035
    },
    {
      "epoch": 0.26092956156306846,
      "grad_norm": 1.5721392631530762,
      "learning_rate": 0.00016431558405352663,
      "loss": 1.6852,
      "step": 1040
    },
    {
      "epoch": 0.2621840306090447,
      "grad_norm": 1.430965542793274,
      "learning_rate": 0.0001640367995539448,
      "loss": 1.6254,
      "step": 1045
    },
    {
      "epoch": 0.26343849965502103,
      "grad_norm": 1.6304048299789429,
      "learning_rate": 0.000163758015054363,
      "loss": 1.6954,
      "step": 1050
    },
    {
      "epoch": 0.2646929687009973,
      "grad_norm": 1.5228047370910645,
      "learning_rate": 0.00016347923055478117,
      "loss": 1.7095,
      "step": 1055
    },
    {
      "epoch": 0.2659474377469736,
      "grad_norm": 1.9678256511688232,
      "learning_rate": 0.00016320044605519932,
      "loss": 1.6708,
      "step": 1060
    },
    {
      "epoch": 0.26720190679294986,
      "grad_norm": 1.6282693147659302,
      "learning_rate": 0.00016292166155561753,
      "loss": 1.5958,
      "step": 1065
    },
    {
      "epoch": 0.2684563758389262,
      "grad_norm": 1.749875783920288,
      "learning_rate": 0.00016264287705603569,
      "loss": 1.6449,
      "step": 1070
    },
    {
      "epoch": 0.2697108448849025,
      "grad_norm": 1.5609071254730225,
      "learning_rate": 0.00016236409255645387,
      "loss": 1.6325,
      "step": 1075
    },
    {
      "epoch": 0.27096531393087875,
      "grad_norm": 1.8210757970809937,
      "learning_rate": 0.00016208530805687205,
      "loss": 1.6967,
      "step": 1080
    },
    {
      "epoch": 0.27221978297685506,
      "grad_norm": 1.9798849821090698,
      "learning_rate": 0.00016180652355729023,
      "loss": 1.686,
      "step": 1085
    },
    {
      "epoch": 0.2734742520228313,
      "grad_norm": 1.6853595972061157,
      "learning_rate": 0.00016152773905770838,
      "loss": 1.6566,
      "step": 1090
    },
    {
      "epoch": 0.27472872106880764,
      "grad_norm": 1.5516512393951416,
      "learning_rate": 0.0001612489545581266,
      "loss": 1.6245,
      "step": 1095
    },
    {
      "epoch": 0.2759831901147839,
      "grad_norm": 1.88346529006958,
      "learning_rate": 0.00016097017005854475,
      "loss": 1.6395,
      "step": 1100
    },
    {
      "epoch": 0.2772376591607602,
      "grad_norm": 1.6387193202972412,
      "learning_rate": 0.00016069138555896293,
      "loss": 1.6453,
      "step": 1105
    },
    {
      "epoch": 0.2784921282067365,
      "grad_norm": 1.7507480382919312,
      "learning_rate": 0.0001604126010593811,
      "loss": 1.6998,
      "step": 1110
    },
    {
      "epoch": 0.2797465972527128,
      "grad_norm": 1.744163155555725,
      "learning_rate": 0.0001601338165597993,
      "loss": 1.6705,
      "step": 1115
    },
    {
      "epoch": 0.2810010662986891,
      "grad_norm": 1.5398951768875122,
      "learning_rate": 0.00015985503206021747,
      "loss": 1.6594,
      "step": 1120
    },
    {
      "epoch": 0.28225553534466535,
      "grad_norm": 2.0034947395324707,
      "learning_rate": 0.00015957624756063562,
      "loss": 1.7158,
      "step": 1125
    },
    {
      "epoch": 0.28351000439064167,
      "grad_norm": 1.591568946838379,
      "learning_rate": 0.00015929746306105383,
      "loss": 1.6219,
      "step": 1130
    },
    {
      "epoch": 0.2847644734366179,
      "grad_norm": 1.6958308219909668,
      "learning_rate": 0.000159018678561472,
      "loss": 1.6156,
      "step": 1135
    },
    {
      "epoch": 0.28601894248259424,
      "grad_norm": 1.9034945964813232,
      "learning_rate": 0.00015873989406189017,
      "loss": 1.6716,
      "step": 1140
    },
    {
      "epoch": 0.28727341152857055,
      "grad_norm": 1.5771782398223877,
      "learning_rate": 0.00015846110956230835,
      "loss": 1.729,
      "step": 1145
    },
    {
      "epoch": 0.2885278805745468,
      "grad_norm": 1.7011818885803223,
      "learning_rate": 0.00015818232506272653,
      "loss": 1.6493,
      "step": 1150
    },
    {
      "epoch": 0.2897823496205231,
      "grad_norm": 1.7127310037612915,
      "learning_rate": 0.00015790354056314468,
      "loss": 1.5924,
      "step": 1155
    },
    {
      "epoch": 0.2910368186664994,
      "grad_norm": 1.7121738195419312,
      "learning_rate": 0.0001576247560635629,
      "loss": 1.6683,
      "step": 1160
    },
    {
      "epoch": 0.2922912877124757,
      "grad_norm": 1.8665292263031006,
      "learning_rate": 0.00015734597156398105,
      "loss": 1.6333,
      "step": 1165
    },
    {
      "epoch": 0.29354575675845196,
      "grad_norm": 1.8535325527191162,
      "learning_rate": 0.00015706718706439923,
      "loss": 1.7024,
      "step": 1170
    },
    {
      "epoch": 0.2948002258044283,
      "grad_norm": 1.8122323751449585,
      "learning_rate": 0.00015678840256481738,
      "loss": 1.6337,
      "step": 1175
    },
    {
      "epoch": 0.2960546948504046,
      "grad_norm": 2.1502559185028076,
      "learning_rate": 0.0001565096180652356,
      "loss": 1.6681,
      "step": 1180
    },
    {
      "epoch": 0.29730916389638085,
      "grad_norm": 1.928582787513733,
      "learning_rate": 0.00015623083356565374,
      "loss": 1.718,
      "step": 1185
    },
    {
      "epoch": 0.29856363294235716,
      "grad_norm": 1.763296127319336,
      "learning_rate": 0.00015595204906607193,
      "loss": 1.6298,
      "step": 1190
    },
    {
      "epoch": 0.2998181019883334,
      "grad_norm": 1.9737002849578857,
      "learning_rate": 0.0001556732645664901,
      "loss": 1.6308,
      "step": 1195
    },
    {
      "epoch": 0.30107257103430973,
      "grad_norm": 1.8902207612991333,
      "learning_rate": 0.0001553944800669083,
      "loss": 1.6403,
      "step": 1200
    },
    {
      "epoch": 0.302327040080286,
      "grad_norm": 1.5328165292739868,
      "learning_rate": 0.00015511569556732647,
      "loss": 1.6269,
      "step": 1205
    },
    {
      "epoch": 0.3035815091262623,
      "grad_norm": 1.430749773979187,
      "learning_rate": 0.00015483691106774465,
      "loss": 1.7169,
      "step": 1210
    },
    {
      "epoch": 0.3048359781722386,
      "grad_norm": 23.060165405273438,
      "learning_rate": 0.00015455812656816283,
      "loss": 1.7352,
      "step": 1215
    },
    {
      "epoch": 0.3060904472182149,
      "grad_norm": 26.415626525878906,
      "learning_rate": 0.00015427934206858098,
      "loss": 1.6583,
      "step": 1220
    },
    {
      "epoch": 0.3073449162641912,
      "grad_norm": 1.8812003135681152,
      "learning_rate": 0.0001540005575689992,
      "loss": 1.7123,
      "step": 1225
    },
    {
      "epoch": 0.30859938531016745,
      "grad_norm": 7.629766464233398,
      "learning_rate": 0.00015372177306941735,
      "loss": 1.7481,
      "step": 1230
    },
    {
      "epoch": 0.30985385435614377,
      "grad_norm": 1.6906757354736328,
      "learning_rate": 0.00015344298856983553,
      "loss": 1.711,
      "step": 1235
    },
    {
      "epoch": 0.3111083234021201,
      "grad_norm": 1.8586000204086304,
      "learning_rate": 0.00015316420407025368,
      "loss": 1.6638,
      "step": 1240
    },
    {
      "epoch": 0.31236279244809634,
      "grad_norm": 24.01845359802246,
      "learning_rate": 0.0001528854195706719,
      "loss": 1.6692,
      "step": 1245
    },
    {
      "epoch": 0.31361726149407265,
      "grad_norm": 1.9818251132965088,
      "learning_rate": 0.00015260663507109004,
      "loss": 1.703,
      "step": 1250
    },
    {
      "epoch": 0.3148717305400489,
      "grad_norm": 1.5615471601486206,
      "learning_rate": 0.00015232785057150823,
      "loss": 1.5927,
      "step": 1255
    },
    {
      "epoch": 0.3161261995860252,
      "grad_norm": 1.812896728515625,
      "learning_rate": 0.0001520490660719264,
      "loss": 1.7212,
      "step": 1260
    },
    {
      "epoch": 0.3173806686320015,
      "grad_norm": 1.63309907913208,
      "learning_rate": 0.0001517702815723446,
      "loss": 1.6779,
      "step": 1265
    },
    {
      "epoch": 0.3186351376779778,
      "grad_norm": 1.6149749755859375,
      "learning_rate": 0.00015149149707276277,
      "loss": 1.6611,
      "step": 1270
    },
    {
      "epoch": 0.3198896067239541,
      "grad_norm": 1.7380577325820923,
      "learning_rate": 0.00015121271257318095,
      "loss": 1.6228,
      "step": 1275
    },
    {
      "epoch": 0.32114407576993037,
      "grad_norm": 1.8977404832839966,
      "learning_rate": 0.0001509339280735991,
      "loss": 1.7466,
      "step": 1280
    },
    {
      "epoch": 0.3223985448159067,
      "grad_norm": 3.3576972484588623,
      "learning_rate": 0.00015065514357401729,
      "loss": 1.6603,
      "step": 1285
    },
    {
      "epoch": 0.32365301386188294,
      "grad_norm": 1.7637215852737427,
      "learning_rate": 0.00015037635907443547,
      "loss": 1.6672,
      "step": 1290
    },
    {
      "epoch": 0.32490748290785926,
      "grad_norm": 1.690110683441162,
      "learning_rate": 0.00015009757457485365,
      "loss": 1.5965,
      "step": 1295
    },
    {
      "epoch": 0.3261619519538355,
      "grad_norm": 1.7841615676879883,
      "learning_rate": 0.00014981879007527183,
      "loss": 1.6595,
      "step": 1300
    },
    {
      "epoch": 0.32741642099981183,
      "grad_norm": 1.7812838554382324,
      "learning_rate": 0.00014954000557568998,
      "loss": 1.6798,
      "step": 1305
    },
    {
      "epoch": 0.32867089004578814,
      "grad_norm": 1.9502500295639038,
      "learning_rate": 0.0001492612210761082,
      "loss": 1.659,
      "step": 1310
    },
    {
      "epoch": 0.3299253590917644,
      "grad_norm": 1.5713616609573364,
      "learning_rate": 0.00014898243657652635,
      "loss": 1.6666,
      "step": 1315
    },
    {
      "epoch": 0.3311798281377407,
      "grad_norm": 1.6910758018493652,
      "learning_rate": 0.00014870365207694453,
      "loss": 1.6669,
      "step": 1320
    },
    {
      "epoch": 0.332434297183717,
      "grad_norm": 1.7564833164215088,
      "learning_rate": 0.0001484248675773627,
      "loss": 1.6303,
      "step": 1325
    },
    {
      "epoch": 0.3336887662296933,
      "grad_norm": 1.7962658405303955,
      "learning_rate": 0.0001481460830777809,
      "loss": 1.6705,
      "step": 1330
    },
    {
      "epoch": 0.33494323527566955,
      "grad_norm": 1.8658915758132935,
      "learning_rate": 0.00014786729857819904,
      "loss": 1.696,
      "step": 1335
    },
    {
      "epoch": 0.33619770432164586,
      "grad_norm": 1.7769945859909058,
      "learning_rate": 0.00014758851407861725,
      "loss": 1.6053,
      "step": 1340
    },
    {
      "epoch": 0.3374521733676222,
      "grad_norm": 1.6663650274276733,
      "learning_rate": 0.0001473097295790354,
      "loss": 1.6418,
      "step": 1345
    },
    {
      "epoch": 0.33870664241359844,
      "grad_norm": 2.0161819458007812,
      "learning_rate": 0.00014703094507945359,
      "loss": 1.7085,
      "step": 1350
    },
    {
      "epoch": 0.33996111145957475,
      "grad_norm": 1.4605634212493896,
      "learning_rate": 0.00014675216057987177,
      "loss": 1.6325,
      "step": 1355
    },
    {
      "epoch": 0.341215580505551,
      "grad_norm": 1.9617465734481812,
      "learning_rate": 0.00014647337608028995,
      "loss": 1.7257,
      "step": 1360
    },
    {
      "epoch": 0.3424700495515273,
      "grad_norm": 1.8870182037353516,
      "learning_rate": 0.00014619459158070813,
      "loss": 1.6509,
      "step": 1365
    },
    {
      "epoch": 0.3437245185975036,
      "grad_norm": 1.5309830904006958,
      "learning_rate": 0.00014591580708112628,
      "loss": 1.6001,
      "step": 1370
    },
    {
      "epoch": 0.3449789876434799,
      "grad_norm": 1.804937720298767,
      "learning_rate": 0.0001456370225815445,
      "loss": 1.6521,
      "step": 1375
    },
    {
      "epoch": 0.3462334566894562,
      "grad_norm": 1.9400882720947266,
      "learning_rate": 0.00014535823808196265,
      "loss": 1.5953,
      "step": 1380
    },
    {
      "epoch": 0.34748792573543247,
      "grad_norm": 1.6156758069992065,
      "learning_rate": 0.00014507945358238083,
      "loss": 1.6317,
      "step": 1385
    },
    {
      "epoch": 0.3487423947814088,
      "grad_norm": 1.6138368844985962,
      "learning_rate": 0.000144800669082799,
      "loss": 1.5796,
      "step": 1390
    },
    {
      "epoch": 0.34999686382738504,
      "grad_norm": 1.519411563873291,
      "learning_rate": 0.0001445218845832172,
      "loss": 1.5996,
      "step": 1395
    },
    {
      "epoch": 0.35125133287336135,
      "grad_norm": 1.7902144193649292,
      "learning_rate": 0.00014424310008363534,
      "loss": 1.6326,
      "step": 1400
    },
    {
      "epoch": 0.3525058019193376,
      "grad_norm": 1.64842689037323,
      "learning_rate": 0.00014396431558405355,
      "loss": 1.6207,
      "step": 1405
    },
    {
      "epoch": 0.3537602709653139,
      "grad_norm": 1.4543744325637817,
      "learning_rate": 0.0001436855310844717,
      "loss": 1.5526,
      "step": 1410
    },
    {
      "epoch": 0.35501474001129024,
      "grad_norm": 1.8049907684326172,
      "learning_rate": 0.0001434067465848899,
      "loss": 1.665,
      "step": 1415
    },
    {
      "epoch": 0.3562692090572665,
      "grad_norm": 1.6804150342941284,
      "learning_rate": 0.00014312796208530804,
      "loss": 1.6562,
      "step": 1420
    },
    {
      "epoch": 0.3575236781032428,
      "grad_norm": 1.8215969800949097,
      "learning_rate": 0.00014284917758572625,
      "loss": 1.6596,
      "step": 1425
    },
    {
      "epoch": 0.3587781471492191,
      "grad_norm": 1.7333627939224243,
      "learning_rate": 0.0001425703930861444,
      "loss": 1.6385,
      "step": 1430
    },
    {
      "epoch": 0.3600326161951954,
      "grad_norm": 1.806034803390503,
      "learning_rate": 0.00014229160858656258,
      "loss": 1.7137,
      "step": 1435
    },
    {
      "epoch": 0.3612870852411717,
      "grad_norm": 1.815750002861023,
      "learning_rate": 0.00014201282408698077,
      "loss": 1.6008,
      "step": 1440
    },
    {
      "epoch": 0.36254155428714796,
      "grad_norm": 9.66055679321289,
      "learning_rate": 0.00014173403958739895,
      "loss": 1.6101,
      "step": 1445
    },
    {
      "epoch": 0.3637960233331243,
      "grad_norm": 1.7629473209381104,
      "learning_rate": 0.00014145525508781713,
      "loss": 1.6653,
      "step": 1450
    },
    {
      "epoch": 0.36505049237910053,
      "grad_norm": 1.733941674232483,
      "learning_rate": 0.0001411764705882353,
      "loss": 1.5572,
      "step": 1455
    },
    {
      "epoch": 0.36630496142507685,
      "grad_norm": 1.6288827657699585,
      "learning_rate": 0.0001408976860886535,
      "loss": 1.6164,
      "step": 1460
    },
    {
      "epoch": 0.3675594304710531,
      "grad_norm": 1.9351530075073242,
      "learning_rate": 0.00014061890158907164,
      "loss": 1.6697,
      "step": 1465
    },
    {
      "epoch": 0.3688138995170294,
      "grad_norm": 10.271966934204102,
      "learning_rate": 0.00014034011708948985,
      "loss": 1.7448,
      "step": 1470
    },
    {
      "epoch": 0.37006836856300573,
      "grad_norm": 1.7281320095062256,
      "learning_rate": 0.000140061332589908,
      "loss": 1.638,
      "step": 1475
    },
    {
      "epoch": 0.371322837608982,
      "grad_norm": 5.074897766113281,
      "learning_rate": 0.0001397825480903262,
      "loss": 1.7414,
      "step": 1480
    },
    {
      "epoch": 0.3725773066549583,
      "grad_norm": 1.639053225517273,
      "learning_rate": 0.00013950376359074434,
      "loss": 1.6267,
      "step": 1485
    },
    {
      "epoch": 0.37383177570093457,
      "grad_norm": 2.61637544631958,
      "learning_rate": 0.00013922497909116255,
      "loss": 1.5809,
      "step": 1490
    },
    {
      "epoch": 0.3750862447469109,
      "grad_norm": 1.566248893737793,
      "learning_rate": 0.0001389461945915807,
      "loss": 1.5904,
      "step": 1495
    },
    {
      "epoch": 0.37634071379288714,
      "grad_norm": 1.7143135070800781,
      "learning_rate": 0.00013866741009199888,
      "loss": 1.5475,
      "step": 1500
    },
    {
      "epoch": 0.37759518283886345,
      "grad_norm": 1.7530694007873535,
      "learning_rate": 0.00013838862559241707,
      "loss": 1.6264,
      "step": 1505
    },
    {
      "epoch": 0.37884965188483977,
      "grad_norm": 1.6328117847442627,
      "learning_rate": 0.00013810984109283525,
      "loss": 1.6084,
      "step": 1510
    },
    {
      "epoch": 0.380104120930816,
      "grad_norm": 1.8683792352676392,
      "learning_rate": 0.00013783105659325343,
      "loss": 1.5359,
      "step": 1515
    },
    {
      "epoch": 0.38135858997679234,
      "grad_norm": 1.765854001045227,
      "learning_rate": 0.0001375522720936716,
      "loss": 1.6125,
      "step": 1520
    },
    {
      "epoch": 0.3826130590227686,
      "grad_norm": 1.783743143081665,
      "learning_rate": 0.0001372734875940898,
      "loss": 1.687,
      "step": 1525
    },
    {
      "epoch": 0.3838675280687449,
      "grad_norm": 1.7782588005065918,
      "learning_rate": 0.00013699470309450794,
      "loss": 1.691,
      "step": 1530
    },
    {
      "epoch": 0.38512199711472117,
      "grad_norm": 1.6240943670272827,
      "learning_rate": 0.00013671591859492613,
      "loss": 1.6057,
      "step": 1535
    },
    {
      "epoch": 0.3863764661606975,
      "grad_norm": 1.6810225248336792,
      "learning_rate": 0.0001364371340953443,
      "loss": 1.6694,
      "step": 1540
    },
    {
      "epoch": 0.3876309352066738,
      "grad_norm": 16.948509216308594,
      "learning_rate": 0.0001361583495957625,
      "loss": 1.6062,
      "step": 1545
    },
    {
      "epoch": 0.38888540425265006,
      "grad_norm": 1.6407891511917114,
      "learning_rate": 0.00013587956509618064,
      "loss": 1.6119,
      "step": 1550
    },
    {
      "epoch": 0.39013987329862637,
      "grad_norm": 2.083441972732544,
      "learning_rate": 0.00013560078059659885,
      "loss": 1.642,
      "step": 1555
    },
    {
      "epoch": 0.39139434234460263,
      "grad_norm": 2.034794569015503,
      "learning_rate": 0.000135321996097017,
      "loss": 1.6859,
      "step": 1560
    },
    {
      "epoch": 0.39264881139057894,
      "grad_norm": 1.5966172218322754,
      "learning_rate": 0.00013504321159743519,
      "loss": 1.569,
      "step": 1565
    },
    {
      "epoch": 0.3939032804365552,
      "grad_norm": 1.745413899421692,
      "learning_rate": 0.00013476442709785337,
      "loss": 1.6435,
      "step": 1570
    },
    {
      "epoch": 0.3951577494825315,
      "grad_norm": 1.9672809839248657,
      "learning_rate": 0.00013448564259827155,
      "loss": 1.6315,
      "step": 1575
    },
    {
      "epoch": 0.39641221852850783,
      "grad_norm": 1.7696527242660522,
      "learning_rate": 0.0001342068580986897,
      "loss": 1.5861,
      "step": 1580
    },
    {
      "epoch": 0.3976666875744841,
      "grad_norm": 1.6173467636108398,
      "learning_rate": 0.0001339280735991079,
      "loss": 1.5939,
      "step": 1585
    },
    {
      "epoch": 0.3989211566204604,
      "grad_norm": 1.7929608821868896,
      "learning_rate": 0.00013364928909952606,
      "loss": 1.635,
      "step": 1590
    },
    {
      "epoch": 0.40017562566643666,
      "grad_norm": 1.9891796112060547,
      "learning_rate": 0.00013337050459994425,
      "loss": 1.6469,
      "step": 1595
    },
    {
      "epoch": 0.401430094712413,
      "grad_norm": 1.8855159282684326,
      "learning_rate": 0.00013309172010036243,
      "loss": 1.587,
      "step": 1600
    },
    {
      "epoch": 0.40268456375838924,
      "grad_norm": 1.476567029953003,
      "learning_rate": 0.0001328129356007806,
      "loss": 1.5808,
      "step": 1605
    },
    {
      "epoch": 0.40393903280436555,
      "grad_norm": 1.9572749137878418,
      "learning_rate": 0.0001325341511011988,
      "loss": 1.6385,
      "step": 1610
    },
    {
      "epoch": 0.40519350185034186,
      "grad_norm": 1.5309637784957886,
      "learning_rate": 0.00013225536660161694,
      "loss": 1.5057,
      "step": 1615
    },
    {
      "epoch": 0.4064479708963181,
      "grad_norm": 1.6003170013427734,
      "learning_rate": 0.00013197658210203515,
      "loss": 1.6058,
      "step": 1620
    },
    {
      "epoch": 0.40770243994229444,
      "grad_norm": 1.749345064163208,
      "learning_rate": 0.0001316977976024533,
      "loss": 1.6606,
      "step": 1625
    },
    {
      "epoch": 0.4089569089882707,
      "grad_norm": 2.0618927478790283,
      "learning_rate": 0.00013141901310287149,
      "loss": 1.706,
      "step": 1630
    },
    {
      "epoch": 0.410211378034247,
      "grad_norm": 1.9557620286941528,
      "learning_rate": 0.00013114022860328967,
      "loss": 1.642,
      "step": 1635
    },
    {
      "epoch": 0.41146584708022327,
      "grad_norm": 1.469817876815796,
      "learning_rate": 0.00013086144410370785,
      "loss": 1.4971,
      "step": 1640
    },
    {
      "epoch": 0.4127203161261996,
      "grad_norm": 1.5075334310531616,
      "learning_rate": 0.000130582659604126,
      "loss": 1.6035,
      "step": 1645
    },
    {
      "epoch": 0.4139747851721759,
      "grad_norm": 1.806117057800293,
      "learning_rate": 0.0001303038751045442,
      "loss": 1.663,
      "step": 1650
    },
    {
      "epoch": 0.41522925421815216,
      "grad_norm": 1.615362286567688,
      "learning_rate": 0.00013002509060496236,
      "loss": 1.577,
      "step": 1655
    },
    {
      "epoch": 0.41648372326412847,
      "grad_norm": 2.084035634994507,
      "learning_rate": 0.00012974630610538055,
      "loss": 1.6045,
      "step": 1660
    },
    {
      "epoch": 0.41773819231010473,
      "grad_norm": 1.8468501567840576,
      "learning_rate": 0.00012946752160579873,
      "loss": 1.6064,
      "step": 1665
    },
    {
      "epoch": 0.41899266135608104,
      "grad_norm": 1.816624402999878,
      "learning_rate": 0.0001291887371062169,
      "loss": 1.663,
      "step": 1670
    },
    {
      "epoch": 0.42024713040205736,
      "grad_norm": 1.5089259147644043,
      "learning_rate": 0.0001289099526066351,
      "loss": 1.5608,
      "step": 1675
    },
    {
      "epoch": 0.4215015994480336,
      "grad_norm": 2.044510841369629,
      "learning_rate": 0.00012863116810705324,
      "loss": 1.6166,
      "step": 1680
    },
    {
      "epoch": 0.42275606849400993,
      "grad_norm": 1.6449106931686401,
      "learning_rate": 0.00012835238360747145,
      "loss": 1.5683,
      "step": 1685
    },
    {
      "epoch": 0.4240105375399862,
      "grad_norm": 1.4553724527359009,
      "learning_rate": 0.0001280735991078896,
      "loss": 1.4964,
      "step": 1690
    },
    {
      "epoch": 0.4252650065859625,
      "grad_norm": 1.9654874801635742,
      "learning_rate": 0.0001277948146083078,
      "loss": 1.6293,
      "step": 1695
    },
    {
      "epoch": 0.42651947563193876,
      "grad_norm": 1.664455771446228,
      "learning_rate": 0.00012751603010872597,
      "loss": 1.5895,
      "step": 1700
    },
    {
      "epoch": 0.4277739446779151,
      "grad_norm": 1.7400180101394653,
      "learning_rate": 0.00012723724560914415,
      "loss": 1.5367,
      "step": 1705
    },
    {
      "epoch": 0.4290284137238914,
      "grad_norm": 1.620496392250061,
      "learning_rate": 0.0001269584611095623,
      "loss": 1.5927,
      "step": 1710
    },
    {
      "epoch": 0.43028288276986765,
      "grad_norm": 2.1480002403259277,
      "learning_rate": 0.0001266796766099805,
      "loss": 1.6247,
      "step": 1715
    },
    {
      "epoch": 0.43153735181584396,
      "grad_norm": 1.8104326725006104,
      "learning_rate": 0.00012640089211039867,
      "loss": 1.5173,
      "step": 1720
    },
    {
      "epoch": 0.4327918208618202,
      "grad_norm": 1.8558257818222046,
      "learning_rate": 0.00012612210761081685,
      "loss": 1.6568,
      "step": 1725
    },
    {
      "epoch": 0.43404628990779653,
      "grad_norm": 1.9526532888412476,
      "learning_rate": 0.000125843323111235,
      "loss": 1.5823,
      "step": 1730
    },
    {
      "epoch": 0.4353007589537728,
      "grad_norm": 1.511832356452942,
      "learning_rate": 0.0001255645386116532,
      "loss": 1.5726,
      "step": 1735
    },
    {
      "epoch": 0.4365552279997491,
      "grad_norm": 3.478926658630371,
      "learning_rate": 0.00012528575411207136,
      "loss": 1.5287,
      "step": 1740
    },
    {
      "epoch": 0.4378096970457254,
      "grad_norm": 1.6181970834732056,
      "learning_rate": 0.00012500696961248954,
      "loss": 1.5426,
      "step": 1745
    },
    {
      "epoch": 0.4390641660917017,
      "grad_norm": 1.8830581903457642,
      "learning_rate": 0.00012472818511290773,
      "loss": 1.5609,
      "step": 1750
    },
    {
      "epoch": 0.440318635137678,
      "grad_norm": 1.4466880559921265,
      "learning_rate": 0.0001244494006133259,
      "loss": 1.5297,
      "step": 1755
    },
    {
      "epoch": 0.44157310418365425,
      "grad_norm": 1.9963891506195068,
      "learning_rate": 0.0001241706161137441,
      "loss": 1.5765,
      "step": 1760
    },
    {
      "epoch": 0.44282757322963057,
      "grad_norm": 1.6092082262039185,
      "learning_rate": 0.00012389183161416227,
      "loss": 1.6213,
      "step": 1765
    },
    {
      "epoch": 0.4440820422756068,
      "grad_norm": 1.7526721954345703,
      "learning_rate": 0.00012361304711458045,
      "loss": 1.5408,
      "step": 1770
    },
    {
      "epoch": 0.44533651132158314,
      "grad_norm": 1.7513006925582886,
      "learning_rate": 0.0001233342626149986,
      "loss": 1.5642,
      "step": 1775
    },
    {
      "epoch": 0.44659098036755945,
      "grad_norm": 1.7749135494232178,
      "learning_rate": 0.00012305547811541678,
      "loss": 1.5019,
      "step": 1780
    },
    {
      "epoch": 0.4478454494135357,
      "grad_norm": 1.7779805660247803,
      "learning_rate": 0.00012277669361583497,
      "loss": 1.5428,
      "step": 1785
    },
    {
      "epoch": 0.449099918459512,
      "grad_norm": 2.033297300338745,
      "learning_rate": 0.00012249790911625315,
      "loss": 1.7151,
      "step": 1790
    },
    {
      "epoch": 0.4503543875054883,
      "grad_norm": 1.7537617683410645,
      "learning_rate": 0.0001222191246166713,
      "loss": 1.5664,
      "step": 1795
    },
    {
      "epoch": 0.4516088565514646,
      "grad_norm": 1.6016541719436646,
      "learning_rate": 0.0001219403401170895,
      "loss": 1.5182,
      "step": 1800
    },
    {
      "epoch": 0.45286332559744086,
      "grad_norm": 1.8089118003845215,
      "learning_rate": 0.00012166155561750768,
      "loss": 1.51,
      "step": 1805
    },
    {
      "epoch": 0.45411779464341717,
      "grad_norm": 1.9558982849121094,
      "learning_rate": 0.00012138277111792584,
      "loss": 1.5701,
      "step": 1810
    },
    {
      "epoch": 0.4553722636893935,
      "grad_norm": 1.8000240325927734,
      "learning_rate": 0.00012110398661834404,
      "loss": 1.6182,
      "step": 1815
    },
    {
      "epoch": 0.45662673273536974,
      "grad_norm": 1.679664969444275,
      "learning_rate": 0.00012082520211876221,
      "loss": 1.5733,
      "step": 1820
    },
    {
      "epoch": 0.45788120178134606,
      "grad_norm": 1.7305753231048584,
      "learning_rate": 0.00012054641761918037,
      "loss": 1.6052,
      "step": 1825
    },
    {
      "epoch": 0.4591356708273223,
      "grad_norm": 1.6860376596450806,
      "learning_rate": 0.00012026763311959857,
      "loss": 1.6734,
      "step": 1830
    },
    {
      "epoch": 0.46039013987329863,
      "grad_norm": 2.2306668758392334,
      "learning_rate": 0.00011998884862001674,
      "loss": 1.6007,
      "step": 1835
    },
    {
      "epoch": 0.4616446089192749,
      "grad_norm": 2.0438623428344727,
      "learning_rate": 0.0001197100641204349,
      "loss": 1.5759,
      "step": 1840
    },
    {
      "epoch": 0.4628990779652512,
      "grad_norm": 1.7946553230285645,
      "learning_rate": 0.00011943127962085307,
      "loss": 1.6036,
      "step": 1845
    },
    {
      "epoch": 0.4641535470112275,
      "grad_norm": 1.8840603828430176,
      "learning_rate": 0.00011915249512127127,
      "loss": 1.593,
      "step": 1850
    },
    {
      "epoch": 0.4654080160572038,
      "grad_norm": 2.606351852416992,
      "learning_rate": 0.00011887371062168943,
      "loss": 1.6269,
      "step": 1855
    },
    {
      "epoch": 0.4666624851031801,
      "grad_norm": 2.049088954925537,
      "learning_rate": 0.0001185949261221076,
      "loss": 1.5879,
      "step": 1860
    },
    {
      "epoch": 0.46791695414915635,
      "grad_norm": 1.8628287315368652,
      "learning_rate": 0.0001183161416225258,
      "loss": 1.6369,
      "step": 1865
    },
    {
      "epoch": 0.46917142319513266,
      "grad_norm": 1.6227308511734009,
      "learning_rate": 0.00011803735712294396,
      "loss": 1.5987,
      "step": 1870
    },
    {
      "epoch": 0.470425892241109,
      "grad_norm": 1.6882938146591187,
      "learning_rate": 0.00011775857262336215,
      "loss": 1.5416,
      "step": 1875
    },
    {
      "epoch": 0.47168036128708524,
      "grad_norm": 1.5420031547546387,
      "learning_rate": 0.00011747978812378033,
      "loss": 1.4681,
      "step": 1880
    },
    {
      "epoch": 0.47293483033306155,
      "grad_norm": 1.8430618047714233,
      "learning_rate": 0.00011720100362419851,
      "loss": 1.5881,
      "step": 1885
    },
    {
      "epoch": 0.4741892993790378,
      "grad_norm": 1.5588006973266602,
      "learning_rate": 0.00011692221912461668,
      "loss": 1.5573,
      "step": 1890
    },
    {
      "epoch": 0.4754437684250141,
      "grad_norm": 1.5825518369674683,
      "learning_rate": 0.00011664343462503487,
      "loss": 1.4427,
      "step": 1895
    },
    {
      "epoch": 0.4766982374709904,
      "grad_norm": 1.939867615699768,
      "learning_rate": 0.00011636465012545304,
      "loss": 1.5852,
      "step": 1900
    },
    {
      "epoch": 0.4779527065169667,
      "grad_norm": 1.7592788934707642,
      "learning_rate": 0.0001160858656258712,
      "loss": 1.5878,
      "step": 1905
    },
    {
      "epoch": 0.479207175562943,
      "grad_norm": 1.8558201789855957,
      "learning_rate": 0.00011580708112628937,
      "loss": 1.6582,
      "step": 1910
    },
    {
      "epoch": 0.48046164460891927,
      "grad_norm": 1.9487053155899048,
      "learning_rate": 0.00011552829662670757,
      "loss": 1.5135,
      "step": 1915
    },
    {
      "epoch": 0.4817161136548956,
      "grad_norm": 1.7995846271514893,
      "learning_rate": 0.00011524951212712573,
      "loss": 1.5339,
      "step": 1920
    },
    {
      "epoch": 0.48297058270087184,
      "grad_norm": 1.6064162254333496,
      "learning_rate": 0.0001149707276275439,
      "loss": 1.5343,
      "step": 1925
    },
    {
      "epoch": 0.48422505174684816,
      "grad_norm": 2.057915210723877,
      "learning_rate": 0.0001146919431279621,
      "loss": 1.5678,
      "step": 1930
    },
    {
      "epoch": 0.4854795207928244,
      "grad_norm": 1.7145236730575562,
      "learning_rate": 0.00011441315862838026,
      "loss": 1.6263,
      "step": 1935
    },
    {
      "epoch": 0.48673398983880073,
      "grad_norm": 1.5261329412460327,
      "learning_rate": 0.00011413437412879843,
      "loss": 1.5442,
      "step": 1940
    },
    {
      "epoch": 0.48798845888477704,
      "grad_norm": 1.452208399772644,
      "learning_rate": 0.00011385558962921663,
      "loss": 1.5895,
      "step": 1945
    },
    {
      "epoch": 0.4892429279307533,
      "grad_norm": 1.679078221321106,
      "learning_rate": 0.0001135768051296348,
      "loss": 1.5053,
      "step": 1950
    },
    {
      "epoch": 0.4904973969767296,
      "grad_norm": 1.9442126750946045,
      "learning_rate": 0.00011329802063005298,
      "loss": 1.5473,
      "step": 1955
    },
    {
      "epoch": 0.4917518660227059,
      "grad_norm": 1.610682725906372,
      "learning_rate": 0.00011301923613047116,
      "loss": 1.5115,
      "step": 1960
    },
    {
      "epoch": 0.4930063350686822,
      "grad_norm": 1.9989380836486816,
      "learning_rate": 0.00011274045163088934,
      "loss": 1.6276,
      "step": 1965
    },
    {
      "epoch": 0.49426080411465845,
      "grad_norm": 1.860224962234497,
      "learning_rate": 0.0001124616671313075,
      "loss": 1.4632,
      "step": 1970
    },
    {
      "epoch": 0.49551527316063476,
      "grad_norm": 1.7446688413619995,
      "learning_rate": 0.00011218288263172567,
      "loss": 1.5393,
      "step": 1975
    },
    {
      "epoch": 0.4967697422066111,
      "grad_norm": 1.6215356588363647,
      "learning_rate": 0.00011190409813214387,
      "loss": 1.4447,
      "step": 1980
    },
    {
      "epoch": 0.49802421125258733,
      "grad_norm": 1.8915698528289795,
      "learning_rate": 0.00011162531363256204,
      "loss": 1.6007,
      "step": 1985
    },
    {
      "epoch": 0.49927868029856365,
      "grad_norm": 1.8725160360336304,
      "learning_rate": 0.0001113465291329802,
      "loss": 1.6251,
      "step": 1990
    },
    {
      "epoch": 0.50053314934454,
      "grad_norm": 1.6327500343322754,
      "learning_rate": 0.0001110677446333984,
      "loss": 1.6209,
      "step": 1995
    },
    {
      "epoch": 0.5017876183905162,
      "grad_norm": 1.485655665397644,
      "learning_rate": 0.00011078896013381657,
      "loss": 1.4765,
      "step": 2000
    },
    {
      "epoch": 0.5030420874364925,
      "grad_norm": 1.6838631629943848,
      "learning_rate": 0.00011051017563423473,
      "loss": 1.4971,
      "step": 2005
    },
    {
      "epoch": 0.5042965564824688,
      "grad_norm": 2.105912685394287,
      "learning_rate": 0.00011023139113465293,
      "loss": 1.5758,
      "step": 2010
    },
    {
      "epoch": 0.5055510255284451,
      "grad_norm": 1.7158699035644531,
      "learning_rate": 0.0001099526066350711,
      "loss": 1.5384,
      "step": 2015
    },
    {
      "epoch": 0.5068054945744214,
      "grad_norm": 2.036625862121582,
      "learning_rate": 0.00010967382213548926,
      "loss": 1.5233,
      "step": 2020
    },
    {
      "epoch": 0.5080599636203976,
      "grad_norm": 1.7438721656799316,
      "learning_rate": 0.00010939503763590744,
      "loss": 1.5394,
      "step": 2025
    },
    {
      "epoch": 0.509314432666374,
      "grad_norm": 1.7064036130905151,
      "learning_rate": 0.00010911625313632563,
      "loss": 1.599,
      "step": 2030
    },
    {
      "epoch": 0.5105689017123503,
      "grad_norm": 1.6606018543243408,
      "learning_rate": 0.0001088374686367438,
      "loss": 1.508,
      "step": 2035
    },
    {
      "epoch": 0.5118233707583265,
      "grad_norm": 1.7972254753112793,
      "learning_rate": 0.00010855868413716197,
      "loss": 1.5253,
      "step": 2040
    },
    {
      "epoch": 0.5130778398043029,
      "grad_norm": 2.0007007122039795,
      "learning_rate": 0.00010827989963758017,
      "loss": 1.6134,
      "step": 2045
    },
    {
      "epoch": 0.5143323088502791,
      "grad_norm": 1.962728500366211,
      "learning_rate": 0.00010800111513799834,
      "loss": 1.4892,
      "step": 2050
    },
    {
      "epoch": 0.5155867778962554,
      "grad_norm": 1.8548332452774048,
      "learning_rate": 0.0001077223306384165,
      "loss": 1.5342,
      "step": 2055
    },
    {
      "epoch": 0.5168412469422317,
      "grad_norm": 2.013817548751831,
      "learning_rate": 0.0001074435461388347,
      "loss": 1.4766,
      "step": 2060
    },
    {
      "epoch": 0.518095715988208,
      "grad_norm": 1.7309274673461914,
      "learning_rate": 0.00010716476163925287,
      "loss": 1.5149,
      "step": 2065
    },
    {
      "epoch": 0.5193501850341843,
      "grad_norm": 1.8693091869354248,
      "learning_rate": 0.00010688597713967103,
      "loss": 1.4993,
      "step": 2070
    },
    {
      "epoch": 0.5206046540801605,
      "grad_norm": 1.5966943502426147,
      "learning_rate": 0.00010660719264008923,
      "loss": 1.5092,
      "step": 2075
    },
    {
      "epoch": 0.5218591231261369,
      "grad_norm": 1.6452528238296509,
      "learning_rate": 0.0001063284081405074,
      "loss": 1.4726,
      "step": 2080
    },
    {
      "epoch": 0.5231135921721132,
      "grad_norm": 2.0056068897247314,
      "learning_rate": 0.00010604962364092556,
      "loss": 1.5659,
      "step": 2085
    },
    {
      "epoch": 0.5243680612180894,
      "grad_norm": 1.6949278116226196,
      "learning_rate": 0.00010577083914134373,
      "loss": 1.5523,
      "step": 2090
    },
    {
      "epoch": 0.5256225302640657,
      "grad_norm": 1.2616167068481445,
      "learning_rate": 0.00010549205464176193,
      "loss": 1.4819,
      "step": 2095
    },
    {
      "epoch": 0.5268769993100421,
      "grad_norm": 1.6745471954345703,
      "learning_rate": 0.0001052132701421801,
      "loss": 1.5138,
      "step": 2100
    },
    {
      "epoch": 0.5281314683560183,
      "grad_norm": 1.781965732574463,
      "learning_rate": 0.00010493448564259827,
      "loss": 1.4657,
      "step": 2105
    },
    {
      "epoch": 0.5293859374019946,
      "grad_norm": 1.608910322189331,
      "learning_rate": 0.00010465570114301646,
      "loss": 1.5365,
      "step": 2110
    },
    {
      "epoch": 0.530640406447971,
      "grad_norm": 1.7578625679016113,
      "learning_rate": 0.00010437691664343464,
      "loss": 1.5106,
      "step": 2115
    },
    {
      "epoch": 0.5318948754939472,
      "grad_norm": 1.8372209072113037,
      "learning_rate": 0.0001040981321438528,
      "loss": 1.4988,
      "step": 2120
    },
    {
      "epoch": 0.5331493445399235,
      "grad_norm": 1.9951471090316772,
      "learning_rate": 0.00010381934764427099,
      "loss": 1.5156,
      "step": 2125
    },
    {
      "epoch": 0.5344038135858997,
      "grad_norm": 1.875085711479187,
      "learning_rate": 0.00010354056314468917,
      "loss": 1.5446,
      "step": 2130
    },
    {
      "epoch": 0.5356582826318761,
      "grad_norm": 1.7733670473098755,
      "learning_rate": 0.00010326177864510733,
      "loss": 1.5395,
      "step": 2135
    },
    {
      "epoch": 0.5369127516778524,
      "grad_norm": 1.457724928855896,
      "learning_rate": 0.00010298299414552553,
      "loss": 1.464,
      "step": 2140
    },
    {
      "epoch": 0.5381672207238286,
      "grad_norm": 1.4960108995437622,
      "learning_rate": 0.0001027042096459437,
      "loss": 1.5085,
      "step": 2145
    },
    {
      "epoch": 0.539421689769805,
      "grad_norm": 1.7301265001296997,
      "learning_rate": 0.00010242542514636186,
      "loss": 1.4931,
      "step": 2150
    },
    {
      "epoch": 0.5406761588157812,
      "grad_norm": 2.1541378498077393,
      "learning_rate": 0.00010214664064678003,
      "loss": 1.5827,
      "step": 2155
    },
    {
      "epoch": 0.5419306278617575,
      "grad_norm": 1.6953366994857788,
      "learning_rate": 0.00010186785614719823,
      "loss": 1.4873,
      "step": 2160
    },
    {
      "epoch": 0.5431850969077338,
      "grad_norm": 1.770772933959961,
      "learning_rate": 0.0001015890716476164,
      "loss": 1.5281,
      "step": 2165
    },
    {
      "epoch": 0.5444395659537101,
      "grad_norm": 1.8876218795776367,
      "learning_rate": 0.00010131028714803456,
      "loss": 1.513,
      "step": 2170
    },
    {
      "epoch": 0.5456940349996864,
      "grad_norm": 1.8507603406906128,
      "learning_rate": 0.00010103150264845276,
      "loss": 1.573,
      "step": 2175
    },
    {
      "epoch": 0.5469485040456626,
      "grad_norm": 1.8148856163024902,
      "learning_rate": 0.00010075271814887092,
      "loss": 1.5228,
      "step": 2180
    },
    {
      "epoch": 0.548202973091639,
      "grad_norm": 1.8263763189315796,
      "learning_rate": 0.00010047393364928909,
      "loss": 1.4567,
      "step": 2185
    },
    {
      "epoch": 0.5494574421376153,
      "grad_norm": 1.7174385786056519,
      "learning_rate": 0.00010019514914970729,
      "loss": 1.4499,
      "step": 2190
    },
    {
      "epoch": 0.5507119111835915,
      "grad_norm": 1.9660890102386475,
      "learning_rate": 9.991636465012545e-05,
      "loss": 1.4978,
      "step": 2195
    },
    {
      "epoch": 0.5519663802295678,
      "grad_norm": 1.7448723316192627,
      "learning_rate": 9.963758015054363e-05,
      "loss": 1.478,
      "step": 2200
    },
    {
      "epoch": 0.5532208492755442,
      "grad_norm": 1.703122615814209,
      "learning_rate": 9.935879565096182e-05,
      "loss": 1.5047,
      "step": 2205
    },
    {
      "epoch": 0.5544753183215204,
      "grad_norm": 1.7582722902297974,
      "learning_rate": 9.908001115138e-05,
      "loss": 1.5367,
      "step": 2210
    },
    {
      "epoch": 0.5557297873674967,
      "grad_norm": 1.7954161167144775,
      "learning_rate": 9.880122665179816e-05,
      "loss": 1.4899,
      "step": 2215
    },
    {
      "epoch": 0.556984256413473,
      "grad_norm": 1.6216354370117188,
      "learning_rate": 9.852244215221635e-05,
      "loss": 1.4251,
      "step": 2220
    },
    {
      "epoch": 0.5582387254594493,
      "grad_norm": 1.6841858625411987,
      "learning_rate": 9.824365765263451e-05,
      "loss": 1.5515,
      "step": 2225
    },
    {
      "epoch": 0.5594931945054256,
      "grad_norm": 1.847439169883728,
      "learning_rate": 9.79648731530527e-05,
      "loss": 1.546,
      "step": 2230
    },
    {
      "epoch": 0.5607476635514018,
      "grad_norm": 1.8237285614013672,
      "learning_rate": 9.768608865347088e-05,
      "loss": 1.4577,
      "step": 2235
    },
    {
      "epoch": 0.5620021325973782,
      "grad_norm": 1.550959587097168,
      "learning_rate": 9.740730415388904e-05,
      "loss": 1.5397,
      "step": 2240
    },
    {
      "epoch": 0.5632566016433544,
      "grad_norm": 2.0919864177703857,
      "learning_rate": 9.712851965430722e-05,
      "loss": 1.5715,
      "step": 2245
    },
    {
      "epoch": 0.5645110706893307,
      "grad_norm": 1.6619282960891724,
      "learning_rate": 9.68497351547254e-05,
      "loss": 1.3984,
      "step": 2250
    },
    {
      "epoch": 0.5657655397353071,
      "grad_norm": 1.7022043466567993,
      "learning_rate": 9.657095065514357e-05,
      "loss": 1.506,
      "step": 2255
    },
    {
      "epoch": 0.5670200087812833,
      "grad_norm": 1.5372856855392456,
      "learning_rate": 9.629216615556175e-05,
      "loss": 1.5153,
      "step": 2260
    },
    {
      "epoch": 0.5682744778272596,
      "grad_norm": 1.7792476415634155,
      "learning_rate": 9.601338165597992e-05,
      "loss": 1.5044,
      "step": 2265
    },
    {
      "epoch": 0.5695289468732359,
      "grad_norm": 1.8689391613006592,
      "learning_rate": 9.57345971563981e-05,
      "loss": 1.5179,
      "step": 2270
    },
    {
      "epoch": 0.5707834159192122,
      "grad_norm": 2.2360386848449707,
      "learning_rate": 9.545581265681628e-05,
      "loss": 1.4871,
      "step": 2275
    },
    {
      "epoch": 0.5720378849651885,
      "grad_norm": 1.885640025138855,
      "learning_rate": 9.517702815723447e-05,
      "loss": 1.5182,
      "step": 2280
    },
    {
      "epoch": 0.5732923540111647,
      "grad_norm": 1.8321630954742432,
      "learning_rate": 9.489824365765265e-05,
      "loss": 1.4684,
      "step": 2285
    },
    {
      "epoch": 0.5745468230571411,
      "grad_norm": 1.476534366607666,
      "learning_rate": 9.461945915807081e-05,
      "loss": 1.5258,
      "step": 2290
    },
    {
      "epoch": 0.5758012921031174,
      "grad_norm": 1.955039143562317,
      "learning_rate": 9.4340674658489e-05,
      "loss": 1.5809,
      "step": 2295
    },
    {
      "epoch": 0.5770557611490936,
      "grad_norm": 1.8638811111450195,
      "learning_rate": 9.406189015890718e-05,
      "loss": 1.5619,
      "step": 2300
    },
    {
      "epoch": 0.5783102301950699,
      "grad_norm": 1.9259756803512573,
      "learning_rate": 9.378310565932534e-05,
      "loss": 1.5464,
      "step": 2305
    },
    {
      "epoch": 0.5795646992410463,
      "grad_norm": 1.6905441284179688,
      "learning_rate": 9.350432115974353e-05,
      "loss": 1.4745,
      "step": 2310
    },
    {
      "epoch": 0.5808191682870225,
      "grad_norm": 1.6947731971740723,
      "learning_rate": 9.322553666016169e-05,
      "loss": 1.5751,
      "step": 2315
    },
    {
      "epoch": 0.5820736373329988,
      "grad_norm": 1.6318566799163818,
      "learning_rate": 9.294675216057987e-05,
      "loss": 1.4576,
      "step": 2320
    },
    {
      "epoch": 0.5833281063789751,
      "grad_norm": 1.6140421628952026,
      "learning_rate": 9.266796766099805e-05,
      "loss": 1.4394,
      "step": 2325
    },
    {
      "epoch": 0.5845825754249514,
      "grad_norm": 1.834080457687378,
      "learning_rate": 9.238918316141622e-05,
      "loss": 1.5331,
      "step": 2330
    },
    {
      "epoch": 0.5858370444709277,
      "grad_norm": 1.756449818611145,
      "learning_rate": 9.21103986618344e-05,
      "loss": 1.4085,
      "step": 2335
    },
    {
      "epoch": 0.5870915135169039,
      "grad_norm": 2.122802257537842,
      "learning_rate": 9.183161416225258e-05,
      "loss": 1.4183,
      "step": 2340
    },
    {
      "epoch": 0.5883459825628803,
      "grad_norm": 1.9618897438049316,
      "learning_rate": 9.155282966267075e-05,
      "loss": 1.4907,
      "step": 2345
    },
    {
      "epoch": 0.5896004516088565,
      "grad_norm": 1.611991286277771,
      "learning_rate": 9.127404516308893e-05,
      "loss": 1.4474,
      "step": 2350
    },
    {
      "epoch": 0.5908549206548328,
      "grad_norm": 1.8242735862731934,
      "learning_rate": 9.099526066350711e-05,
      "loss": 1.4318,
      "step": 2355
    },
    {
      "epoch": 0.5921093897008092,
      "grad_norm": 1.678141474723816,
      "learning_rate": 9.07164761639253e-05,
      "loss": 1.4736,
      "step": 2360
    },
    {
      "epoch": 0.5933638587467854,
      "grad_norm": 1.7510193586349487,
      "learning_rate": 9.043769166434348e-05,
      "loss": 1.4334,
      "step": 2365
    },
    {
      "epoch": 0.5946183277927617,
      "grad_norm": 2.000408411026001,
      "learning_rate": 9.015890716476164e-05,
      "loss": 1.5425,
      "step": 2370
    },
    {
      "epoch": 0.595872796838738,
      "grad_norm": 1.770009160041809,
      "learning_rate": 8.988012266517983e-05,
      "loss": 1.4982,
      "step": 2375
    },
    {
      "epoch": 0.5971272658847143,
      "grad_norm": 2.0010862350463867,
      "learning_rate": 8.9601338165598e-05,
      "loss": 1.53,
      "step": 2380
    },
    {
      "epoch": 0.5983817349306906,
      "grad_norm": 1.7962405681610107,
      "learning_rate": 8.932255366601617e-05,
      "loss": 1.4583,
      "step": 2385
    },
    {
      "epoch": 0.5996362039766668,
      "grad_norm": 1.361533522605896,
      "learning_rate": 8.904376916643436e-05,
      "loss": 1.503,
      "step": 2390
    },
    {
      "epoch": 0.6008906730226432,
      "grad_norm": 2.028146982192993,
      "learning_rate": 8.876498466685252e-05,
      "loss": 1.5917,
      "step": 2395
    },
    {
      "epoch": 0.6021451420686195,
      "grad_norm": 1.822441577911377,
      "learning_rate": 8.84862001672707e-05,
      "loss": 1.4685,
      "step": 2400
    },
    {
      "epoch": 0.6033996111145957,
      "grad_norm": 1.8898284435272217,
      "learning_rate": 8.820741566768887e-05,
      "loss": 1.5985,
      "step": 2405
    },
    {
      "epoch": 0.604654080160572,
      "grad_norm": 1.8564144372940063,
      "learning_rate": 8.792863116810705e-05,
      "loss": 1.5534,
      "step": 2410
    },
    {
      "epoch": 0.6059085492065484,
      "grad_norm": 1.6725417375564575,
      "learning_rate": 8.764984666852523e-05,
      "loss": 1.4699,
      "step": 2415
    },
    {
      "epoch": 0.6071630182525246,
      "grad_norm": 6.611977577209473,
      "learning_rate": 8.73710621689434e-05,
      "loss": 1.5397,
      "step": 2420
    },
    {
      "epoch": 0.6084174872985009,
      "grad_norm": 1.8782932758331299,
      "learning_rate": 8.709227766936158e-05,
      "loss": 1.4917,
      "step": 2425
    },
    {
      "epoch": 0.6096719563444772,
      "grad_norm": 8.016571044921875,
      "learning_rate": 8.681349316977976e-05,
      "loss": 1.4944,
      "step": 2430
    },
    {
      "epoch": 0.6109264253904535,
      "grad_norm": 2.782437562942505,
      "learning_rate": 8.653470867019795e-05,
      "loss": 1.547,
      "step": 2435
    },
    {
      "epoch": 0.6121808944364298,
      "grad_norm": 10.41448974609375,
      "learning_rate": 8.625592417061613e-05,
      "loss": 1.5418,
      "step": 2440
    },
    {
      "epoch": 0.6134353634824061,
      "grad_norm": 1.8744069337844849,
      "learning_rate": 8.59771396710343e-05,
      "loss": 1.5189,
      "step": 2445
    },
    {
      "epoch": 0.6146898325283824,
      "grad_norm": 1.6239203214645386,
      "learning_rate": 8.569835517145248e-05,
      "loss": 1.4904,
      "step": 2450
    },
    {
      "epoch": 0.6159443015743586,
      "grad_norm": 2.5775558948516846,
      "learning_rate": 8.541957067187066e-05,
      "loss": 1.4376,
      "step": 2455
    },
    {
      "epoch": 0.6171987706203349,
      "grad_norm": 1.7254068851470947,
      "learning_rate": 8.514078617228882e-05,
      "loss": 1.4454,
      "step": 2460
    },
    {
      "epoch": 0.6184532396663113,
      "grad_norm": 6.736400127410889,
      "learning_rate": 8.4862001672707e-05,
      "loss": 1.3937,
      "step": 2465
    },
    {
      "epoch": 0.6197077087122875,
      "grad_norm": 1.7240198850631714,
      "learning_rate": 8.458321717312517e-05,
      "loss": 1.4735,
      "step": 2470
    },
    {
      "epoch": 0.6209621777582638,
      "grad_norm": 2.0387301445007324,
      "learning_rate": 8.430443267354335e-05,
      "loss": 1.4536,
      "step": 2475
    },
    {
      "epoch": 0.6222166468042402,
      "grad_norm": 2.144681453704834,
      "learning_rate": 8.402564817396153e-05,
      "loss": 1.5763,
      "step": 2480
    },
    {
      "epoch": 0.6234711158502164,
      "grad_norm": 1.7959896326065063,
      "learning_rate": 8.37468636743797e-05,
      "loss": 1.5293,
      "step": 2485
    },
    {
      "epoch": 0.6247255848961927,
      "grad_norm": 1.8304764032363892,
      "learning_rate": 8.346807917479788e-05,
      "loss": 1.4342,
      "step": 2490
    },
    {
      "epoch": 0.6259800539421689,
      "grad_norm": 1.9515857696533203,
      "learning_rate": 8.318929467521606e-05,
      "loss": 1.4175,
      "step": 2495
    },
    {
      "epoch": 0.6272345229881453,
      "grad_norm": 1.6879572868347168,
      "learning_rate": 8.291051017563423e-05,
      "loss": 1.5069,
      "step": 2500
    },
    {
      "epoch": 0.6284889920341216,
      "grad_norm": 2.2906153202056885,
      "learning_rate": 8.263172567605241e-05,
      "loss": 1.512,
      "step": 2505
    },
    {
      "epoch": 0.6297434610800978,
      "grad_norm": 4.284633636474609,
      "learning_rate": 8.23529411764706e-05,
      "loss": 1.5332,
      "step": 2510
    },
    {
      "epoch": 0.6309979301260742,
      "grad_norm": 1.6919670104980469,
      "learning_rate": 8.207415667688878e-05,
      "loss": 1.4341,
      "step": 2515
    },
    {
      "epoch": 0.6322523991720504,
      "grad_norm": 1.8165102005004883,
      "learning_rate": 8.179537217730696e-05,
      "loss": 1.5079,
      "step": 2520
    },
    {
      "epoch": 0.6335068682180267,
      "grad_norm": 1.8371599912643433,
      "learning_rate": 8.151658767772512e-05,
      "loss": 1.4851,
      "step": 2525
    },
    {
      "epoch": 0.634761337264003,
      "grad_norm": 1.9038623571395874,
      "learning_rate": 8.12378031781433e-05,
      "loss": 1.4311,
      "step": 2530
    },
    {
      "epoch": 0.6360158063099793,
      "grad_norm": 2.1118061542510986,
      "learning_rate": 8.095901867856147e-05,
      "loss": 1.4338,
      "step": 2535
    },
    {
      "epoch": 0.6372702753559556,
      "grad_norm": 1.7153074741363525,
      "learning_rate": 8.068023417897965e-05,
      "loss": 1.4526,
      "step": 2540
    },
    {
      "epoch": 0.6385247444019319,
      "grad_norm": 2.0548224449157715,
      "learning_rate": 8.040144967939784e-05,
      "loss": 1.4429,
      "step": 2545
    },
    {
      "epoch": 0.6397792134479082,
      "grad_norm": 1.823674201965332,
      "learning_rate": 8.0122665179816e-05,
      "loss": 1.5443,
      "step": 2550
    },
    {
      "epoch": 0.6410336824938845,
      "grad_norm": 1.8636016845703125,
      "learning_rate": 7.984388068023418e-05,
      "loss": 1.5268,
      "step": 2555
    },
    {
      "epoch": 0.6422881515398607,
      "grad_norm": 1.7356541156768799,
      "learning_rate": 7.956509618065235e-05,
      "loss": 1.4364,
      "step": 2560
    },
    {
      "epoch": 0.643542620585837,
      "grad_norm": 1.6931977272033691,
      "learning_rate": 7.928631168107053e-05,
      "loss": 1.4253,
      "step": 2565
    },
    {
      "epoch": 0.6447970896318134,
      "grad_norm": 1.6241636276245117,
      "learning_rate": 7.900752718148871e-05,
      "loss": 1.4786,
      "step": 2570
    },
    {
      "epoch": 0.6460515586777896,
      "grad_norm": 1.538874864578247,
      "learning_rate": 7.872874268190688e-05,
      "loss": 1.3753,
      "step": 2575
    },
    {
      "epoch": 0.6473060277237659,
      "grad_norm": 1.7406299114227295,
      "learning_rate": 7.844995818232506e-05,
      "loss": 1.4346,
      "step": 2580
    },
    {
      "epoch": 0.6485604967697423,
      "grad_norm": 1.9774929285049438,
      "learning_rate": 7.817117368274324e-05,
      "loss": 1.4076,
      "step": 2585
    },
    {
      "epoch": 0.6498149658157185,
      "grad_norm": 1.7401713132858276,
      "learning_rate": 7.789238918316143e-05,
      "loss": 1.4774,
      "step": 2590
    },
    {
      "epoch": 0.6510694348616948,
      "grad_norm": 2.0278327465057373,
      "learning_rate": 7.761360468357959e-05,
      "loss": 1.525,
      "step": 2595
    },
    {
      "epoch": 0.652323903907671,
      "grad_norm": 1.7155312299728394,
      "learning_rate": 7.733482018399777e-05,
      "loss": 1.5339,
      "step": 2600
    },
    {
      "epoch": 0.6535783729536474,
      "grad_norm": 1.7707239389419556,
      "learning_rate": 7.705603568441595e-05,
      "loss": 1.5184,
      "step": 2605
    },
    {
      "epoch": 0.6548328419996237,
      "grad_norm": 1.7216042280197144,
      "learning_rate": 7.677725118483414e-05,
      "loss": 1.5335,
      "step": 2610
    },
    {
      "epoch": 0.6560873110455999,
      "grad_norm": 2.141602039337158,
      "learning_rate": 7.64984666852523e-05,
      "loss": 1.4687,
      "step": 2615
    },
    {
      "epoch": 0.6573417800915763,
      "grad_norm": 1.7288453578948975,
      "learning_rate": 7.621968218567048e-05,
      "loss": 1.4299,
      "step": 2620
    },
    {
      "epoch": 0.6585962491375525,
      "grad_norm": 1.8436803817749023,
      "learning_rate": 7.594089768608865e-05,
      "loss": 1.5152,
      "step": 2625
    },
    {
      "epoch": 0.6598507181835288,
      "grad_norm": 1.7426602840423584,
      "learning_rate": 7.566211318650683e-05,
      "loss": 1.4576,
      "step": 2630
    },
    {
      "epoch": 0.6611051872295051,
      "grad_norm": 1.9455689191818237,
      "learning_rate": 7.538332868692501e-05,
      "loss": 1.4585,
      "step": 2635
    },
    {
      "epoch": 0.6623596562754814,
      "grad_norm": 1.6677132844924927,
      "learning_rate": 7.510454418734318e-05,
      "loss": 1.4242,
      "step": 2640
    },
    {
      "epoch": 0.6636141253214577,
      "grad_norm": 2.0407629013061523,
      "learning_rate": 7.482575968776136e-05,
      "loss": 1.5011,
      "step": 2645
    },
    {
      "epoch": 0.664868594367434,
      "grad_norm": 1.6732772588729858,
      "learning_rate": 7.454697518817953e-05,
      "loss": 1.511,
      "step": 2650
    },
    {
      "epoch": 0.6661230634134103,
      "grad_norm": 1.8190311193466187,
      "learning_rate": 7.426819068859771e-05,
      "loss": 1.5709,
      "step": 2655
    },
    {
      "epoch": 0.6673775324593866,
      "grad_norm": 1.8877604007720947,
      "learning_rate": 7.39894061890159e-05,
      "loss": 1.4114,
      "step": 2660
    },
    {
      "epoch": 0.6686320015053628,
      "grad_norm": 1.7636263370513916,
      "learning_rate": 7.371062168943406e-05,
      "loss": 1.537,
      "step": 2665
    },
    {
      "epoch": 0.6698864705513391,
      "grad_norm": 1.844830870628357,
      "learning_rate": 7.343183718985224e-05,
      "loss": 1.4737,
      "step": 2670
    },
    {
      "epoch": 0.6711409395973155,
      "grad_norm": 1.93467378616333,
      "learning_rate": 7.315305269027042e-05,
      "loss": 1.4347,
      "step": 2675
    },
    {
      "epoch": 0.6723954086432917,
      "grad_norm": 2.133052110671997,
      "learning_rate": 7.28742681906886e-05,
      "loss": 1.3934,
      "step": 2680
    },
    {
      "epoch": 0.673649877689268,
      "grad_norm": 1.791480541229248,
      "learning_rate": 7.259548369110679e-05,
      "loss": 1.4229,
      "step": 2685
    },
    {
      "epoch": 0.6749043467352444,
      "grad_norm": 1.744071364402771,
      "learning_rate": 7.231669919152495e-05,
      "loss": 1.4213,
      "step": 2690
    },
    {
      "epoch": 0.6761588157812206,
      "grad_norm": 1.9081169366836548,
      "learning_rate": 7.203791469194313e-05,
      "loss": 1.4553,
      "step": 2695
    },
    {
      "epoch": 0.6774132848271969,
      "grad_norm": 1.6067748069763184,
      "learning_rate": 7.175913019236132e-05,
      "loss": 1.4287,
      "step": 2700
    },
    {
      "epoch": 0.6786677538731731,
      "grad_norm": 1.6224788427352905,
      "learning_rate": 7.148034569277948e-05,
      "loss": 1.4208,
      "step": 2705
    },
    {
      "epoch": 0.6799222229191495,
      "grad_norm": 1.587640404701233,
      "learning_rate": 7.120156119319766e-05,
      "loss": 1.4469,
      "step": 2710
    },
    {
      "epoch": 0.6811766919651258,
      "grad_norm": 1.7055336236953735,
      "learning_rate": 7.092277669361583e-05,
      "loss": 1.4675,
      "step": 2715
    },
    {
      "epoch": 0.682431161011102,
      "grad_norm": 1.6953974962234497,
      "learning_rate": 7.064399219403401e-05,
      "loss": 1.4401,
      "step": 2720
    },
    {
      "epoch": 0.6836856300570784,
      "grad_norm": 1.7100416421890259,
      "learning_rate": 7.03652076944522e-05,
      "loss": 1.4206,
      "step": 2725
    },
    {
      "epoch": 0.6849400991030546,
      "grad_norm": 1.4395337104797363,
      "learning_rate": 7.008642319487036e-05,
      "loss": 1.4686,
      "step": 2730
    },
    {
      "epoch": 0.6861945681490309,
      "grad_norm": 2.0909974575042725,
      "learning_rate": 6.980763869528854e-05,
      "loss": 1.5276,
      "step": 2735
    },
    {
      "epoch": 0.6874490371950072,
      "grad_norm": 1.9206271171569824,
      "learning_rate": 6.952885419570672e-05,
      "loss": 1.4467,
      "step": 2740
    },
    {
      "epoch": 0.6887035062409835,
      "grad_norm": 1.8874069452285767,
      "learning_rate": 6.925006969612489e-05,
      "loss": 1.433,
      "step": 2745
    },
    {
      "epoch": 0.6899579752869598,
      "grad_norm": 1.9574871063232422,
      "learning_rate": 6.897128519654307e-05,
      "loss": 1.4914,
      "step": 2750
    },
    {
      "epoch": 0.691212444332936,
      "grad_norm": 1.8027238845825195,
      "learning_rate": 6.869250069696125e-05,
      "loss": 1.4376,
      "step": 2755
    },
    {
      "epoch": 0.6924669133789124,
      "grad_norm": 23.8575382232666,
      "learning_rate": 6.841371619737943e-05,
      "loss": 1.3275,
      "step": 2760
    },
    {
      "epoch": 0.6937213824248887,
      "grad_norm": 1.843116044998169,
      "learning_rate": 6.813493169779762e-05,
      "loss": 1.3649,
      "step": 2765
    },
    {
      "epoch": 0.6949758514708649,
      "grad_norm": 1.6256579160690308,
      "learning_rate": 6.785614719821578e-05,
      "loss": 1.4452,
      "step": 2770
    },
    {
      "epoch": 0.6962303205168412,
      "grad_norm": 2.0334255695343018,
      "learning_rate": 6.757736269863396e-05,
      "loss": 1.3984,
      "step": 2775
    },
    {
      "epoch": 0.6974847895628176,
      "grad_norm": 1.6559646129608154,
      "learning_rate": 6.729857819905213e-05,
      "loss": 1.3906,
      "step": 2780
    },
    {
      "epoch": 0.6987392586087938,
      "grad_norm": 1.8816168308258057,
      "learning_rate": 6.701979369947031e-05,
      "loss": 1.4098,
      "step": 2785
    },
    {
      "epoch": 0.6999937276547701,
      "grad_norm": 2.1779890060424805,
      "learning_rate": 6.67410091998885e-05,
      "loss": 1.4446,
      "step": 2790
    },
    {
      "epoch": 0.7012481967007465,
      "grad_norm": 2.040961742401123,
      "learning_rate": 6.646222470030666e-05,
      "loss": 1.3881,
      "step": 2795
    },
    {
      "epoch": 0.7025026657467227,
      "grad_norm": 1.5992298126220703,
      "learning_rate": 6.618344020072484e-05,
      "loss": 1.4221,
      "step": 2800
    },
    {
      "epoch": 0.703757134792699,
      "grad_norm": 1.8125954866409302,
      "learning_rate": 6.590465570114301e-05,
      "loss": 1.4851,
      "step": 2805
    },
    {
      "epoch": 0.7050116038386752,
      "grad_norm": 2.2856810092926025,
      "learning_rate": 6.562587120156119e-05,
      "loss": 1.4627,
      "step": 2810
    },
    {
      "epoch": 0.7062660728846516,
      "grad_norm": 1.5303488969802856,
      "learning_rate": 6.534708670197937e-05,
      "loss": 1.4765,
      "step": 2815
    },
    {
      "epoch": 0.7075205419306279,
      "grad_norm": 1.951664686203003,
      "learning_rate": 6.506830220239754e-05,
      "loss": 1.4117,
      "step": 2820
    },
    {
      "epoch": 0.7087750109766041,
      "grad_norm": 1.9135644435882568,
      "learning_rate": 6.478951770281572e-05,
      "loss": 1.5144,
      "step": 2825
    },
    {
      "epoch": 0.7100294800225805,
      "grad_norm": 1.9205561876296997,
      "learning_rate": 6.45107332032339e-05,
      "loss": 1.4883,
      "step": 2830
    },
    {
      "epoch": 0.7112839490685567,
      "grad_norm": 1.5934282541275024,
      "learning_rate": 6.423194870365208e-05,
      "loss": 1.4271,
      "step": 2835
    },
    {
      "epoch": 0.712538418114533,
      "grad_norm": 1.610111951828003,
      "learning_rate": 6.395316420407027e-05,
      "loss": 1.4073,
      "step": 2840
    },
    {
      "epoch": 0.7137928871605093,
      "grad_norm": 1.4920272827148438,
      "learning_rate": 6.367437970448843e-05,
      "loss": 1.3465,
      "step": 2845
    },
    {
      "epoch": 0.7150473562064856,
      "grad_norm": 1.7976492643356323,
      "learning_rate": 6.339559520490661e-05,
      "loss": 1.4886,
      "step": 2850
    },
    {
      "epoch": 0.7163018252524619,
      "grad_norm": 1.9474754333496094,
      "learning_rate": 6.31168107053248e-05,
      "loss": 1.2863,
      "step": 2855
    },
    {
      "epoch": 0.7175562942984381,
      "grad_norm": 1.828946828842163,
      "learning_rate": 6.283802620574296e-05,
      "loss": 1.3392,
      "step": 2860
    },
    {
      "epoch": 0.7188107633444145,
      "grad_norm": 1.7905179262161255,
      "learning_rate": 6.255924170616114e-05,
      "loss": 1.4824,
      "step": 2865
    },
    {
      "epoch": 0.7200652323903908,
      "grad_norm": 1.8741108179092407,
      "learning_rate": 6.228045720657931e-05,
      "loss": 1.4846,
      "step": 2870
    },
    {
      "epoch": 0.721319701436367,
      "grad_norm": 1.833809733390808,
      "learning_rate": 6.200167270699749e-05,
      "loss": 1.4736,
      "step": 2875
    },
    {
      "epoch": 0.7225741704823434,
      "grad_norm": 1.6132196187973022,
      "learning_rate": 6.172288820741567e-05,
      "loss": 1.4739,
      "step": 2880
    },
    {
      "epoch": 0.7238286395283197,
      "grad_norm": 2.164198875427246,
      "learning_rate": 6.144410370783384e-05,
      "loss": 1.4105,
      "step": 2885
    },
    {
      "epoch": 0.7250831085742959,
      "grad_norm": 1.7883100509643555,
      "learning_rate": 6.116531920825202e-05,
      "loss": 1.4721,
      "step": 2890
    },
    {
      "epoch": 0.7263375776202722,
      "grad_norm": 1.7030059099197388,
      "learning_rate": 6.08865347086702e-05,
      "loss": 1.4009,
      "step": 2895
    },
    {
      "epoch": 0.7275920466662485,
      "grad_norm": 1.985499620437622,
      "learning_rate": 6.060775020908838e-05,
      "loss": 1.3451,
      "step": 2900
    },
    {
      "epoch": 0.7288465157122248,
      "grad_norm": 1.5954220294952393,
      "learning_rate": 6.032896570950656e-05,
      "loss": 1.3109,
      "step": 2905
    },
    {
      "epoch": 0.7301009847582011,
      "grad_norm": 1.6366221904754639,
      "learning_rate": 6.005018120992473e-05,
      "loss": 1.4054,
      "step": 2910
    },
    {
      "epoch": 0.7313554538041774,
      "grad_norm": 1.6659557819366455,
      "learning_rate": 5.977139671034291e-05,
      "loss": 1.4377,
      "step": 2915
    },
    {
      "epoch": 0.7326099228501537,
      "grad_norm": 1.7933005094528198,
      "learning_rate": 5.949261221076109e-05,
      "loss": 1.4533,
      "step": 2920
    },
    {
      "epoch": 0.73386439189613,
      "grad_norm": 1.9510546922683716,
      "learning_rate": 5.921382771117926e-05,
      "loss": 1.4398,
      "step": 2925
    },
    {
      "epoch": 0.7351188609421062,
      "grad_norm": 1.4886364936828613,
      "learning_rate": 5.893504321159744e-05,
      "loss": 1.3731,
      "step": 2930
    },
    {
      "epoch": 0.7363733299880826,
      "grad_norm": 2.1014559268951416,
      "learning_rate": 5.865625871201561e-05,
      "loss": 1.4417,
      "step": 2935
    },
    {
      "epoch": 0.7376277990340588,
      "grad_norm": 1.9273035526275635,
      "learning_rate": 5.837747421243379e-05,
      "loss": 1.4233,
      "step": 2940
    },
    {
      "epoch": 0.7388822680800351,
      "grad_norm": 1.817284345626831,
      "learning_rate": 5.8098689712851974e-05,
      "loss": 1.4454,
      "step": 2945
    },
    {
      "epoch": 0.7401367371260115,
      "grad_norm": 1.874295949935913,
      "learning_rate": 5.781990521327014e-05,
      "loss": 1.406,
      "step": 2950
    },
    {
      "epoch": 0.7413912061719877,
      "grad_norm": 2.027015209197998,
      "learning_rate": 5.754112071368832e-05,
      "loss": 1.4217,
      "step": 2955
    },
    {
      "epoch": 0.742645675217964,
      "grad_norm": 1.9956263303756714,
      "learning_rate": 5.726233621410649e-05,
      "loss": 1.3969,
      "step": 2960
    },
    {
      "epoch": 0.7439001442639402,
      "grad_norm": 1.6890944242477417,
      "learning_rate": 5.698355171452467e-05,
      "loss": 1.4651,
      "step": 2965
    },
    {
      "epoch": 0.7451546133099166,
      "grad_norm": 1.6406022310256958,
      "learning_rate": 5.670476721494285e-05,
      "loss": 1.3947,
      "step": 2970
    },
    {
      "epoch": 0.7464090823558929,
      "grad_norm": 1.9093599319458008,
      "learning_rate": 5.642598271536103e-05,
      "loss": 1.385,
      "step": 2975
    },
    {
      "epoch": 0.7476635514018691,
      "grad_norm": 1.7584134340286255,
      "learning_rate": 5.614719821577921e-05,
      "loss": 1.414,
      "step": 2980
    },
    {
      "epoch": 0.7489180204478455,
      "grad_norm": 2.1494829654693604,
      "learning_rate": 5.586841371619739e-05,
      "loss": 1.3281,
      "step": 2985
    },
    {
      "epoch": 0.7501724894938218,
      "grad_norm": 1.8263181447982788,
      "learning_rate": 5.558962921661556e-05,
      "loss": 1.4142,
      "step": 2990
    },
    {
      "epoch": 0.751426958539798,
      "grad_norm": 1.567487359046936,
      "learning_rate": 5.531084471703374e-05,
      "loss": 1.4321,
      "step": 2995
    },
    {
      "epoch": 0.7526814275857743,
      "grad_norm": 1.692433476448059,
      "learning_rate": 5.5032060217451906e-05,
      "loss": 1.3666,
      "step": 3000
    },
    {
      "epoch": 0.7539358966317506,
      "grad_norm": 1.8549550771713257,
      "learning_rate": 5.475327571787009e-05,
      "loss": 1.4457,
      "step": 3005
    },
    {
      "epoch": 0.7551903656777269,
      "grad_norm": 1.7190402746200562,
      "learning_rate": 5.447449121828827e-05,
      "loss": 1.3793,
      "step": 3010
    },
    {
      "epoch": 0.7564448347237032,
      "grad_norm": 2.0532772541046143,
      "learning_rate": 5.419570671870644e-05,
      "loss": 1.4794,
      "step": 3015
    },
    {
      "epoch": 0.7576993037696795,
      "grad_norm": 1.8464161157608032,
      "learning_rate": 5.3916922219124624e-05,
      "loss": 1.4025,
      "step": 3020
    },
    {
      "epoch": 0.7589537728156558,
      "grad_norm": 1.6306676864624023,
      "learning_rate": 5.363813771954279e-05,
      "loss": 1.4218,
      "step": 3025
    },
    {
      "epoch": 0.760208241861632,
      "grad_norm": 1.7813371419906616,
      "learning_rate": 5.335935321996097e-05,
      "loss": 1.4588,
      "step": 3030
    },
    {
      "epoch": 0.7614627109076083,
      "grad_norm": 2.42553973197937,
      "learning_rate": 5.3080568720379154e-05,
      "loss": 1.4387,
      "step": 3035
    },
    {
      "epoch": 0.7627171799535847,
      "grad_norm": 1.6262506246566772,
      "learning_rate": 5.280178422079732e-05,
      "loss": 1.4557,
      "step": 3040
    },
    {
      "epoch": 0.7639716489995609,
      "grad_norm": 2.103818655014038,
      "learning_rate": 5.25229997212155e-05,
      "loss": 1.4717,
      "step": 3045
    },
    {
      "epoch": 0.7652261180455372,
      "grad_norm": 1.907253623008728,
      "learning_rate": 5.224421522163368e-05,
      "loss": 1.3861,
      "step": 3050
    },
    {
      "epoch": 0.7664805870915136,
      "grad_norm": 1.6150102615356445,
      "learning_rate": 5.196543072205186e-05,
      "loss": 1.3511,
      "step": 3055
    },
    {
      "epoch": 0.7677350561374898,
      "grad_norm": 1.7104185819625854,
      "learning_rate": 5.168664622247004e-05,
      "loss": 1.3278,
      "step": 3060
    },
    {
      "epoch": 0.7689895251834661,
      "grad_norm": 1.8651574850082397,
      "learning_rate": 5.1407861722888207e-05,
      "loss": 1.3685,
      "step": 3065
    },
    {
      "epoch": 0.7702439942294423,
      "grad_norm": 1.6947500705718994,
      "learning_rate": 5.112907722330639e-05,
      "loss": 1.3394,
      "step": 3070
    },
    {
      "epoch": 0.7714984632754187,
      "grad_norm": 1.7579238414764404,
      "learning_rate": 5.085029272372457e-05,
      "loss": 1.4095,
      "step": 3075
    },
    {
      "epoch": 0.772752932321395,
      "grad_norm": 1.6247235536575317,
      "learning_rate": 5.0571508224142736e-05,
      "loss": 1.482,
      "step": 3080
    },
    {
      "epoch": 0.7740074013673712,
      "grad_norm": 1.5865241289138794,
      "learning_rate": 5.029272372456092e-05,
      "loss": 1.3467,
      "step": 3085
    },
    {
      "epoch": 0.7752618704133476,
      "grad_norm": 1.95722496509552,
      "learning_rate": 5.001393922497909e-05,
      "loss": 1.4495,
      "step": 3090
    },
    {
      "epoch": 0.7765163394593239,
      "grad_norm": 2.1474103927612305,
      "learning_rate": 4.973515472539727e-05,
      "loss": 1.3856,
      "step": 3095
    },
    {
      "epoch": 0.7777708085053001,
      "grad_norm": 2.103471279144287,
      "learning_rate": 4.945637022581545e-05,
      "loss": 1.3475,
      "step": 3100
    },
    {
      "epoch": 0.7790252775512764,
      "grad_norm": 1.5505810976028442,
      "learning_rate": 4.917758572623362e-05,
      "loss": 1.3862,
      "step": 3105
    },
    {
      "epoch": 0.7802797465972527,
      "grad_norm": 1.567158579826355,
      "learning_rate": 4.88988012266518e-05,
      "loss": 1.3837,
      "step": 3110
    },
    {
      "epoch": 0.781534215643229,
      "grad_norm": 1.8441133499145508,
      "learning_rate": 4.862001672706998e-05,
      "loss": 1.5005,
      "step": 3115
    },
    {
      "epoch": 0.7827886846892053,
      "grad_norm": 2.117445230484009,
      "learning_rate": 4.834123222748815e-05,
      "loss": 1.3913,
      "step": 3120
    },
    {
      "epoch": 0.7840431537351816,
      "grad_norm": 1.8759260177612305,
      "learning_rate": 4.8062447727906326e-05,
      "loss": 1.3823,
      "step": 3125
    },
    {
      "epoch": 0.7852976227811579,
      "grad_norm": 1.6271440982818604,
      "learning_rate": 4.778366322832451e-05,
      "loss": 1.4174,
      "step": 3130
    },
    {
      "epoch": 0.7865520918271341,
      "grad_norm": 1.7490438222885132,
      "learning_rate": 4.750487872874269e-05,
      "loss": 1.3729,
      "step": 3135
    },
    {
      "epoch": 0.7878065608731104,
      "grad_norm": 1.9499852657318115,
      "learning_rate": 4.722609422916086e-05,
      "loss": 1.3989,
      "step": 3140
    },
    {
      "epoch": 0.7890610299190868,
      "grad_norm": 1.9285269975662231,
      "learning_rate": 4.694730972957904e-05,
      "loss": 1.4281,
      "step": 3145
    },
    {
      "epoch": 0.790315498965063,
      "grad_norm": 1.7951672077178955,
      "learning_rate": 4.666852522999721e-05,
      "loss": 1.3459,
      "step": 3150
    },
    {
      "epoch": 0.7915699680110393,
      "grad_norm": 1.5221606492996216,
      "learning_rate": 4.638974073041539e-05,
      "loss": 1.4478,
      "step": 3155
    },
    {
      "epoch": 0.7928244370570157,
      "grad_norm": 1.7098174095153809,
      "learning_rate": 4.611095623083357e-05,
      "loss": 1.4886,
      "step": 3160
    },
    {
      "epoch": 0.7940789061029919,
      "grad_norm": 1.8723411560058594,
      "learning_rate": 4.583217173125174e-05,
      "loss": 1.4792,
      "step": 3165
    },
    {
      "epoch": 0.7953333751489682,
      "grad_norm": 1.9433155059814453,
      "learning_rate": 4.555338723166992e-05,
      "loss": 1.3968,
      "step": 3170
    },
    {
      "epoch": 0.7965878441949444,
      "grad_norm": 1.6479839086532593,
      "learning_rate": 4.5274602732088104e-05,
      "loss": 1.3081,
      "step": 3175
    },
    {
      "epoch": 0.7978423132409208,
      "grad_norm": 1.7864203453063965,
      "learning_rate": 4.499581823250628e-05,
      "loss": 1.3516,
      "step": 3180
    },
    {
      "epoch": 0.7990967822868971,
      "grad_norm": 2.085583209991455,
      "learning_rate": 4.471703373292445e-05,
      "loss": 1.4117,
      "step": 3185
    },
    {
      "epoch": 0.8003512513328733,
      "grad_norm": 2.0570433139801025,
      "learning_rate": 4.443824923334263e-05,
      "loss": 1.4006,
      "step": 3190
    },
    {
      "epoch": 0.8016057203788497,
      "grad_norm": 1.9007998704910278,
      "learning_rate": 4.41594647337608e-05,
      "loss": 1.42,
      "step": 3195
    },
    {
      "epoch": 0.802860189424826,
      "grad_norm": 1.9608538150787354,
      "learning_rate": 4.388068023417898e-05,
      "loss": 1.4636,
      "step": 3200
    },
    {
      "epoch": 0.8041146584708022,
      "grad_norm": 1.8109617233276367,
      "learning_rate": 4.3601895734597157e-05,
      "loss": 1.3676,
      "step": 3205
    },
    {
      "epoch": 0.8053691275167785,
      "grad_norm": 1.8141679763793945,
      "learning_rate": 4.332311123501534e-05,
      "loss": 1.4076,
      "step": 3210
    },
    {
      "epoch": 0.8066235965627548,
      "grad_norm": 1.7986860275268555,
      "learning_rate": 4.304432673543351e-05,
      "loss": 1.3933,
      "step": 3215
    },
    {
      "epoch": 0.8078780656087311,
      "grad_norm": 1.7418559789657593,
      "learning_rate": 4.276554223585169e-05,
      "loss": 1.3968,
      "step": 3220
    },
    {
      "epoch": 0.8091325346547074,
      "grad_norm": 1.9799284934997559,
      "learning_rate": 4.248675773626987e-05,
      "loss": 1.3169,
      "step": 3225
    },
    {
      "epoch": 0.8103870037006837,
      "grad_norm": 1.5959290266036987,
      "learning_rate": 4.220797323668804e-05,
      "loss": 1.372,
      "step": 3230
    },
    {
      "epoch": 0.81164147274666,
      "grad_norm": 2.1646411418914795,
      "learning_rate": 4.1929188737106216e-05,
      "loss": 1.5386,
      "step": 3235
    },
    {
      "epoch": 0.8128959417926362,
      "grad_norm": 1.934982180595398,
      "learning_rate": 4.165040423752439e-05,
      "loss": 1.4701,
      "step": 3240
    },
    {
      "epoch": 0.8141504108386125,
      "grad_norm": 1.478590965270996,
      "learning_rate": 4.137161973794257e-05,
      "loss": 1.318,
      "step": 3245
    },
    {
      "epoch": 0.8154048798845889,
      "grad_norm": 1.7151294946670532,
      "learning_rate": 4.1092835238360746e-05,
      "loss": 1.4409,
      "step": 3250
    },
    {
      "epoch": 0.8166593489305651,
      "grad_norm": 1.73770272731781,
      "learning_rate": 4.081405073877893e-05,
      "loss": 1.3599,
      "step": 3255
    },
    {
      "epoch": 0.8179138179765414,
      "grad_norm": 1.6381434202194214,
      "learning_rate": 4.05352662391971e-05,
      "loss": 1.3506,
      "step": 3260
    },
    {
      "epoch": 0.8191682870225178,
      "grad_norm": 1.8253756761550903,
      "learning_rate": 4.025648173961528e-05,
      "loss": 1.3933,
      "step": 3265
    },
    {
      "epoch": 0.820422756068494,
      "grad_norm": 1.8467297554016113,
      "learning_rate": 3.997769724003346e-05,
      "loss": 1.314,
      "step": 3270
    },
    {
      "epoch": 0.8216772251144703,
      "grad_norm": 1.6677159070968628,
      "learning_rate": 3.969891274045163e-05,
      "loss": 1.3765,
      "step": 3275
    },
    {
      "epoch": 0.8229316941604465,
      "grad_norm": 1.61935293674469,
      "learning_rate": 3.9420128240869806e-05,
      "loss": 1.3609,
      "step": 3280
    },
    {
      "epoch": 0.8241861632064229,
      "grad_norm": 1.6119663715362549,
      "learning_rate": 3.914134374128798e-05,
      "loss": 1.4172,
      "step": 3285
    },
    {
      "epoch": 0.8254406322523992,
      "grad_norm": 1.6173800230026245,
      "learning_rate": 3.886255924170616e-05,
      "loss": 1.3524,
      "step": 3290
    },
    {
      "epoch": 0.8266951012983754,
      "grad_norm": 1.8142807483673096,
      "learning_rate": 3.858377474212434e-05,
      "loss": 1.307,
      "step": 3295
    },
    {
      "epoch": 0.8279495703443518,
      "grad_norm": 1.7005691528320312,
      "learning_rate": 3.830499024254252e-05,
      "loss": 1.3339,
      "step": 3300
    },
    {
      "epoch": 0.829204039390328,
      "grad_norm": 1.829628825187683,
      "learning_rate": 3.802620574296069e-05,
      "loss": 1.3965,
      "step": 3305
    },
    {
      "epoch": 0.8304585084363043,
      "grad_norm": 1.9307702779769897,
      "learning_rate": 3.774742124337887e-05,
      "loss": 1.3648,
      "step": 3310
    },
    {
      "epoch": 0.8317129774822807,
      "grad_norm": 1.6690027713775635,
      "learning_rate": 3.746863674379705e-05,
      "loss": 1.3633,
      "step": 3315
    },
    {
      "epoch": 0.8329674465282569,
      "grad_norm": 1.7594032287597656,
      "learning_rate": 3.718985224421522e-05,
      "loss": 1.2866,
      "step": 3320
    },
    {
      "epoch": 0.8342219155742332,
      "grad_norm": 1.6894630193710327,
      "learning_rate": 3.6911067744633396e-05,
      "loss": 1.404,
      "step": 3325
    },
    {
      "epoch": 0.8354763846202095,
      "grad_norm": 1.6938586235046387,
      "learning_rate": 3.663228324505158e-05,
      "loss": 1.384,
      "step": 3330
    },
    {
      "epoch": 0.8367308536661858,
      "grad_norm": 1.6167218685150146,
      "learning_rate": 3.635349874546976e-05,
      "loss": 1.2864,
      "step": 3335
    },
    {
      "epoch": 0.8379853227121621,
      "grad_norm": 1.7210954427719116,
      "learning_rate": 3.607471424588793e-05,
      "loss": 1.3941,
      "step": 3340
    },
    {
      "epoch": 0.8392397917581383,
      "grad_norm": 1.8328754901885986,
      "learning_rate": 3.5795929746306107e-05,
      "loss": 1.394,
      "step": 3345
    },
    {
      "epoch": 0.8404942608041147,
      "grad_norm": 1.615810513496399,
      "learning_rate": 3.551714524672428e-05,
      "loss": 1.293,
      "step": 3350
    },
    {
      "epoch": 0.841748729850091,
      "grad_norm": 1.5967369079589844,
      "learning_rate": 3.523836074714246e-05,
      "loss": 1.3777,
      "step": 3355
    },
    {
      "epoch": 0.8430031988960672,
      "grad_norm": 2.107098340988159,
      "learning_rate": 3.4959576247560636e-05,
      "loss": 1.3716,
      "step": 3360
    },
    {
      "epoch": 0.8442576679420435,
      "grad_norm": 1.7497153282165527,
      "learning_rate": 3.468079174797881e-05,
      "loss": 1.3524,
      "step": 3365
    },
    {
      "epoch": 0.8455121369880199,
      "grad_norm": 2.091533660888672,
      "learning_rate": 3.440200724839699e-05,
      "loss": 1.418,
      "step": 3370
    },
    {
      "epoch": 0.8467666060339961,
      "grad_norm": 1.8086683750152588,
      "learning_rate": 3.412322274881517e-05,
      "loss": 1.3002,
      "step": 3375
    },
    {
      "epoch": 0.8480210750799724,
      "grad_norm": 1.742392659187317,
      "learning_rate": 3.384443824923335e-05,
      "loss": 1.3204,
      "step": 3380
    },
    {
      "epoch": 0.8492755441259487,
      "grad_norm": 1.6443361043930054,
      "learning_rate": 3.356565374965152e-05,
      "loss": 1.4072,
      "step": 3385
    },
    {
      "epoch": 0.850530013171925,
      "grad_norm": 1.6755337715148926,
      "learning_rate": 3.3286869250069696e-05,
      "loss": 1.3833,
      "step": 3390
    },
    {
      "epoch": 0.8517844822179013,
      "grad_norm": 1.7123792171478271,
      "learning_rate": 3.300808475048787e-05,
      "loss": 1.386,
      "step": 3395
    },
    {
      "epoch": 0.8530389512638775,
      "grad_norm": 1.882964015007019,
      "learning_rate": 3.272930025090605e-05,
      "loss": 1.3824,
      "step": 3400
    },
    {
      "epoch": 0.8542934203098539,
      "grad_norm": 1.876444697380066,
      "learning_rate": 3.2450515751324226e-05,
      "loss": 1.344,
      "step": 3405
    },
    {
      "epoch": 0.8555478893558301,
      "grad_norm": 1.711208462715149,
      "learning_rate": 3.217173125174241e-05,
      "loss": 1.3352,
      "step": 3410
    },
    {
      "epoch": 0.8568023584018064,
      "grad_norm": 1.7990893125534058,
      "learning_rate": 3.189294675216058e-05,
      "loss": 1.3352,
      "step": 3415
    },
    {
      "epoch": 0.8580568274477828,
      "grad_norm": 1.7469227313995361,
      "learning_rate": 3.161416225257876e-05,
      "loss": 1.3514,
      "step": 3420
    },
    {
      "epoch": 0.859311296493759,
      "grad_norm": 1.9243056774139404,
      "learning_rate": 3.133537775299694e-05,
      "loss": 1.3045,
      "step": 3425
    },
    {
      "epoch": 0.8605657655397353,
      "grad_norm": 1.773990273475647,
      "learning_rate": 3.105659325341511e-05,
      "loss": 1.3272,
      "step": 3430
    },
    {
      "epoch": 0.8618202345857116,
      "grad_norm": 1.9435144662857056,
      "learning_rate": 3.0777808753833286e-05,
      "loss": 1.3816,
      "step": 3435
    },
    {
      "epoch": 0.8630747036316879,
      "grad_norm": 1.4165475368499756,
      "learning_rate": 3.0499024254251464e-05,
      "loss": 1.3336,
      "step": 3440
    },
    {
      "epoch": 0.8643291726776642,
      "grad_norm": 1.9049582481384277,
      "learning_rate": 3.0220239754669645e-05,
      "loss": 1.3502,
      "step": 3445
    },
    {
      "epoch": 0.8655836417236404,
      "grad_norm": 1.636354684829712,
      "learning_rate": 2.994145525508782e-05,
      "loss": 1.3875,
      "step": 3450
    },
    {
      "epoch": 0.8668381107696168,
      "grad_norm": 1.7458758354187012,
      "learning_rate": 2.9662670755505993e-05,
      "loss": 1.3484,
      "step": 3455
    },
    {
      "epoch": 0.8680925798155931,
      "grad_norm": 1.6752426624298096,
      "learning_rate": 2.938388625592417e-05,
      "loss": 1.298,
      "step": 3460
    },
    {
      "epoch": 0.8693470488615693,
      "grad_norm": 1.4453366994857788,
      "learning_rate": 2.9105101756342352e-05,
      "loss": 1.365,
      "step": 3465
    },
    {
      "epoch": 0.8706015179075456,
      "grad_norm": 1.8393924236297607,
      "learning_rate": 2.8826317256760527e-05,
      "loss": 1.3193,
      "step": 3470
    },
    {
      "epoch": 0.871855986953522,
      "grad_norm": 1.764402151107788,
      "learning_rate": 2.85475327571787e-05,
      "loss": 1.2933,
      "step": 3475
    },
    {
      "epoch": 0.8731104559994982,
      "grad_norm": 1.7144312858581543,
      "learning_rate": 2.826874825759688e-05,
      "loss": 1.3138,
      "step": 3480
    },
    {
      "epoch": 0.8743649250454745,
      "grad_norm": 2.038161516189575,
      "learning_rate": 2.7989963758015053e-05,
      "loss": 1.3505,
      "step": 3485
    },
    {
      "epoch": 0.8756193940914508,
      "grad_norm": 1.6701726913452148,
      "learning_rate": 2.7711179258433234e-05,
      "loss": 1.4489,
      "step": 3490
    },
    {
      "epoch": 0.8768738631374271,
      "grad_norm": 1.9338933229446411,
      "learning_rate": 2.743239475885141e-05,
      "loss": 1.4116,
      "step": 3495
    },
    {
      "epoch": 0.8781283321834034,
      "grad_norm": 1.5618373155593872,
      "learning_rate": 2.7153610259269586e-05,
      "loss": 1.3248,
      "step": 3500
    },
    {
      "epoch": 0.8793828012293796,
      "grad_norm": 1.9206109046936035,
      "learning_rate": 2.687482575968776e-05,
      "loss": 1.4312,
      "step": 3505
    },
    {
      "epoch": 0.880637270275356,
      "grad_norm": 1.894228219985962,
      "learning_rate": 2.6596041260105942e-05,
      "loss": 1.3881,
      "step": 3510
    },
    {
      "epoch": 0.8818917393213322,
      "grad_norm": 2.254331111907959,
      "learning_rate": 2.6317256760524116e-05,
      "loss": 1.3744,
      "step": 3515
    },
    {
      "epoch": 0.8831462083673085,
      "grad_norm": 1.5333727598190308,
      "learning_rate": 2.6038472260942294e-05,
      "loss": 1.374,
      "step": 3520
    },
    {
      "epoch": 0.8844006774132849,
      "grad_norm": 1.6157965660095215,
      "learning_rate": 2.575968776136047e-05,
      "loss": 1.3204,
      "step": 3525
    },
    {
      "epoch": 0.8856551464592611,
      "grad_norm": 1.8824197053909302,
      "learning_rate": 2.5480903261778643e-05,
      "loss": 1.4066,
      "step": 3530
    },
    {
      "epoch": 0.8869096155052374,
      "grad_norm": 1.6211658716201782,
      "learning_rate": 2.5202118762196824e-05,
      "loss": 1.3564,
      "step": 3535
    },
    {
      "epoch": 0.8881640845512137,
      "grad_norm": 1.710509181022644,
      "learning_rate": 2.4923334262615e-05,
      "loss": 1.3709,
      "step": 3540
    },
    {
      "epoch": 0.88941855359719,
      "grad_norm": 1.7114139795303345,
      "learning_rate": 2.4644549763033176e-05,
      "loss": 1.3489,
      "step": 3545
    },
    {
      "epoch": 0.8906730226431663,
      "grad_norm": 1.9727154970169067,
      "learning_rate": 2.4365765263451354e-05,
      "loss": 1.4097,
      "step": 3550
    },
    {
      "epoch": 0.8919274916891425,
      "grad_norm": 1.8179163932800293,
      "learning_rate": 2.4086980763869528e-05,
      "loss": 1.3693,
      "step": 3555
    },
    {
      "epoch": 0.8931819607351189,
      "grad_norm": 1.4931888580322266,
      "learning_rate": 2.380819626428771e-05,
      "loss": 1.2376,
      "step": 3560
    },
    {
      "epoch": 0.8944364297810952,
      "grad_norm": 1.8483238220214844,
      "learning_rate": 2.3529411764705884e-05,
      "loss": 1.3622,
      "step": 3565
    },
    {
      "epoch": 0.8956908988270714,
      "grad_norm": 1.7281923294067383,
      "learning_rate": 2.325062726512406e-05,
      "loss": 1.4346,
      "step": 3570
    },
    {
      "epoch": 0.8969453678730477,
      "grad_norm": 1.6993217468261719,
      "learning_rate": 2.2971842765542236e-05,
      "loss": 1.3385,
      "step": 3575
    },
    {
      "epoch": 0.898199836919024,
      "grad_norm": 1.4441702365875244,
      "learning_rate": 2.2693058265960414e-05,
      "loss": 1.3187,
      "step": 3580
    },
    {
      "epoch": 0.8994543059650003,
      "grad_norm": 1.648138165473938,
      "learning_rate": 2.241427376637859e-05,
      "loss": 1.3392,
      "step": 3585
    },
    {
      "epoch": 0.9007087750109766,
      "grad_norm": 1.6321099996566772,
      "learning_rate": 2.2135489266796766e-05,
      "loss": 1.3054,
      "step": 3590
    },
    {
      "epoch": 0.9019632440569529,
      "grad_norm": 1.808176040649414,
      "learning_rate": 2.1856704767214943e-05,
      "loss": 1.3703,
      "step": 3595
    },
    {
      "epoch": 0.9032177131029292,
      "grad_norm": 1.8416849374771118,
      "learning_rate": 2.157792026763312e-05,
      "loss": 1.2889,
      "step": 3600
    },
    {
      "epoch": 0.9044721821489055,
      "grad_norm": 1.9851354360580444,
      "learning_rate": 2.12991357680513e-05,
      "loss": 1.3668,
      "step": 3605
    },
    {
      "epoch": 0.9057266511948817,
      "grad_norm": 1.5478920936584473,
      "learning_rate": 2.1020351268469473e-05,
      "loss": 1.2572,
      "step": 3610
    },
    {
      "epoch": 0.9069811202408581,
      "grad_norm": 1.6281780004501343,
      "learning_rate": 2.074156676888765e-05,
      "loss": 1.3607,
      "step": 3615
    },
    {
      "epoch": 0.9082355892868343,
      "grad_norm": 1.6937538385391235,
      "learning_rate": 2.046278226930583e-05,
      "loss": 1.3992,
      "step": 3620
    },
    {
      "epoch": 0.9094900583328106,
      "grad_norm": 1.7245457172393799,
      "learning_rate": 2.0183997769724007e-05,
      "loss": 1.4504,
      "step": 3625
    },
    {
      "epoch": 0.910744527378787,
      "grad_norm": 1.6039317846298218,
      "learning_rate": 1.990521327014218e-05,
      "loss": 1.3289,
      "step": 3630
    },
    {
      "epoch": 0.9119989964247632,
      "grad_norm": 1.653456687927246,
      "learning_rate": 1.9626428770560355e-05,
      "loss": 1.3311,
      "step": 3635
    },
    {
      "epoch": 0.9132534654707395,
      "grad_norm": 1.7128390073776245,
      "learning_rate": 1.9347644270978536e-05,
      "loss": 1.3995,
      "step": 3640
    },
    {
      "epoch": 0.9145079345167157,
      "grad_norm": 1.7258750200271606,
      "learning_rate": 1.906885977139671e-05,
      "loss": 1.3226,
      "step": 3645
    },
    {
      "epoch": 0.9157624035626921,
      "grad_norm": 1.818243145942688,
      "learning_rate": 1.879007527181489e-05,
      "loss": 1.4076,
      "step": 3650
    },
    {
      "epoch": 0.9170168726086684,
      "grad_norm": 1.5238037109375,
      "learning_rate": 1.8511290772233063e-05,
      "loss": 1.3448,
      "step": 3655
    },
    {
      "epoch": 0.9182713416546446,
      "grad_norm": 1.7661669254302979,
      "learning_rate": 1.8232506272651244e-05,
      "loss": 1.34,
      "step": 3660
    },
    {
      "epoch": 0.919525810700621,
      "grad_norm": 1.9886248111724854,
      "learning_rate": 1.795372177306942e-05,
      "loss": 1.3423,
      "step": 3665
    },
    {
      "epoch": 0.9207802797465973,
      "grad_norm": 1.5747536420822144,
      "learning_rate": 1.7674937273487596e-05,
      "loss": 1.3347,
      "step": 3670
    },
    {
      "epoch": 0.9220347487925735,
      "grad_norm": 1.584535837173462,
      "learning_rate": 1.739615277390577e-05,
      "loss": 1.2957,
      "step": 3675
    },
    {
      "epoch": 0.9232892178385498,
      "grad_norm": 1.5325794219970703,
      "learning_rate": 1.7117368274323948e-05,
      "loss": 1.3648,
      "step": 3680
    },
    {
      "epoch": 0.9245436868845262,
      "grad_norm": 1.5698193311691284,
      "learning_rate": 1.6838583774742126e-05,
      "loss": 1.34,
      "step": 3685
    },
    {
      "epoch": 0.9257981559305024,
      "grad_norm": 1.7394952774047852,
      "learning_rate": 1.65597992751603e-05,
      "loss": 1.3771,
      "step": 3690
    },
    {
      "epoch": 0.9270526249764787,
      "grad_norm": 1.5217808485031128,
      "learning_rate": 1.6281014775578478e-05,
      "loss": 1.3105,
      "step": 3695
    },
    {
      "epoch": 0.928307094022455,
      "grad_norm": 1.84344482421875,
      "learning_rate": 1.6002230275996656e-05,
      "loss": 1.3454,
      "step": 3700
    },
    {
      "epoch": 0.9295615630684313,
      "grad_norm": 1.9470665454864502,
      "learning_rate": 1.5723445776414834e-05,
      "loss": 1.2385,
      "step": 3705
    },
    {
      "epoch": 0.9308160321144076,
      "grad_norm": 1.378627061843872,
      "learning_rate": 1.5444661276833008e-05,
      "loss": 1.3029,
      "step": 3710
    },
    {
      "epoch": 0.9320705011603838,
      "grad_norm": 1.7747728824615479,
      "learning_rate": 1.5165876777251187e-05,
      "loss": 1.3621,
      "step": 3715
    },
    {
      "epoch": 0.9333249702063602,
      "grad_norm": 1.6735436916351318,
      "learning_rate": 1.4887092277669362e-05,
      "loss": 1.3055,
      "step": 3720
    },
    {
      "epoch": 0.9345794392523364,
      "grad_norm": 1.8040850162506104,
      "learning_rate": 1.4608307778087538e-05,
      "loss": 1.2555,
      "step": 3725
    },
    {
      "epoch": 0.9358339082983127,
      "grad_norm": 1.9578782320022583,
      "learning_rate": 1.4329523278505716e-05,
      "loss": 1.3129,
      "step": 3730
    },
    {
      "epoch": 0.9370883773442891,
      "grad_norm": 2.070911169052124,
      "learning_rate": 1.4050738778923892e-05,
      "loss": 1.3831,
      "step": 3735
    },
    {
      "epoch": 0.9383428463902653,
      "grad_norm": 1.5060756206512451,
      "learning_rate": 1.377195427934207e-05,
      "loss": 1.3424,
      "step": 3740
    },
    {
      "epoch": 0.9395973154362416,
      "grad_norm": 1.4332283735275269,
      "learning_rate": 1.3493169779760246e-05,
      "loss": 1.2895,
      "step": 3745
    },
    {
      "epoch": 0.940851784482218,
      "grad_norm": 1.708336591720581,
      "learning_rate": 1.3214385280178423e-05,
      "loss": 1.4075,
      "step": 3750
    },
    {
      "epoch": 0.9421062535281942,
      "grad_norm": 1.7157478332519531,
      "learning_rate": 1.29356007805966e-05,
      "loss": 1.2969,
      "step": 3755
    },
    {
      "epoch": 0.9433607225741705,
      "grad_norm": 1.644660472869873,
      "learning_rate": 1.2656816281014777e-05,
      "loss": 1.2822,
      "step": 3760
    },
    {
      "epoch": 0.9446151916201467,
      "grad_norm": 1.6669507026672363,
      "learning_rate": 1.2378031781432953e-05,
      "loss": 1.3289,
      "step": 3765
    },
    {
      "epoch": 0.9458696606661231,
      "grad_norm": 1.9152599573135376,
      "learning_rate": 1.209924728185113e-05,
      "loss": 1.2996,
      "step": 3770
    },
    {
      "epoch": 0.9471241297120994,
      "grad_norm": 2.0029850006103516,
      "learning_rate": 1.1820462782269307e-05,
      "loss": 1.3746,
      "step": 3775
    },
    {
      "epoch": 0.9483785987580756,
      "grad_norm": 1.6476703882217407,
      "learning_rate": 1.1541678282687483e-05,
      "loss": 1.4262,
      "step": 3780
    },
    {
      "epoch": 0.949633067804052,
      "grad_norm": 1.4019869565963745,
      "learning_rate": 1.126289378310566e-05,
      "loss": 1.3641,
      "step": 3785
    },
    {
      "epoch": 0.9508875368500282,
      "grad_norm": 1.6606228351593018,
      "learning_rate": 1.0984109283523837e-05,
      "loss": 1.3925,
      "step": 3790
    },
    {
      "epoch": 0.9521420058960045,
      "grad_norm": 1.74434232711792,
      "learning_rate": 1.0705324783942015e-05,
      "loss": 1.3698,
      "step": 3795
    },
    {
      "epoch": 0.9533964749419808,
      "grad_norm": 1.825920820236206,
      "learning_rate": 1.0426540284360189e-05,
      "loss": 1.3135,
      "step": 3800
    },
    {
      "epoch": 0.9546509439879571,
      "grad_norm": 1.6647181510925293,
      "learning_rate": 1.0147755784778367e-05,
      "loss": 1.3444,
      "step": 3805
    },
    {
      "epoch": 0.9559054130339334,
      "grad_norm": 1.5962462425231934,
      "learning_rate": 9.868971285196543e-06,
      "loss": 1.3177,
      "step": 3810
    },
    {
      "epoch": 0.9571598820799097,
      "grad_norm": 1.7542108297348022,
      "learning_rate": 9.59018678561472e-06,
      "loss": 1.3851,
      "step": 3815
    },
    {
      "epoch": 0.958414351125886,
      "grad_norm": 1.9888856410980225,
      "learning_rate": 9.311402286032897e-06,
      "loss": 1.3863,
      "step": 3820
    },
    {
      "epoch": 0.9596688201718623,
      "grad_norm": 1.8001360893249512,
      "learning_rate": 9.032617786451074e-06,
      "loss": 1.3488,
      "step": 3825
    },
    {
      "epoch": 0.9609232892178385,
      "grad_norm": 2.362081527709961,
      "learning_rate": 8.75383328686925e-06,
      "loss": 1.3632,
      "step": 3830
    },
    {
      "epoch": 0.9621777582638148,
      "grad_norm": 1.8056241273880005,
      "learning_rate": 8.475048787287428e-06,
      "loss": 1.283,
      "step": 3835
    },
    {
      "epoch": 0.9634322273097912,
      "grad_norm": 1.4213817119598389,
      "learning_rate": 8.196264287705604e-06,
      "loss": 1.36,
      "step": 3840
    },
    {
      "epoch": 0.9646866963557674,
      "grad_norm": 1.7259098291397095,
      "learning_rate": 7.917479788123782e-06,
      "loss": 1.3175,
      "step": 3845
    },
    {
      "epoch": 0.9659411654017437,
      "grad_norm": 1.6881791353225708,
      "learning_rate": 7.638695288541956e-06,
      "loss": 1.4206,
      "step": 3850
    },
    {
      "epoch": 0.96719563444772,
      "grad_norm": 1.7309726476669312,
      "learning_rate": 7.359910788960134e-06,
      "loss": 1.2513,
      "step": 3855
    },
    {
      "epoch": 0.9684501034936963,
      "grad_norm": 1.5149004459381104,
      "learning_rate": 7.081126289378311e-06,
      "loss": 1.3489,
      "step": 3860
    },
    {
      "epoch": 0.9697045725396726,
      "grad_norm": 2.2050650119781494,
      "learning_rate": 6.802341789796488e-06,
      "loss": 1.3305,
      "step": 3865
    },
    {
      "epoch": 0.9709590415856488,
      "grad_norm": 1.7760158777236938,
      "learning_rate": 6.523557290214665e-06,
      "loss": 1.3197,
      "step": 3870
    },
    {
      "epoch": 0.9722135106316252,
      "grad_norm": 1.8403724431991577,
      "learning_rate": 6.244772790632841e-06,
      "loss": 1.4127,
      "step": 3875
    },
    {
      "epoch": 0.9734679796776015,
      "grad_norm": 1.644061803817749,
      "learning_rate": 5.965988291051018e-06,
      "loss": 1.2834,
      "step": 3880
    },
    {
      "epoch": 0.9747224487235777,
      "grad_norm": 1.5961419343948364,
      "learning_rate": 5.687203791469195e-06,
      "loss": 1.3211,
      "step": 3885
    },
    {
      "epoch": 0.9759769177695541,
      "grad_norm": 1.7829469442367554,
      "learning_rate": 5.4084192918873716e-06,
      "loss": 1.301,
      "step": 3890
    },
    {
      "epoch": 0.9772313868155303,
      "grad_norm": 1.7440838813781738,
      "learning_rate": 5.1296347923055485e-06,
      "loss": 1.2499,
      "step": 3895
    },
    {
      "epoch": 0.9784858558615066,
      "grad_norm": 1.6521176099777222,
      "learning_rate": 4.8508502927237245e-06,
      "loss": 1.2554,
      "step": 3900
    },
    {
      "epoch": 0.9797403249074829,
      "grad_norm": 1.814825177192688,
      "learning_rate": 4.5720657931419014e-06,
      "loss": 1.3445,
      "step": 3905
    },
    {
      "epoch": 0.9809947939534592,
      "grad_norm": 1.980621337890625,
      "learning_rate": 4.293281293560078e-06,
      "loss": 1.2819,
      "step": 3910
    },
    {
      "epoch": 0.9822492629994355,
      "grad_norm": 1.406925916671753,
      "learning_rate": 4.014496793978255e-06,
      "loss": 1.3265,
      "step": 3915
    },
    {
      "epoch": 0.9835037320454117,
      "grad_norm": 1.4773796796798706,
      "learning_rate": 3.7357122943964317e-06,
      "loss": 1.2461,
      "step": 3920
    }
  ],
  "logging_steps": 5,
  "max_steps": 3986,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 10,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.537671375097856e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
